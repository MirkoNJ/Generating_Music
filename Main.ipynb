{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#% reset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import math_ops\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import midi_musical_matrix\n",
    "import data\n",
    "import multi_training\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "from MyFunctions import Input_Kernel, LSTM_TimeWise_Training_Layer, LSTM_NoteWise_Layer, Loss_Function\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip bad file =  Fugue12\n",
      "Loaded schub5\n",
      "Loaded tri2\n",
      "Loaded sinfon1 (1)\n",
      "Loaded 01Prelude\n",
      "Loaded Prelude12\n",
      "Loaded BRAND3\n",
      "Loaded Fugue8\n",
      "Loaded Fugue22\n",
      "Loaded Prelude21\n",
      "Loaded 04Bourree\n",
      "Loaded BSGJG_H\n",
      "Loaded Fugue18\n",
      "Loaded 11Jesu\n",
      "Loaded Fugue5\n",
      "Loaded sinfon4\n",
      "Skip bad file =  Fugue19\n",
      "Loaded Fugue4\n",
      "Loaded unfin\n",
      "Loaded can4\n",
      "Loaded Fugue17\n",
      "Loaded 04Prelude\n",
      "Loaded sinfon12\n",
      "Skip bad file =  Prelude20\n",
      "Loaded dou2\n",
      "Loaded prefug5\n",
      "Loaded prefug3\n",
      "Loaded gig1\n",
      "Loaded catech6\n",
      "Loaded prefug7\n",
      "Loaded BRAND43\n",
      "Loaded 08Freueteuch\n",
      "Loaded dou1\n",
      "Loaded Fugue9 (1)\n",
      "Loaded Fugue7 (1)\n",
      "Loaded toccata1\n",
      "Loaded sinfon1\n",
      "Loaded BRAND51\n",
      "Loaded invent13\n",
      "Loaded sinfon14\n",
      "Loaded 01Allemande\n",
      "Loaded Fugue3\n",
      "Skip bad file =  Prelude13\n",
      "Loaded Prelude5\n",
      "Loaded cnt1\n",
      "Loaded sinfon3\n",
      "Loaded cnt3\n",
      "Loaded 09Ermuntredich\n",
      "Loaded invent7\n",
      "Loaded Prelude8 (1)\n",
      "Loaded invent5\n",
      "Loaded cap2\n",
      "Loaded 15ChristlaginTode\n",
      "Loaded BRAND53\n",
      "Loaded sin2\n",
      "Loaded Prelude3 (1)\n",
      "Loaded Prelude2 (1)\n",
      "Loaded invent14\n",
      "Loaded 10AustieferNot\n",
      "Loaded inver2\n",
      "Loaded 13Alleinzudir\n",
      "Loaded Fugue7\n",
      "Loaded BSGJG_K\n",
      "Loaded BRAND1\n",
      "Loaded Fugue16\n",
      "Loaded BRAND52\n",
      "Loaded invent15\n",
      "Loaded invent11\n",
      "Loaded schub6\n",
      "Loaded Prelude23\n",
      "Loaded catech10\n",
      "Skip bad file =  Prelude15\n",
      "Loaded fuguegm\n",
      "Loaded invent1\n",
      "Loaded invent2\n",
      "Loaded catech8\n",
      "Loaded Fugue3 (1)\n",
      "Loaded prefug8\n",
      "Loaded trio3a\n",
      "Loaded pre1\n",
      "Loaded Prelude16\n",
      "Loaded Prelude12 (1)\n",
      "Loaded cnt1 (1)\n",
      "Loaded Fugue12 (1)\n",
      "Loaded prefug1\n",
      "Loaded sinfon8\n",
      "Loaded Prelude2\n",
      "Skip bad file =  Prelude19\n",
      "Loaded BSGJG_G\n",
      "Loaded Fugue5 (1)\n",
      "Loaded Prelude10\n",
      "Skip bad file =  Fugue13\n",
      "Skip bad file =  Prelude14\n",
      "Loaded Prelude7\n",
      "Loaded 14OHerreGott\n",
      "Skip bad file =  Fugue15\n",
      "Loaded Fugue1\n",
      "Loaded tri1\n",
      "Skip bad file =  Fugue6\n",
      "Loaded 04Allemande\n",
      "Loaded BSGJG_I\n",
      "Loaded orgel19\n",
      "Loaded cnt2\n",
      "Loaded Fugue1 (1)\n",
      "Loaded Prelude1 (1)\n",
      "Loaded BSGJG_B\n",
      "Loaded prefug2\n",
      "Loaded catech7\n",
      "Loaded Prelude1\n",
      "Loaded 02Ichdankdir\n",
      "Loaded Fugue20\n",
      "Loaded sinfon9\n",
      "Loaded Prelude22\n",
      "Loaded catech9\n",
      "Skip bad file =  Fugue11\n",
      "Loaded Fugue8 (1)\n",
      "Loaded reg2\n",
      "Loaded 05AnWasserflussen\n",
      "Loaded toccata2\n",
      "Loaded Fugue23\n",
      "Loaded Prelude6\n",
      "Loaded catech1\n",
      "Loaded Fugue2\n",
      "Skip bad file =  Prelude24\n",
      "Loaded reg1\n",
      "Loaded Fugue24\n",
      "Loaded mir2\n",
      "Loaded BSGJG_J\n",
      "Loaded Fugue9\n",
      "Loaded fuguecm\n",
      "Loaded inver1\n",
      "\n",
      "Number of training pieces =  119\n",
      "Sample of State Input Batch: shape =  (15, 78, 128, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import All Training Data\n",
    "# Convert Entire Music .MIDI set to list of musical 'pieces'\n",
    "# During training runs, getPieceBatch will return a tensor for Note_State_Batch, and corresponding Note_State_Expand\n",
    "# Note_State_Expand will be fed into the graph input, and Note_State_Batch will be used for the loss function.\n",
    "\n",
    "# Import Midi files to list\n",
    "Working_Directory = os.getcwd()\n",
    "Training_Midi_Folder = Working_Directory + \"/Midi_Files/Bach\"\n",
    "max_time_steps = 256 # only files atleast this many 16th note steps are saved\n",
    "\n",
    "practice_batch_size = 15\n",
    "practice_num_timesteps = 128\n",
    "\n",
    "\n",
    "training_pieces = multi_training.loadPieces(Training_Midi_Folder, max_time_steps)\n",
    "print('')\n",
    "print('Number of training pieces = ', len(training_pieces))\n",
    "\n",
    "# Generate sample Note State Matrix for dimension measurement and numerical checking purposes\n",
    "# (Using external code to generate the Note State Matrix but using our own NoteInputForm (as defined in author's code) function\n",
    "_, sample_state = multi_training.getPieceBatch(training_pieces, practice_batch_size, practice_num_timesteps)\n",
    "sample_state = np.array(sample_state)\n",
    "sample_state = np.swapaxes(sample_state, axis1=1, axis2=2)\n",
    "print('Sample of State Input Batch: shape = ', sample_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note_State_Batch Placeholder Shape =  (?, 78, ?, 2)\n",
      "Note_State_Expand output Shape =  (?, 78, ?, 80)\n"
     ]
    }
   ],
   "source": [
    "# Beginning of Model Graph:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#input_size = sample_state.shape[-1]\n",
    "num_notes = sample_state.shape[1]\n",
    "\n",
    "\n",
    "#place holder inputs\n",
    "# num_batches and num_time steps are variable lengths.  These values do not affect the model parameters\n",
    "# Dimension(0) =  num_batches. Dimension(2) = num_time_steps\n",
    "\n",
    "#final_t_sample_run = np.zeros((batch_size, num_notes, 1, 2)) #start every batch with zero previous input\n",
    "\n",
    "\n",
    "        \n",
    "Note_State_Batch = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, 2])\n",
    "#prev_t_sample = tf.placeholder(dtype=tf.float32, shape=[None, num_notes,1,2])\n",
    "time_init = tf.placeholder(dtype=tf.int32, shape=())\n",
    "#Generates expanded tensor input to LSTM-timewise layer\n",
    "Note_State_Expand = Input_Kernel(Note_State_Batch, Midi_low=24, Midi_high=101, time_init=time_init)\n",
    "\n",
    "print('Note_State_Batch Placeholder Shape = ', Note_State_Batch.get_shape())\n",
    "print('Note_State_Expand output Shape = ', Note_State_Expand.get_shape())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_expand shape =  (15, 78, 128, 80)\n",
      "MIDI note_0, t_0 =  [ 24.]\n",
      "MIDI note_1, t_0 =  [ 25.]\n",
      "MIDI note_2, t_0 =  [ 26.]\n",
      "MIDI note_0, t_0 =  [ 24.]\n",
      "MIDI note_0, t_1 =  [ 24.]\n",
      "MIDI note_0, t_29 =  [ 24.]\n",
      "\n",
      "pitchclass note_0, t_0 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "pitchclass note_1, t_0 =  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "pitchclass note_11, t_0 =  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "pitchclass note_0, t_0 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "pitchclass note_0, t_1 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "pitchclass note_0, t_29 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "sample state local vicinity =  [[0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "calculated vicinity note_45, t_29 =  [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "calculated context note_45, t_29 =  [  9.99999046e-01   7.63136256e-08   9.97204666e-08   9.99999285e-01\n",
      "   7.51044809e-08  -1.84237379e-07  -6.39887432e-08   1.42839355e-07\n",
      "   1.61950496e-07   1.00000000e+00   1.43299644e-07  -4.02006322e-08]\n",
      "actual all note plays at, t_29 =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n",
      "\n",
      "beat note_0, t_0 =  [ 1.  0.  0.  0.]\n",
      "beat note_1, t_0 =  [ 1.  0.  0.  0.]\n",
      "beat note_2, t_0 =  [ 1.  0.  0.  0.]\n",
      "beat note_0, t_0 =  [ 1.  0.  0.  0.]\n",
      "beat note_0, t_1 =  [ 0.  1.  0.  0.]\n",
      "beat note_0, t_29 =  [ 0.  1.  1.  1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Input Kernel on sample data\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_expand = sess.run(Note_State_Expand, feed_dict={Note_State_Batch: sample_state, time_init: 1})\n",
    "\n",
    "\n",
    "\n",
    "#check MIDI note\n",
    "print('sample_expand shape = ', sample_expand.shape)\n",
    "print('MIDI note_0, t_0 = ', sample_expand[0,0,0,[0]]) \n",
    "print('MIDI note_1, t_0 = ', sample_expand[0,1,0,[0]])  \n",
    "print('MIDI note_2, t_0 = ', sample_expand[0,2,0,[0]]) \n",
    "\n",
    "print('MIDI note_0, t_0 = ', sample_expand[0,0,0,[0]]) \n",
    "print('MIDI note_0, t_1 = ', sample_expand[0,0,1,[0]])  \n",
    "print('MIDI note_0, t_29 = ', sample_expand[0,0,29,[0]]) \n",
    "print('') \n",
    "\n",
    "#check pitchclass\n",
    "print('pitchclass note_0, t_0 = ', sample_expand[0,0,0,1:13]) \n",
    "print('pitchclass note_1, t_0 = ', sample_expand[0,1,0,1:13])  \n",
    "print('pitchclass note_11, t_0 = ', sample_expand[0,11,0,1:13]) \n",
    "\n",
    "print('pitchclass note_0, t_0 = ', sample_expand[0,0,0,1:13]) \n",
    "print('pitchclass note_0, t_1 = ', sample_expand[0,0,1,1:13])  \n",
    "print('pitchclass note_0, t_29 = ', sample_expand[0,0,29,1:13]) \n",
    "print('') \n",
    "\n",
    "#check vicinity\n",
    "print('sample state local vicinity = ', sample_state[0,33:58,29,:])\n",
    "print('calculated vicinity note_45, t_29 = ', sample_expand[0,45,29,13:63])\n",
    "print('')\n",
    "\n",
    "#check  context\n",
    "print('calculated context note_45, t_29 = ', sample_expand[0,45,29,63:75])\n",
    "print('actual all note plays at, t_29 = ', sample_state[0,:,29,0])\n",
    "print('')\n",
    "\n",
    "#check beat\n",
    "print('beat note_0, t_0 = ', sample_expand[0,0,0,75:79]) \n",
    "print('beat note_1, t_0 = ', sample_expand[0,1,0,75:79])  \n",
    "print('beat note_2, t_0 = ', sample_expand[0,2,0,75:79]) \n",
    "\n",
    "print('beat note_0, t_0 = ', sample_expand[0,0,0,75:79]) \n",
    "print('beat note_0, t_1 = ', sample_expand[0,0,1,75:79])  \n",
    "print('beat note_0, t_29 = ', sample_expand[0,0,29,75:79]) \n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-wise output shape =  (?, 78, ?, 200)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#lSTM Time Wise Training Graph \n",
    "#tf.reset_default_graph()\n",
    "#Note_State_Expand = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, 80])\n",
    "#Note_State_Expand_val = np.ones((10,78,128,80))\n",
    "\n",
    "num_t_units=[300,200]\n",
    "output_keep_prob = tf.placeholder(dtype=tf.float32, shape=())\n",
    "\n",
    "# Generate initial state (at t=0) placeholder\n",
    "timewise_state=[]\n",
    "for i in range(len(num_t_units)):\n",
    "    timewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_t_units[i]]) #None = batch_size * num_notes\n",
    "    timewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_t_units[i]])\n",
    "    timewise_state.append(LSTMStateTuple(timewise_h, timewise_c))\n",
    "\n",
    "timewise_state=tuple(timewise_state)\n",
    "\n",
    "\n",
    "timewise_out, timewise_state_out = LSTM_TimeWise_Training_Layer(input_data=Note_State_Expand, state_init=timewise_state, output_keep_prob=output_keep_prob)\n",
    "\n",
    "\n",
    "print('Time-wise output shape = ', timewise_out.get_shape())\n",
    "print(len(timewise_state_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_out shape =  (?, 78, ?, 2, 2)\n",
      "generated samples shape =  (?, 78, ?, 2)\n"
     ]
    }
   ],
   "source": [
    "#LSTM Note Wise Graph\n",
    "#tf.reset_default_graph()\n",
    "#timewise_out = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, 50])\n",
    "#output_keep_prob=1\n",
    "num_n_units = [100,50]\n",
    "\n",
    "# Generate initial state (at n=0) placeholder\n",
    "notewise_state=[]\n",
    "for i in range(len(num_n_units)):\n",
    "    notewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_n_units[i]]) #None = batch_size * num_timesteps\n",
    "    notewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_n_units[i]])\n",
    "    notewise_state.append(LSTMStateTuple(notewise_h, notewise_c))\n",
    "\n",
    "notewise_state=tuple(notewise_state)\n",
    "\n",
    "\n",
    "y_out, note_gen_out = LSTM_NoteWise_Layer(timewise_out, state_init=notewise_state, output_keep_prob=output_keep_prob)\n",
    "\n",
    "\n",
    "print('y_out shape = ', y_out.get_shape())\n",
    "print('generated samples shape = ', note_gen_out.get_shape())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_align shape = :  (?, ?, ?, ?, ?)\n",
      "Note_State_Batch_align shape = :  (?, ?, ?, ?)\n"
     ]
    }
   ],
   "source": [
    "# Loss Function and Optimizer\n",
    "\n",
    "#y_out_val = np.random.randn(1, 78, 128, 2, 2)*5\n",
    "\n",
    "\n",
    "loss, cross_entropy = Loss_Function(Note_State_Batch, y_out)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate = 1).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    sess.run(tf.global_variables_initializer())\n",
    "#    cross_entropy_out, loss_out = sess.run([cross_entropy, loss], feed_dict={y_out: y_out_val, Note_State_Batch: batch_input_state})\n",
    "#print('cross entropy shape = ', cross_entropy_out.shape)\n",
    "#print('loss = ', loss_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining new batch of pieces\n",
      "epoch =  0 ; loss =  0.700679\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1 ; loss =  0.673665\n",
      "epoch =  2 ; loss =  0.660626\n",
      "epoch =  3 ; loss =  0.651752\n",
      "epoch =  4 ; loss =  0.644578\n",
      "epoch =  5 ; loss =  0.636394\n",
      "epoch =  6 ; loss =  0.630118\n",
      "epoch =  7 ; loss =  0.622833\n",
      "epoch =  8 ; loss =  0.616701\n",
      "epoch =  9 ; loss =  0.610311\n",
      "epoch =  10 ; loss =  0.604745\n",
      "epoch =  11 ; loss =  0.598922\n",
      "epoch =  12 ; loss =  0.592865\n",
      "epoch =  13 ; loss =  0.584081\n",
      "epoch =  14 ; loss =  0.577033\n",
      "epoch =  15 ; loss =  0.568621\n",
      "epoch =  16 ; loss =  0.561925\n",
      "epoch =  17 ; loss =  0.554821\n",
      "epoch =  18 ; loss =  0.549946\n",
      "epoch =  19 ; loss =  0.541312\n",
      "epoch =  20 ; loss =  0.537765\n",
      "epoch =  21 ; loss =  0.527829\n",
      "epoch =  22 ; loss =  0.520507\n",
      "epoch =  23 ; loss =  0.509987\n",
      "epoch =  24 ; loss =  0.502219\n",
      "epoch =  25 ; loss =  0.492965\n",
      "epoch =  26 ; loss =  0.486493\n",
      "epoch =  27 ; loss =  0.477654\n",
      "epoch =  28 ; loss =  0.475338\n",
      "epoch =  29 ; loss =  0.46455\n",
      "epoch =  30 ; loss =  0.460176\n",
      "epoch =  31 ; loss =  0.448973\n",
      "epoch =  32 ; loss =  0.442733\n",
      "epoch =  33 ; loss =  0.432547\n",
      "epoch =  34 ; loss =  0.426394\n",
      "epoch =  35 ; loss =  0.417217\n",
      "epoch =  36 ; loss =  0.411906\n",
      "epoch =  37 ; loss =  0.401079\n",
      "epoch =  38 ; loss =  0.396137\n",
      "epoch =  39 ; loss =  0.386314\n",
      "epoch =  40 ; loss =  0.382604\n",
      "epoch =  41 ; loss =  0.372745\n",
      "epoch =  42 ; loss =  0.369627\n",
      "epoch =  43 ; loss =  0.358567\n",
      "epoch =  44 ; loss =  0.354809\n",
      "epoch =  45 ; loss =  0.345495\n",
      "epoch =  46 ; loss =  0.343109\n",
      "epoch =  47 ; loss =  0.333986\n",
      "epoch =  48 ; loss =  0.330391\n",
      "epoch =  49 ; loss =  0.323215\n",
      "epoch =  50 ; loss =  0.320763\n",
      "epoch =  51 ; loss =  0.310226\n",
      "epoch =  52 ; loss =  0.306024\n",
      "epoch =  53 ; loss =  0.300233\n",
      "epoch =  54 ; loss =  0.296154\n",
      "epoch =  55 ; loss =  0.289327\n",
      "epoch =  56 ; loss =  0.284484\n",
      "epoch =  57 ; loss =  0.279782\n",
      "epoch =  58 ; loss =  0.276187\n",
      "epoch =  59 ; loss =  0.270981\n",
      "epoch =  60 ; loss =  0.269199\n",
      "epoch =  61 ; loss =  0.263677\n",
      "epoch =  62 ; loss =  0.263051\n",
      "epoch =  63 ; loss =  0.255828\n",
      "epoch =  64 ; loss =  0.253099\n",
      "epoch =  65 ; loss =  0.247159\n",
      "epoch =  66 ; loss =  0.245373\n",
      "epoch =  67 ; loss =  0.240144\n",
      "epoch =  68 ; loss =  0.238165\n",
      "epoch =  69 ; loss =  0.235021\n",
      "epoch =  70 ; loss =  0.232726\n",
      "epoch =  71 ; loss =  0.229989\n",
      "epoch =  72 ; loss =  0.226485\n",
      "epoch =  73 ; loss =  0.22543\n",
      "epoch =  74 ; loss =  0.224046\n",
      "epoch =  75 ; loss =  0.220019\n",
      "epoch =  76 ; loss =  0.21834\n",
      "epoch =  77 ; loss =  0.214718\n",
      "epoch =  78 ; loss =  0.213996\n",
      "epoch =  79 ; loss =  0.21149\n",
      "epoch =  80 ; loss =  0.20868\n",
      "epoch =  81 ; loss =  0.206935\n",
      "epoch =  82 ; loss =  0.204519\n",
      "epoch =  83 ; loss =  0.202016\n",
      "epoch =  84 ; loss =  0.200375\n",
      "epoch =  85 ; loss =  0.199321\n",
      "epoch =  86 ; loss =  0.197235\n",
      "epoch =  87 ; loss =  0.195413\n",
      "epoch =  88 ; loss =  0.19501\n",
      "epoch =  89 ; loss =  0.192964\n",
      "epoch =  90 ; loss =  0.192735\n",
      "epoch =  91 ; loss =  0.189398\n",
      "epoch =  92 ; loss =  0.189455\n",
      "epoch =  93 ; loss =  0.188013\n",
      "epoch =  94 ; loss =  0.186771\n",
      "epoch =  95 ; loss =  0.186168\n",
      "epoch =  96 ; loss =  0.183956\n",
      "epoch =  97 ; loss =  0.182216\n",
      "epoch =  98 ; loss =  0.182412\n",
      "epoch =  99 ; loss =  0.182064\n",
      "Obtaining new batch of pieces\n",
      "epoch =  100 ; loss =  0.169489\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  101 ; loss =  0.168091\n",
      "epoch =  102 ; loss =  0.166969\n",
      "epoch =  103 ; loss =  0.165803\n",
      "epoch =  104 ; loss =  0.164457\n",
      "epoch =  105 ; loss =  0.163858\n",
      "epoch =  106 ; loss =  0.163876\n",
      "epoch =  107 ; loss =  0.162459\n",
      "epoch =  108 ; loss =  0.162205\n",
      "epoch =  109 ; loss =  0.160644\n",
      "epoch =  110 ; loss =  0.159266\n",
      "epoch =  111 ; loss =  0.157346\n",
      "epoch =  112 ; loss =  0.156199\n",
      "epoch =  113 ; loss =  0.156078\n",
      "epoch =  114 ; loss =  0.1553\n",
      "epoch =  115 ; loss =  0.1542\n",
      "epoch =  116 ; loss =  0.153415\n",
      "epoch =  117 ; loss =  0.152264\n",
      "epoch =  118 ; loss =  0.151696\n",
      "epoch =  119 ; loss =  0.149704\n",
      "epoch =  120 ; loss =  0.150863\n",
      "epoch =  121 ; loss =  0.148786\n",
      "epoch =  122 ; loss =  0.149401\n",
      "epoch =  123 ; loss =  0.148763\n",
      "epoch =  124 ; loss =  0.148141\n",
      "epoch =  125 ; loss =  0.146984\n",
      "epoch =  126 ; loss =  0.146066\n",
      "epoch =  127 ; loss =  0.145833\n",
      "epoch =  128 ; loss =  0.145272\n",
      "epoch =  129 ; loss =  0.145899\n",
      "epoch =  130 ; loss =  0.145096\n",
      "epoch =  131 ; loss =  0.143654\n",
      "epoch =  132 ; loss =  0.143855\n",
      "epoch =  133 ; loss =  0.143208\n",
      "epoch =  134 ; loss =  0.142255\n",
      "epoch =  135 ; loss =  0.142278\n",
      "epoch =  136 ; loss =  0.141692\n",
      "epoch =  137 ; loss =  0.141441\n",
      "epoch =  138 ; loss =  0.14007\n",
      "epoch =  139 ; loss =  0.140924\n",
      "epoch =  140 ; loss =  0.139509\n",
      "epoch =  141 ; loss =  0.139208\n",
      "epoch =  142 ; loss =  0.139238\n",
      "epoch =  143 ; loss =  0.138419\n",
      "epoch =  144 ; loss =  0.138313\n",
      "epoch =  145 ; loss =  0.137118\n",
      "epoch =  146 ; loss =  0.137315\n",
      "epoch =  147 ; loss =  0.137468\n",
      "epoch =  148 ; loss =  0.135698\n",
      "epoch =  149 ; loss =  0.135965\n",
      "epoch =  150 ; loss =  0.135009\n",
      "epoch =  151 ; loss =  0.134875\n",
      "epoch =  152 ; loss =  0.134801\n",
      "epoch =  153 ; loss =  0.134623\n",
      "epoch =  154 ; loss =  0.134595\n",
      "epoch =  155 ; loss =  0.134593\n",
      "epoch =  156 ; loss =  0.13382\n",
      "epoch =  157 ; loss =  0.13247\n",
      "epoch =  158 ; loss =  0.133002\n",
      "epoch =  159 ; loss =  0.131194\n",
      "epoch =  160 ; loss =  0.132529\n",
      "epoch =  161 ; loss =  0.131498\n",
      "epoch =  162 ; loss =  0.132061\n",
      "epoch =  163 ; loss =  0.130803\n",
      "epoch =  164 ; loss =  0.132104\n",
      "epoch =  165 ; loss =  0.131614\n",
      "epoch =  166 ; loss =  0.13058\n",
      "epoch =  167 ; loss =  0.130456\n",
      "epoch =  168 ; loss =  0.130458\n",
      "epoch =  169 ; loss =  0.129657\n",
      "epoch =  170 ; loss =  0.130097\n",
      "epoch =  171 ; loss =  0.129799\n",
      "epoch =  172 ; loss =  0.12846\n",
      "epoch =  173 ; loss =  0.128648\n",
      "epoch =  174 ; loss =  0.128823\n",
      "epoch =  175 ; loss =  0.127598\n",
      "epoch =  176 ; loss =  0.127298\n",
      "epoch =  177 ; loss =  0.128943\n",
      "epoch =  178 ; loss =  0.127143\n",
      "epoch =  179 ; loss =  0.127191\n",
      "epoch =  180 ; loss =  0.127086\n",
      "epoch =  181 ; loss =  0.126984\n",
      "epoch =  182 ; loss =  0.126477\n",
      "epoch =  183 ; loss =  0.126472\n",
      "epoch =  184 ; loss =  0.12563\n",
      "epoch =  185 ; loss =  0.125403\n",
      "epoch =  186 ; loss =  0.126088\n",
      "epoch =  187 ; loss =  0.124728\n",
      "epoch =  188 ; loss =  0.125133\n",
      "epoch =  189 ; loss =  0.12422\n",
      "epoch =  190 ; loss =  0.125675\n",
      "epoch =  191 ; loss =  0.125353\n",
      "epoch =  192 ; loss =  0.125129\n",
      "epoch =  193 ; loss =  0.123918\n",
      "epoch =  194 ; loss =  0.123677\n",
      "epoch =  195 ; loss =  0.124941\n",
      "epoch =  196 ; loss =  0.12345\n",
      "epoch =  197 ; loss =  0.124704\n",
      "epoch =  198 ; loss =  0.122993\n",
      "epoch =  199 ; loss =  0.122774\n",
      "Obtaining new batch of pieces\n",
      "epoch =  200 ; loss =  0.14011\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  201 ; loss =  0.140241\n",
      "epoch =  202 ; loss =  0.139057\n",
      "epoch =  203 ; loss =  0.139087\n",
      "epoch =  204 ; loss =  0.137925\n",
      "epoch =  205 ; loss =  0.138813\n",
      "epoch =  206 ; loss =  0.137552\n",
      "epoch =  207 ; loss =  0.138063\n",
      "epoch =  208 ; loss =  0.137906\n",
      "epoch =  209 ; loss =  0.139122\n",
      "epoch =  210 ; loss =  0.138281\n",
      "epoch =  211 ; loss =  0.139053\n",
      "epoch =  212 ; loss =  0.137843\n",
      "epoch =  213 ; loss =  0.137332\n",
      "epoch =  214 ; loss =  0.136635\n",
      "epoch =  215 ; loss =  0.136686\n",
      "epoch =  216 ; loss =  0.136274\n",
      "epoch =  217 ; loss =  0.135746\n",
      "epoch =  218 ; loss =  0.135837\n",
      "epoch =  219 ; loss =  0.136258\n",
      "epoch =  220 ; loss =  0.134839\n",
      "epoch =  221 ; loss =  0.135219\n",
      "epoch =  222 ; loss =  0.134477\n",
      "epoch =  223 ; loss =  0.135671\n",
      "epoch =  224 ; loss =  0.136159\n",
      "epoch =  225 ; loss =  0.134441\n",
      "epoch =  226 ; loss =  0.134726\n",
      "epoch =  227 ; loss =  0.134571\n",
      "epoch =  228 ; loss =  0.13469\n",
      "epoch =  229 ; loss =  0.135607\n",
      "epoch =  230 ; loss =  0.133109\n",
      "epoch =  231 ; loss =  0.133847\n",
      "epoch =  232 ; loss =  0.133365\n",
      "epoch =  233 ; loss =  0.134365\n",
      "epoch =  234 ; loss =  0.133309\n",
      "epoch =  235 ; loss =  0.133582\n",
      "epoch =  236 ; loss =  0.131803\n",
      "epoch =  237 ; loss =  0.132348\n",
      "epoch =  238 ; loss =  0.132189\n",
      "epoch =  239 ; loss =  0.132375\n",
      "epoch =  240 ; loss =  0.132745\n",
      "epoch =  241 ; loss =  0.131528\n",
      "epoch =  242 ; loss =  0.132222\n",
      "epoch =  243 ; loss =  0.13102\n",
      "epoch =  244 ; loss =  0.13077\n",
      "epoch =  245 ; loss =  0.130966\n",
      "epoch =  246 ; loss =  0.130839\n",
      "epoch =  247 ; loss =  0.130778\n",
      "epoch =  248 ; loss =  0.131136\n",
      "epoch =  249 ; loss =  0.130456\n",
      "epoch =  250 ; loss =  0.130475\n",
      "epoch =  251 ; loss =  0.12969\n",
      "epoch =  252 ; loss =  0.130291\n",
      "epoch =  253 ; loss =  0.130459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  254 ; loss =  0.129366\n",
      "epoch =  255 ; loss =  0.13058\n",
      "epoch =  256 ; loss =  0.129295\n",
      "epoch =  257 ; loss =  0.130484\n",
      "epoch =  258 ; loss =  0.129639\n",
      "epoch =  259 ; loss =  0.129121\n",
      "epoch =  260 ; loss =  0.129772\n",
      "epoch =  261 ; loss =  0.128683\n",
      "epoch =  262 ; loss =  0.129\n",
      "epoch =  263 ; loss =  0.127683\n",
      "epoch =  264 ; loss =  0.128425\n",
      "epoch =  265 ; loss =  0.129105\n",
      "epoch =  266 ; loss =  0.128038\n",
      "epoch =  267 ; loss =  0.128814\n",
      "epoch =  268 ; loss =  0.128421\n",
      "epoch =  269 ; loss =  0.128601\n",
      "epoch =  270 ; loss =  0.128307\n",
      "epoch =  271 ; loss =  0.128924\n",
      "epoch =  272 ; loss =  0.129157\n",
      "epoch =  273 ; loss =  0.129081\n",
      "epoch =  274 ; loss =  0.129537\n",
      "epoch =  275 ; loss =  0.129511\n",
      "epoch =  276 ; loss =  0.128931\n",
      "epoch =  277 ; loss =  0.127514\n",
      "epoch =  278 ; loss =  0.127568\n",
      "epoch =  279 ; loss =  0.126025\n",
      "epoch =  280 ; loss =  0.12671\n",
      "epoch =  281 ; loss =  0.126252\n",
      "epoch =  282 ; loss =  0.126653\n",
      "epoch =  283 ; loss =  0.12598\n",
      "epoch =  284 ; loss =  0.126192\n",
      "epoch =  285 ; loss =  0.126359\n",
      "epoch =  286 ; loss =  0.126256\n",
      "epoch =  287 ; loss =  0.126444\n",
      "epoch =  288 ; loss =  0.125473\n",
      "epoch =  289 ; loss =  0.125796\n",
      "epoch =  290 ; loss =  0.124976\n",
      "epoch =  291 ; loss =  0.126395\n",
      "epoch =  292 ; loss =  0.125427\n",
      "epoch =  293 ; loss =  0.125654\n",
      "epoch =  294 ; loss =  0.126228\n",
      "epoch =  295 ; loss =  0.125037\n",
      "epoch =  296 ; loss =  0.125612\n",
      "epoch =  297 ; loss =  0.124772\n",
      "epoch =  298 ; loss =  0.125304\n",
      "epoch =  299 ; loss =  0.1248\n",
      "Obtaining new batch of pieces\n",
      "epoch =  300 ; loss =  0.130766\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  301 ; loss =  0.129797\n",
      "epoch =  302 ; loss =  0.129645\n",
      "epoch =  303 ; loss =  0.129551\n",
      "epoch =  304 ; loss =  0.128853\n",
      "epoch =  305 ; loss =  0.129022\n",
      "epoch =  306 ; loss =  0.12911\n",
      "epoch =  307 ; loss =  0.128988\n",
      "epoch =  308 ; loss =  0.128183\n",
      "epoch =  309 ; loss =  0.127981\n",
      "epoch =  310 ; loss =  0.128691\n",
      "epoch =  311 ; loss =  0.128017\n",
      "epoch =  312 ; loss =  0.127806\n",
      "epoch =  313 ; loss =  0.128427\n",
      "epoch =  314 ; loss =  0.128925\n",
      "epoch =  315 ; loss =  0.127569\n",
      "epoch =  316 ; loss =  0.128463\n",
      "epoch =  317 ; loss =  0.12737\n",
      "epoch =  318 ; loss =  0.128046\n",
      "epoch =  319 ; loss =  0.126468\n",
      "epoch =  320 ; loss =  0.12759\n",
      "epoch =  321 ; loss =  0.127672\n",
      "epoch =  322 ; loss =  0.127532\n",
      "epoch =  323 ; loss =  0.127288\n",
      "epoch =  324 ; loss =  0.127381\n",
      "epoch =  325 ; loss =  0.127105\n",
      "epoch =  326 ; loss =  0.126537\n",
      "epoch =  327 ; loss =  0.126202\n",
      "epoch =  328 ; loss =  0.126789\n",
      "epoch =  329 ; loss =  0.126902\n",
      "epoch =  330 ; loss =  0.126587\n",
      "epoch =  331 ; loss =  0.126393\n",
      "epoch =  332 ; loss =  0.125916\n",
      "epoch =  333 ; loss =  0.125909\n",
      "epoch =  334 ; loss =  0.126341\n",
      "epoch =  335 ; loss =  0.126361\n",
      "epoch =  336 ; loss =  0.125605\n",
      "epoch =  337 ; loss =  0.125836\n",
      "epoch =  338 ; loss =  0.126124\n",
      "epoch =  339 ; loss =  0.126083\n",
      "epoch =  340 ; loss =  0.126877\n",
      "epoch =  341 ; loss =  0.128764\n",
      "epoch =  342 ; loss =  0.130326\n",
      "epoch =  343 ; loss =  0.126195\n",
      "epoch =  344 ; loss =  0.124905\n",
      "epoch =  345 ; loss =  0.126092\n",
      "epoch =  346 ; loss =  0.125139\n",
      "epoch =  347 ; loss =  0.125516\n",
      "epoch =  348 ; loss =  0.124851\n",
      "epoch =  349 ; loss =  0.125244\n",
      "epoch =  350 ; loss =  0.125167\n",
      "epoch =  351 ; loss =  0.12484\n",
      "epoch =  352 ; loss =  0.12444\n",
      "epoch =  353 ; loss =  0.123765\n",
      "epoch =  354 ; loss =  0.123376\n",
      "epoch =  355 ; loss =  0.124747\n",
      "epoch =  356 ; loss =  0.124933\n",
      "epoch =  357 ; loss =  0.124022\n",
      "epoch =  358 ; loss =  0.124247\n",
      "epoch =  359 ; loss =  0.124674\n",
      "epoch =  360 ; loss =  0.124387\n",
      "epoch =  361 ; loss =  0.124094\n",
      "epoch =  362 ; loss =  0.124273\n",
      "epoch =  363 ; loss =  0.124053\n",
      "epoch =  364 ; loss =  0.123883\n",
      "epoch =  365 ; loss =  0.123648\n",
      "epoch =  366 ; loss =  0.124759\n",
      "epoch =  367 ; loss =  0.12435\n",
      "epoch =  368 ; loss =  0.124285\n",
      "epoch =  369 ; loss =  0.123153\n",
      "epoch =  370 ; loss =  0.124086\n",
      "epoch =  371 ; loss =  0.12377\n",
      "epoch =  372 ; loss =  0.124211\n",
      "epoch =  373 ; loss =  0.123159\n",
      "epoch =  374 ; loss =  0.122946\n",
      "epoch =  375 ; loss =  0.123466\n",
      "epoch =  376 ; loss =  0.122491\n",
      "epoch =  377 ; loss =  0.123009\n",
      "epoch =  378 ; loss =  0.12385\n",
      "epoch =  379 ; loss =  0.122981\n",
      "epoch =  380 ; loss =  0.123863\n",
      "epoch =  381 ; loss =  0.123053\n",
      "epoch =  382 ; loss =  0.122127\n",
      "epoch =  383 ; loss =  0.123751\n",
      "epoch =  384 ; loss =  0.122236\n",
      "epoch =  385 ; loss =  0.12235\n",
      "epoch =  386 ; loss =  0.122518\n",
      "epoch =  387 ; loss =  0.121797\n",
      "epoch =  388 ; loss =  0.12254\n",
      "epoch =  389 ; loss =  0.122348\n",
      "epoch =  390 ; loss =  0.122294\n",
      "epoch =  391 ; loss =  0.122149\n",
      "epoch =  392 ; loss =  0.122476\n",
      "epoch =  393 ; loss =  0.122717\n",
      "epoch =  394 ; loss =  0.122794\n",
      "epoch =  395 ; loss =  0.122036\n",
      "epoch =  396 ; loss =  0.122293\n",
      "epoch =  397 ; loss =  0.122109\n",
      "epoch =  398 ; loss =  0.12143\n",
      "epoch =  399 ; loss =  0.122009\n",
      "Obtaining new batch of pieces\n",
      "epoch =  400 ; loss =  0.130067\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  401 ; loss =  0.130086\n",
      "epoch =  402 ; loss =  0.129667\n",
      "epoch =  403 ; loss =  0.130563\n",
      "epoch =  404 ; loss =  0.129875\n",
      "epoch =  405 ; loss =  0.129233\n",
      "epoch =  406 ; loss =  0.129789\n",
      "epoch =  407 ; loss =  0.12974\n",
      "epoch =  408 ; loss =  0.132183\n",
      "epoch =  409 ; loss =  0.130002\n",
      "epoch =  410 ; loss =  0.129557\n",
      "epoch =  411 ; loss =  0.129273\n",
      "epoch =  412 ; loss =  0.129084\n",
      "epoch =  413 ; loss =  0.129279\n",
      "epoch =  414 ; loss =  0.128979\n",
      "epoch =  415 ; loss =  0.127883\n",
      "epoch =  416 ; loss =  0.128863\n",
      "epoch =  417 ; loss =  0.127663\n",
      "epoch =  418 ; loss =  0.128127\n",
      "epoch =  419 ; loss =  0.127526\n",
      "epoch =  420 ; loss =  0.127717\n",
      "epoch =  421 ; loss =  0.12744\n",
      "epoch =  422 ; loss =  0.126971\n",
      "epoch =  423 ; loss =  0.128793\n",
      "epoch =  424 ; loss =  0.128531\n",
      "epoch =  425 ; loss =  0.126889\n",
      "epoch =  426 ; loss =  0.127105\n",
      "epoch =  427 ; loss =  0.127804\n",
      "epoch =  428 ; loss =  0.12786\n",
      "epoch =  429 ; loss =  0.126802\n",
      "epoch =  430 ; loss =  0.126905\n",
      "epoch =  431 ; loss =  0.127358\n",
      "epoch =  432 ; loss =  0.127315\n",
      "epoch =  433 ; loss =  0.126991\n",
      "epoch =  434 ; loss =  0.127296\n",
      "epoch =  435 ; loss =  0.126725\n",
      "epoch =  436 ; loss =  0.127413\n",
      "epoch =  437 ; loss =  0.126133\n",
      "epoch =  438 ; loss =  0.126919\n",
      "epoch =  439 ; loss =  0.127138\n",
      "epoch =  440 ; loss =  0.126712\n",
      "epoch =  441 ; loss =  0.130215\n",
      "epoch =  442 ; loss =  0.130991\n",
      "epoch =  443 ; loss =  0.125637\n",
      "epoch =  444 ; loss =  0.1261\n",
      "epoch =  445 ; loss =  0.126157\n",
      "epoch =  446 ; loss =  0.126322\n",
      "epoch =  447 ; loss =  0.125273\n",
      "epoch =  448 ; loss =  0.125914\n",
      "epoch =  449 ; loss =  0.126343\n",
      "epoch =  450 ; loss =  0.126412\n",
      "epoch =  451 ; loss =  0.125965\n",
      "epoch =  452 ; loss =  0.125336\n",
      "epoch =  453 ; loss =  0.125598\n",
      "epoch =  454 ; loss =  0.124628\n",
      "epoch =  455 ; loss =  0.125529\n",
      "epoch =  456 ; loss =  0.125277\n",
      "epoch =  457 ; loss =  0.125767\n",
      "epoch =  458 ; loss =  0.1258\n",
      "epoch =  459 ; loss =  0.125189\n",
      "epoch =  460 ; loss =  0.124783\n",
      "epoch =  461 ; loss =  0.125507\n",
      "epoch =  462 ; loss =  0.12536\n",
      "epoch =  463 ; loss =  0.124872\n",
      "epoch =  464 ; loss =  0.124567\n",
      "epoch =  465 ; loss =  0.125043\n",
      "epoch =  466 ; loss =  0.124641\n",
      "epoch =  467 ; loss =  0.124874\n",
      "epoch =  468 ; loss =  0.125744\n",
      "epoch =  469 ; loss =  0.127576\n",
      "epoch =  470 ; loss =  0.125708\n",
      "epoch =  471 ; loss =  0.126721\n",
      "epoch =  472 ; loss =  0.126604\n",
      "epoch =  473 ; loss =  0.129157\n",
      "epoch =  474 ; loss =  0.124665\n",
      "epoch =  475 ; loss =  0.125162\n",
      "epoch =  476 ; loss =  0.124009\n",
      "epoch =  477 ; loss =  0.12483\n",
      "epoch =  478 ; loss =  0.123872\n",
      "epoch =  479 ; loss =  0.124341\n",
      "epoch =  480 ; loss =  0.124089\n",
      "epoch =  481 ; loss =  0.124393\n",
      "epoch =  482 ; loss =  0.123891\n",
      "epoch =  483 ; loss =  0.124262\n",
      "epoch =  484 ; loss =  0.123903\n",
      "epoch =  485 ; loss =  0.124102\n",
      "epoch =  486 ; loss =  0.124144\n",
      "epoch =  487 ; loss =  0.123707\n",
      "epoch =  488 ; loss =  0.12379\n",
      "epoch =  489 ; loss =  0.123362\n",
      "epoch =  490 ; loss =  0.123707\n",
      "epoch =  491 ; loss =  0.123988\n",
      "epoch =  492 ; loss =  0.124635\n",
      "epoch =  493 ; loss =  0.123979\n",
      "epoch =  494 ; loss =  0.124127\n",
      "epoch =  495 ; loss =  0.123849\n",
      "epoch =  496 ; loss =  0.124718\n",
      "epoch =  497 ; loss =  0.124024\n",
      "epoch =  498 ; loss =  0.123566\n",
      "epoch =  499 ; loss =  0.123395\n",
      "Obtaining new batch of pieces\n",
      "epoch =  500 ; loss =  0.122195\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  501 ; loss =  0.121556\n",
      "epoch =  502 ; loss =  0.121226\n",
      "epoch =  503 ; loss =  0.121643\n",
      "epoch =  504 ; loss =  0.12142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  505 ; loss =  0.121841\n",
      "epoch =  506 ; loss =  0.121178\n",
      "epoch =  507 ; loss =  0.120758\n",
      "epoch =  508 ; loss =  0.120856\n",
      "epoch =  509 ; loss =  0.120556\n",
      "epoch =  510 ; loss =  0.120541\n",
      "epoch =  511 ; loss =  0.119302\n",
      "epoch =  512 ; loss =  0.12033\n",
      "epoch =  513 ; loss =  0.120527\n",
      "epoch =  514 ; loss =  0.120566\n",
      "epoch =  515 ; loss =  0.120391\n",
      "epoch =  516 ; loss =  0.12028\n",
      "epoch =  517 ; loss =  0.120644\n",
      "epoch =  518 ; loss =  0.120176\n",
      "epoch =  519 ; loss =  0.120374\n",
      "epoch =  520 ; loss =  0.119606\n",
      "epoch =  521 ; loss =  0.119878\n",
      "epoch =  522 ; loss =  0.12011\n",
      "epoch =  523 ; loss =  0.119842\n",
      "epoch =  524 ; loss =  0.120814\n",
      "epoch =  525 ; loss =  0.121577\n",
      "epoch =  526 ; loss =  0.120465\n",
      "epoch =  527 ; loss =  0.119572\n",
      "epoch =  528 ; loss =  0.119298\n",
      "epoch =  529 ; loss =  0.118939\n",
      "epoch =  530 ; loss =  0.119143\n",
      "epoch =  531 ; loss =  0.119559\n",
      "epoch =  532 ; loss =  0.119749\n",
      "epoch =  533 ; loss =  0.119586\n",
      "epoch =  534 ; loss =  0.119405\n",
      "epoch =  535 ; loss =  0.119285\n",
      "epoch =  536 ; loss =  0.11978\n",
      "epoch =  537 ; loss =  0.119341\n",
      "epoch =  538 ; loss =  0.119604\n",
      "epoch =  539 ; loss =  0.119351\n",
      "epoch =  540 ; loss =  0.119482\n",
      "epoch =  541 ; loss =  0.119485\n",
      "epoch =  542 ; loss =  0.119856\n",
      "epoch =  543 ; loss =  0.119862\n",
      "epoch =  544 ; loss =  0.118864\n",
      "epoch =  545 ; loss =  0.118749\n",
      "epoch =  546 ; loss =  0.119426\n",
      "epoch =  547 ; loss =  0.118634\n",
      "epoch =  548 ; loss =  0.118834\n",
      "epoch =  549 ; loss =  0.118742\n",
      "epoch =  550 ; loss =  0.118888\n",
      "epoch =  551 ; loss =  0.118927\n",
      "epoch =  552 ; loss =  0.118547\n",
      "epoch =  553 ; loss =  0.118727\n",
      "epoch =  554 ; loss =  0.11897\n",
      "epoch =  555 ; loss =  0.118646\n",
      "epoch =  556 ; loss =  0.118952\n",
      "epoch =  557 ; loss =  0.118797\n",
      "epoch =  558 ; loss =  0.118478\n",
      "epoch =  559 ; loss =  0.118398\n",
      "epoch =  560 ; loss =  0.119044\n",
      "epoch =  561 ; loss =  0.11806\n",
      "epoch =  562 ; loss =  0.118263\n",
      "epoch =  563 ; loss =  0.118001\n",
      "epoch =  564 ; loss =  0.118863\n",
      "epoch =  565 ; loss =  0.118461\n",
      "epoch =  566 ; loss =  0.11846\n",
      "epoch =  567 ; loss =  0.118617\n",
      "epoch =  568 ; loss =  0.118728\n",
      "epoch =  569 ; loss =  0.118813\n",
      "epoch =  570 ; loss =  0.118906\n",
      "epoch =  571 ; loss =  0.118828\n",
      "epoch =  572 ; loss =  0.118661\n",
      "epoch =  573 ; loss =  0.119122\n",
      "epoch =  574 ; loss =  0.11776\n",
      "epoch =  575 ; loss =  0.117763\n",
      "epoch =  576 ; loss =  0.118077\n",
      "epoch =  577 ; loss =  0.118453\n",
      "epoch =  578 ; loss =  0.118168\n",
      "epoch =  579 ; loss =  0.118375\n",
      "epoch =  580 ; loss =  0.117835\n",
      "epoch =  581 ; loss =  0.117046\n",
      "epoch =  582 ; loss =  0.116862\n",
      "epoch =  583 ; loss =  0.117699\n",
      "epoch =  584 ; loss =  0.118026\n",
      "epoch =  585 ; loss =  0.117944\n",
      "epoch =  586 ; loss =  0.118287\n",
      "epoch =  587 ; loss =  0.117314\n",
      "epoch =  588 ; loss =  0.117693\n",
      "epoch =  589 ; loss =  0.11808\n",
      "epoch =  590 ; loss =  0.118276\n",
      "epoch =  591 ; loss =  0.117945\n",
      "epoch =  592 ; loss =  0.11753\n",
      "epoch =  593 ; loss =  0.117016\n",
      "epoch =  594 ; loss =  0.116856\n",
      "epoch =  595 ; loss =  0.117087\n",
      "epoch =  596 ; loss =  0.118134\n",
      "epoch =  597 ; loss =  0.118025\n",
      "epoch =  598 ; loss =  0.118993\n",
      "epoch =  599 ; loss =  0.116946\n",
      "Obtaining new batch of pieces\n",
      "epoch =  600 ; loss =  0.107783\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  601 ; loss =  0.107572\n",
      "epoch =  602 ; loss =  0.107629\n",
      "epoch =  603 ; loss =  0.107001\n",
      "epoch =  604 ; loss =  0.107161\n",
      "epoch =  605 ; loss =  0.107222\n",
      "epoch =  606 ; loss =  0.106904\n",
      "epoch =  607 ; loss =  0.107233\n",
      "epoch =  608 ; loss =  0.10695\n",
      "epoch =  609 ; loss =  0.106334\n",
      "epoch =  610 ; loss =  0.106815\n",
      "epoch =  611 ; loss =  0.106668\n",
      "epoch =  612 ; loss =  0.1064\n",
      "epoch =  613 ; loss =  0.106197\n",
      "epoch =  614 ; loss =  0.106178\n",
      "epoch =  615 ; loss =  0.106819\n",
      "epoch =  616 ; loss =  0.106129\n",
      "epoch =  617 ; loss =  0.106806\n",
      "epoch =  618 ; loss =  0.106052\n",
      "epoch =  619 ; loss =  0.105394\n",
      "epoch =  620 ; loss =  0.105268\n",
      "epoch =  621 ; loss =  0.105619\n",
      "epoch =  622 ; loss =  0.105891\n",
      "epoch =  623 ; loss =  0.105735\n",
      "epoch =  624 ; loss =  0.105297\n",
      "epoch =  625 ; loss =  0.106213\n",
      "epoch =  626 ; loss =  0.105698\n",
      "epoch =  627 ; loss =  0.105943\n",
      "epoch =  628 ; loss =  0.105314\n",
      "epoch =  629 ; loss =  0.105407\n",
      "epoch =  630 ; loss =  0.104761\n",
      "epoch =  631 ; loss =  0.105719\n",
      "epoch =  632 ; loss =  0.105343\n",
      "epoch =  633 ; loss =  0.105745\n",
      "epoch =  634 ; loss =  0.105319\n",
      "epoch =  635 ; loss =  0.104565\n",
      "epoch =  636 ; loss =  0.104667\n",
      "epoch =  637 ; loss =  0.104492\n",
      "epoch =  638 ; loss =  0.104975\n",
      "epoch =  639 ; loss =  0.10526\n",
      "epoch =  640 ; loss =  0.104946\n",
      "epoch =  641 ; loss =  0.105487\n",
      "epoch =  642 ; loss =  0.104938\n",
      "epoch =  643 ; loss =  0.104982\n",
      "epoch =  644 ; loss =  0.105511\n",
      "epoch =  645 ; loss =  0.104811\n",
      "epoch =  646 ; loss =  0.106144\n",
      "epoch =  647 ; loss =  0.104336\n",
      "epoch =  648 ; loss =  0.104966\n",
      "epoch =  649 ; loss =  0.104637\n",
      "epoch =  650 ; loss =  0.104952\n",
      "epoch =  651 ; loss =  0.104247\n",
      "epoch =  652 ; loss =  0.105311\n",
      "epoch =  653 ; loss =  0.104641\n",
      "epoch =  654 ; loss =  0.104898\n",
      "epoch =  655 ; loss =  0.10438\n",
      "epoch =  656 ; loss =  0.104483\n",
      "epoch =  657 ; loss =  0.104522\n",
      "epoch =  658 ; loss =  0.104024\n",
      "epoch =  659 ; loss =  0.104608\n",
      "epoch =  660 ; loss =  0.103711\n",
      "epoch =  661 ; loss =  0.103343\n",
      "epoch =  662 ; loss =  0.104947\n",
      "epoch =  663 ; loss =  0.104593\n",
      "epoch =  664 ; loss =  0.103343\n",
      "epoch =  665 ; loss =  0.104398\n",
      "epoch =  666 ; loss =  0.103879\n",
      "epoch =  667 ; loss =  0.104014\n",
      "epoch =  668 ; loss =  0.103845\n",
      "epoch =  669 ; loss =  0.103865\n",
      "epoch =  670 ; loss =  0.104019\n",
      "epoch =  671 ; loss =  0.103378\n",
      "epoch =  672 ; loss =  0.104384\n",
      "epoch =  673 ; loss =  0.104028\n",
      "epoch =  674 ; loss =  0.10373\n",
      "epoch =  675 ; loss =  0.103704\n",
      "epoch =  676 ; loss =  0.104294\n",
      "epoch =  677 ; loss =  0.103828\n",
      "epoch =  678 ; loss =  0.104374\n",
      "epoch =  679 ; loss =  0.103875\n",
      "epoch =  680 ; loss =  0.104565\n",
      "epoch =  681 ; loss =  0.103551\n",
      "epoch =  682 ; loss =  0.10417\n",
      "epoch =  683 ; loss =  0.103808\n",
      "epoch =  684 ; loss =  0.103583\n",
      "epoch =  685 ; loss =  0.103882\n",
      "epoch =  686 ; loss =  0.104326\n",
      "epoch =  687 ; loss =  0.103765\n",
      "epoch =  688 ; loss =  0.103786\n",
      "epoch =  689 ; loss =  0.104298\n",
      "epoch =  690 ; loss =  0.104356\n",
      "epoch =  691 ; loss =  0.103503\n",
      "epoch =  692 ; loss =  0.103859\n",
      "epoch =  693 ; loss =  0.103305\n",
      "epoch =  694 ; loss =  0.103208\n",
      "epoch =  695 ; loss =  0.103464\n",
      "epoch =  696 ; loss =  0.103312\n",
      "epoch =  697 ; loss =  0.103399\n",
      "epoch =  698 ; loss =  0.10343\n",
      "epoch =  699 ; loss =  0.102974\n",
      "Obtaining new batch of pieces\n",
      "epoch =  700 ; loss =  0.114508\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  701 ; loss =  0.114528\n",
      "epoch =  702 ; loss =  0.113928\n",
      "epoch =  703 ; loss =  0.112508\n",
      "epoch =  704 ; loss =  0.113127\n",
      "epoch =  705 ; loss =  0.113372\n",
      "epoch =  706 ; loss =  0.113282\n",
      "epoch =  707 ; loss =  0.114114\n",
      "epoch =  708 ; loss =  0.114825\n",
      "epoch =  709 ; loss =  0.113773\n",
      "epoch =  710 ; loss =  0.115101\n",
      "epoch =  711 ; loss =  0.114021\n",
      "epoch =  712 ; loss =  0.113367\n",
      "epoch =  713 ; loss =  0.113388\n",
      "epoch =  714 ; loss =  0.112138\n",
      "epoch =  715 ; loss =  0.112018\n",
      "epoch =  716 ; loss =  0.112133\n",
      "epoch =  717 ; loss =  0.111943\n",
      "epoch =  718 ; loss =  0.111314\n",
      "epoch =  719 ; loss =  0.111478\n",
      "epoch =  720 ; loss =  0.110934\n",
      "epoch =  721 ; loss =  0.111015\n",
      "epoch =  722 ; loss =  0.111132\n",
      "epoch =  723 ; loss =  0.111112\n",
      "epoch =  724 ; loss =  0.11165\n",
      "epoch =  725 ; loss =  0.111648\n",
      "epoch =  726 ; loss =  0.111482\n",
      "epoch =  727 ; loss =  0.111799\n",
      "epoch =  728 ; loss =  0.111902\n",
      "epoch =  729 ; loss =  0.111247\n",
      "epoch =  730 ; loss =  0.111584\n",
      "epoch =  731 ; loss =  0.111507\n",
      "epoch =  732 ; loss =  0.112265\n",
      "epoch =  733 ; loss =  0.112514\n",
      "epoch =  734 ; loss =  0.112045\n",
      "epoch =  735 ; loss =  0.112111\n",
      "epoch =  736 ; loss =  0.112822\n",
      "epoch =  737 ; loss =  0.111019\n",
      "epoch =  738 ; loss =  0.111309\n",
      "epoch =  739 ; loss =  0.110597\n",
      "epoch =  740 ; loss =  0.111788\n",
      "epoch =  741 ; loss =  0.111323\n",
      "epoch =  742 ; loss =  0.111651\n",
      "epoch =  743 ; loss =  0.110935\n",
      "epoch =  744 ; loss =  0.11118\n",
      "epoch =  745 ; loss =  0.110976\n",
      "epoch =  746 ; loss =  0.110933\n",
      "epoch =  747 ; loss =  0.110794\n",
      "epoch =  748 ; loss =  0.111066\n",
      "epoch =  749 ; loss =  0.110522\n",
      "epoch =  750 ; loss =  0.110908\n",
      "epoch =  751 ; loss =  0.111116\n",
      "epoch =  752 ; loss =  0.111021\n",
      "epoch =  753 ; loss =  0.110683\n",
      "epoch =  754 ; loss =  0.11026\n",
      "epoch =  755 ; loss =  0.11069\n",
      "epoch =  756 ; loss =  0.110962\n",
      "epoch =  757 ; loss =  0.1103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  758 ; loss =  0.110257\n",
      "epoch =  759 ; loss =  0.110378\n",
      "epoch =  760 ; loss =  0.110173\n",
      "epoch =  761 ; loss =  0.11083\n",
      "epoch =  762 ; loss =  0.111014\n",
      "epoch =  763 ; loss =  0.110482\n",
      "epoch =  764 ; loss =  0.110777\n",
      "epoch =  765 ; loss =  0.110668\n",
      "epoch =  766 ; loss =  0.111181\n",
      "epoch =  767 ; loss =  0.111526\n",
      "epoch =  768 ; loss =  0.11106\n",
      "epoch =  769 ; loss =  0.110153\n",
      "epoch =  770 ; loss =  0.110107\n",
      "epoch =  771 ; loss =  0.110788\n",
      "epoch =  772 ; loss =  0.110346\n",
      "epoch =  773 ; loss =  0.110149\n",
      "epoch =  774 ; loss =  0.109449\n",
      "epoch =  775 ; loss =  0.10978\n",
      "epoch =  776 ; loss =  0.110864\n",
      "epoch =  777 ; loss =  0.111269\n",
      "epoch =  778 ; loss =  0.110521\n",
      "epoch =  779 ; loss =  0.110131\n",
      "epoch =  780 ; loss =  0.110477\n",
      "epoch =  781 ; loss =  0.109925\n",
      "epoch =  782 ; loss =  0.111031\n",
      "epoch =  783 ; loss =  0.11084\n",
      "epoch =  784 ; loss =  0.110056\n",
      "epoch =  785 ; loss =  0.110539\n",
      "epoch =  786 ; loss =  0.110038\n",
      "epoch =  787 ; loss =  0.110287\n",
      "epoch =  788 ; loss =  0.110203\n",
      "epoch =  789 ; loss =  0.109525\n",
      "epoch =  790 ; loss =  0.110126\n",
      "epoch =  791 ; loss =  0.110233\n",
      "epoch =  792 ; loss =  0.109818\n",
      "epoch =  793 ; loss =  0.109898\n",
      "epoch =  794 ; loss =  0.109958\n",
      "epoch =  795 ; loss =  0.109321\n",
      "epoch =  796 ; loss =  0.109701\n",
      "epoch =  797 ; loss =  0.109386\n",
      "epoch =  798 ; loss =  0.109678\n",
      "epoch =  799 ; loss =  0.110203\n",
      "Obtaining new batch of pieces\n",
      "epoch =  800 ; loss =  0.112171\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  801 ; loss =  0.112284\n",
      "epoch =  802 ; loss =  0.111985\n",
      "epoch =  803 ; loss =  0.111478\n",
      "epoch =  804 ; loss =  0.110974\n",
      "epoch =  805 ; loss =  0.11109\n",
      "epoch =  806 ; loss =  0.110869\n",
      "epoch =  807 ; loss =  0.110937\n",
      "epoch =  808 ; loss =  0.110543\n",
      "epoch =  809 ; loss =  0.111167\n",
      "epoch =  810 ; loss =  0.11073\n",
      "epoch =  811 ; loss =  0.111284\n",
      "epoch =  812 ; loss =  0.110734\n",
      "epoch =  813 ; loss =  0.1115\n",
      "epoch =  814 ; loss =  0.109972\n",
      "epoch =  815 ; loss =  0.109873\n",
      "epoch =  816 ; loss =  0.11138\n",
      "epoch =  817 ; loss =  0.11413\n",
      "epoch =  818 ; loss =  0.110999\n",
      "epoch =  819 ; loss =  0.112804\n",
      "epoch =  820 ; loss =  0.111564\n",
      "epoch =  821 ; loss =  0.110872\n",
      "epoch =  822 ; loss =  0.110897\n",
      "epoch =  823 ; loss =  0.111475\n",
      "epoch =  824 ; loss =  0.110313\n",
      "epoch =  825 ; loss =  0.110519\n",
      "epoch =  826 ; loss =  0.110746\n",
      "epoch =  827 ; loss =  0.110831\n",
      "epoch =  828 ; loss =  0.110267\n",
      "epoch =  829 ; loss =  0.11007\n",
      "epoch =  830 ; loss =  0.109952\n",
      "epoch =  831 ; loss =  0.109578\n",
      "epoch =  832 ; loss =  0.110045\n",
      "epoch =  833 ; loss =  0.109577\n",
      "epoch =  834 ; loss =  0.109854\n",
      "epoch =  835 ; loss =  0.109994\n",
      "epoch =  836 ; loss =  0.109864\n",
      "epoch =  837 ; loss =  0.10983\n",
      "epoch =  838 ; loss =  0.109533\n",
      "epoch =  839 ; loss =  0.110392\n",
      "epoch =  840 ; loss =  0.10993\n",
      "epoch =  841 ; loss =  0.108978\n",
      "epoch =  842 ; loss =  0.110002\n",
      "epoch =  843 ; loss =  0.110579\n",
      "epoch =  844 ; loss =  0.109542\n",
      "epoch =  845 ; loss =  0.110019\n",
      "epoch =  846 ; loss =  0.109564\n",
      "epoch =  847 ; loss =  0.110105\n",
      "epoch =  848 ; loss =  0.109327\n",
      "epoch =  849 ; loss =  0.110371\n",
      "epoch =  850 ; loss =  0.110146\n",
      "epoch =  851 ; loss =  0.109561\n",
      "epoch =  852 ; loss =  0.109533\n",
      "epoch =  853 ; loss =  0.109036\n",
      "epoch =  854 ; loss =  0.108964\n",
      "epoch =  855 ; loss =  0.109052\n",
      "epoch =  856 ; loss =  0.110705\n",
      "epoch =  857 ; loss =  0.113094\n",
      "epoch =  858 ; loss =  0.110441\n",
      "epoch =  859 ; loss =  0.11045\n",
      "epoch =  860 ; loss =  0.110141\n",
      "epoch =  861 ; loss =  0.109202\n",
      "epoch =  862 ; loss =  0.109494\n",
      "epoch =  863 ; loss =  0.109395\n",
      "epoch =  864 ; loss =  0.109226\n",
      "epoch =  865 ; loss =  0.108228\n",
      "epoch =  866 ; loss =  0.108658\n",
      "epoch =  867 ; loss =  0.108778\n",
      "epoch =  868 ; loss =  0.108638\n",
      "epoch =  869 ; loss =  0.108383\n",
      "epoch =  870 ; loss =  0.108746\n",
      "epoch =  871 ; loss =  0.108781\n",
      "epoch =  872 ; loss =  0.10903\n",
      "epoch =  873 ; loss =  0.109574\n",
      "epoch =  874 ; loss =  0.109123\n",
      "epoch =  875 ; loss =  0.108692\n",
      "epoch =  876 ; loss =  0.108945\n",
      "epoch =  877 ; loss =  0.108689\n",
      "epoch =  878 ; loss =  0.108454\n",
      "epoch =  879 ; loss =  0.10892\n",
      "epoch =  880 ; loss =  0.109483\n",
      "epoch =  881 ; loss =  0.109468\n",
      "epoch =  882 ; loss =  0.108756\n",
      "epoch =  883 ; loss =  0.109296\n",
      "epoch =  884 ; loss =  0.108932\n",
      "epoch =  885 ; loss =  0.107861\n",
      "epoch =  886 ; loss =  0.108738\n",
      "epoch =  887 ; loss =  0.108709\n",
      "epoch =  888 ; loss =  0.108543\n",
      "epoch =  889 ; loss =  0.108084\n",
      "epoch =  890 ; loss =  0.108394\n",
      "epoch =  891 ; loss =  0.10835\n",
      "epoch =  892 ; loss =  0.109285\n",
      "epoch =  893 ; loss =  0.109379\n",
      "epoch =  894 ; loss =  0.109238\n",
      "epoch =  895 ; loss =  0.11005\n",
      "epoch =  896 ; loss =  0.111392\n",
      "epoch =  897 ; loss =  0.113024\n",
      "epoch =  898 ; loss =  0.109514\n",
      "epoch =  899 ; loss =  0.110221\n",
      "Obtaining new batch of pieces\n",
      "epoch =  900 ; loss =  0.110559\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  901 ; loss =  0.110092\n",
      "epoch =  902 ; loss =  0.109715\n",
      "epoch =  903 ; loss =  0.110523\n",
      "epoch =  904 ; loss =  0.110278\n",
      "epoch =  905 ; loss =  0.109964\n",
      "epoch =  906 ; loss =  0.111975\n",
      "epoch =  907 ; loss =  0.109117\n",
      "epoch =  908 ; loss =  0.109716\n",
      "epoch =  909 ; loss =  0.108851\n",
      "epoch =  910 ; loss =  0.110229\n",
      "epoch =  911 ; loss =  0.10913\n",
      "epoch =  912 ; loss =  0.110953\n",
      "epoch =  913 ; loss =  0.108962\n",
      "epoch =  914 ; loss =  0.109143\n",
      "epoch =  915 ; loss =  0.108926\n",
      "epoch =  916 ; loss =  0.11036\n",
      "epoch =  917 ; loss =  0.108543\n",
      "epoch =  918 ; loss =  0.108493\n",
      "epoch =  919 ; loss =  0.108922\n",
      "epoch =  920 ; loss =  0.109382\n",
      "epoch =  921 ; loss =  0.108737\n",
      "epoch =  922 ; loss =  0.110744\n",
      "epoch =  923 ; loss =  0.10842\n",
      "epoch =  924 ; loss =  0.107672\n",
      "epoch =  925 ; loss =  0.107485\n",
      "epoch =  926 ; loss =  0.108431\n",
      "epoch =  927 ; loss =  0.109208\n",
      "epoch =  928 ; loss =  0.111364\n",
      "epoch =  929 ; loss =  0.108878\n",
      "epoch =  930 ; loss =  0.108202\n",
      "epoch =  931 ; loss =  0.108458\n",
      "epoch =  932 ; loss =  0.108235\n",
      "epoch =  933 ; loss =  0.108073\n",
      "epoch =  934 ; loss =  0.109132\n",
      "epoch =  935 ; loss =  0.107954\n",
      "epoch =  936 ; loss =  0.108697\n",
      "epoch =  937 ; loss =  0.107908\n",
      "epoch =  938 ; loss =  0.10946\n",
      "epoch =  939 ; loss =  0.107133\n",
      "epoch =  940 ; loss =  0.10658\n",
      "epoch =  941 ; loss =  0.107176\n",
      "epoch =  942 ; loss =  0.107218\n",
      "epoch =  943 ; loss =  0.107612\n",
      "epoch =  944 ; loss =  0.109788\n",
      "epoch =  945 ; loss =  0.107496\n",
      "epoch =  946 ; loss =  0.107716\n",
      "epoch =  947 ; loss =  0.106874\n",
      "epoch =  948 ; loss =  0.10788\n",
      "epoch =  949 ; loss =  0.108721\n",
      "epoch =  950 ; loss =  0.107032\n",
      "epoch =  951 ; loss =  0.107165\n",
      "epoch =  952 ; loss =  0.106858\n",
      "epoch =  953 ; loss =  0.106722\n",
      "epoch =  954 ; loss =  0.10774\n",
      "epoch =  955 ; loss =  0.111238\n",
      "epoch =  956 ; loss =  0.108774\n",
      "epoch =  957 ; loss =  0.1072\n",
      "epoch =  958 ; loss =  0.107212\n",
      "epoch =  959 ; loss =  0.107818\n",
      "epoch =  960 ; loss =  0.107702\n",
      "epoch =  961 ; loss =  0.109789\n",
      "epoch =  962 ; loss =  0.107786\n",
      "epoch =  963 ; loss =  0.106594\n",
      "epoch =  964 ; loss =  0.106226\n",
      "epoch =  965 ; loss =  0.106054\n",
      "epoch =  966 ; loss =  0.106434\n",
      "epoch =  967 ; loss =  0.106047\n",
      "epoch =  968 ; loss =  0.108432\n",
      "epoch =  969 ; loss =  0.106415\n",
      "epoch =  970 ; loss =  0.106623\n",
      "epoch =  971 ; loss =  0.107088\n",
      "epoch =  972 ; loss =  0.10875\n",
      "epoch =  973 ; loss =  0.106923\n",
      "epoch =  974 ; loss =  0.106952\n",
      "epoch =  975 ; loss =  0.107344\n",
      "epoch =  976 ; loss =  0.110947\n",
      "epoch =  977 ; loss =  0.106627\n",
      "epoch =  978 ; loss =  0.106196\n",
      "epoch =  979 ; loss =  0.105985\n",
      "epoch =  980 ; loss =  0.105841\n",
      "epoch =  981 ; loss =  0.106\n",
      "epoch =  982 ; loss =  0.105264\n",
      "epoch =  983 ; loss =  0.10566\n",
      "epoch =  984 ; loss =  0.106505\n",
      "epoch =  985 ; loss =  0.109425\n",
      "epoch =  986 ; loss =  0.105979\n",
      "epoch =  987 ; loss =  0.106033\n",
      "epoch =  988 ; loss =  0.105322\n",
      "epoch =  989 ; loss =  0.105855\n",
      "epoch =  990 ; loss =  0.106038\n",
      "epoch =  991 ; loss =  0.109877\n",
      "epoch =  992 ; loss =  0.107733\n",
      "epoch =  993 ; loss =  0.106815\n",
      "epoch =  994 ; loss =  0.106229\n",
      "epoch =  995 ; loss =  0.105373\n",
      "epoch =  996 ; loss =  0.106103\n",
      "epoch =  997 ; loss =  0.108987\n",
      "epoch =  998 ; loss =  0.105776\n",
      "epoch =  999 ; loss =  0.105754\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1000 ; loss =  0.101431\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1001 ; loss =  0.10194\n",
      "epoch =  1002 ; loss =  0.10312\n",
      "epoch =  1003 ; loss =  0.108321\n",
      "epoch =  1004 ; loss =  0.103209\n",
      "epoch =  1005 ; loss =  0.101659\n",
      "epoch =  1006 ; loss =  0.101169\n",
      "epoch =  1007 ; loss =  0.101816\n",
      "epoch =  1008 ; loss =  0.101107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1009 ; loss =  0.101285\n",
      "epoch =  1010 ; loss =  0.10168\n",
      "epoch =  1011 ; loss =  0.102184\n",
      "epoch =  1012 ; loss =  0.101968\n",
      "epoch =  1013 ; loss =  0.102788\n",
      "epoch =  1014 ; loss =  0.101825\n",
      "epoch =  1015 ; loss =  0.100934\n",
      "epoch =  1016 ; loss =  0.100739\n",
      "epoch =  1017 ; loss =  0.101901\n",
      "epoch =  1018 ; loss =  0.101796\n",
      "epoch =  1019 ; loss =  0.10452\n",
      "epoch =  1020 ; loss =  0.101889\n",
      "epoch =  1021 ; loss =  0.100836\n",
      "epoch =  1022 ; loss =  0.0999821\n",
      "epoch =  1023 ; loss =  0.0999702\n",
      "epoch =  1024 ; loss =  0.0992721\n",
      "epoch =  1025 ; loss =  0.10014\n",
      "epoch =  1026 ; loss =  0.0995589\n",
      "epoch =  1027 ; loss =  0.0993693\n",
      "epoch =  1028 ; loss =  0.0994757\n",
      "epoch =  1029 ; loss =  0.100017\n",
      "epoch =  1030 ; loss =  0.101421\n",
      "epoch =  1031 ; loss =  0.108423\n",
      "epoch =  1032 ; loss =  0.102893\n",
      "epoch =  1033 ; loss =  0.100303\n",
      "epoch =  1034 ; loss =  0.100363\n",
      "epoch =  1035 ; loss =  0.10178\n",
      "epoch =  1036 ; loss =  0.101944\n",
      "epoch =  1037 ; loss =  0.104703\n",
      "epoch =  1038 ; loss =  0.100421\n",
      "epoch =  1039 ; loss =  0.0993791\n",
      "epoch =  1040 ; loss =  0.0991789\n",
      "epoch =  1041 ; loss =  0.0994824\n",
      "epoch =  1042 ; loss =  0.0998343\n",
      "epoch =  1043 ; loss =  0.101331\n",
      "epoch =  1044 ; loss =  0.100704\n",
      "epoch =  1045 ; loss =  0.101196\n",
      "epoch =  1046 ; loss =  0.100625\n",
      "epoch =  1047 ; loss =  0.101984\n",
      "epoch =  1048 ; loss =  0.100157\n",
      "epoch =  1049 ; loss =  0.0999215\n",
      "epoch =  1050 ; loss =  0.0996813\n",
      "epoch =  1051 ; loss =  0.100673\n",
      "epoch =  1052 ; loss =  0.100037\n",
      "epoch =  1053 ; loss =  0.100379\n",
      "epoch =  1054 ; loss =  0.100118\n",
      "epoch =  1055 ; loss =  0.102051\n",
      "epoch =  1056 ; loss =  0.0999627\n",
      "epoch =  1057 ; loss =  0.0996929\n",
      "epoch =  1058 ; loss =  0.0992527\n",
      "epoch =  1059 ; loss =  0.100627\n",
      "epoch =  1060 ; loss =  0.0998947\n",
      "epoch =  1061 ; loss =  0.101898\n",
      "epoch =  1062 ; loss =  0.0993041\n",
      "epoch =  1063 ; loss =  0.0988915\n",
      "epoch =  1064 ; loss =  0.0997149\n",
      "epoch =  1065 ; loss =  0.100624\n",
      "epoch =  1066 ; loss =  0.100174\n",
      "epoch =  1067 ; loss =  0.10111\n",
      "epoch =  1068 ; loss =  0.0993453\n",
      "epoch =  1069 ; loss =  0.099561\n",
      "epoch =  1070 ; loss =  0.0995941\n",
      "epoch =  1071 ; loss =  0.100479\n",
      "epoch =  1072 ; loss =  0.0993367\n",
      "epoch =  1073 ; loss =  0.100004\n",
      "epoch =  1074 ; loss =  0.0992056\n",
      "epoch =  1075 ; loss =  0.100536\n",
      "epoch =  1076 ; loss =  0.0993661\n",
      "epoch =  1077 ; loss =  0.0996475\n",
      "epoch =  1078 ; loss =  0.0988713\n",
      "epoch =  1079 ; loss =  0.1005\n",
      "epoch =  1080 ; loss =  0.0989639\n",
      "epoch =  1081 ; loss =  0.0996027\n",
      "epoch =  1082 ; loss =  0.0988883\n",
      "epoch =  1083 ; loss =  0.100468\n",
      "epoch =  1084 ; loss =  0.0994959\n",
      "epoch =  1085 ; loss =  0.100044\n",
      "epoch =  1086 ; loss =  0.0998308\n",
      "epoch =  1087 ; loss =  0.0993497\n",
      "epoch =  1088 ; loss =  0.0994696\n",
      "epoch =  1089 ; loss =  0.099588\n",
      "epoch =  1090 ; loss =  0.0999184\n",
      "epoch =  1091 ; loss =  0.100294\n",
      "epoch =  1092 ; loss =  0.0988647\n",
      "epoch =  1093 ; loss =  0.0997795\n",
      "epoch =  1094 ; loss =  0.0990696\n",
      "epoch =  1095 ; loss =  0.0992605\n",
      "epoch =  1096 ; loss =  0.0985438\n",
      "epoch =  1097 ; loss =  0.0992408\n",
      "epoch =  1098 ; loss =  0.0988254\n",
      "epoch =  1099 ; loss =  0.0998364\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1100 ; loss =  0.108119\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1101 ; loss =  0.108361\n",
      "epoch =  1102 ; loss =  0.107554\n",
      "epoch =  1103 ; loss =  0.108306\n",
      "epoch =  1104 ; loss =  0.107964\n",
      "epoch =  1105 ; loss =  0.108254\n",
      "epoch =  1106 ; loss =  0.107253\n",
      "epoch =  1107 ; loss =  0.10761\n",
      "epoch =  1108 ; loss =  0.107365\n",
      "epoch =  1109 ; loss =  0.107908\n",
      "epoch =  1110 ; loss =  0.1077\n",
      "epoch =  1111 ; loss =  0.107623\n",
      "epoch =  1112 ; loss =  0.107173\n",
      "epoch =  1113 ; loss =  0.107442\n",
      "epoch =  1114 ; loss =  0.107867\n",
      "epoch =  1115 ; loss =  0.107633\n",
      "epoch =  1116 ; loss =  0.107524\n",
      "epoch =  1117 ; loss =  0.106858\n",
      "epoch =  1118 ; loss =  0.107692\n",
      "epoch =  1119 ; loss =  0.108534\n",
      "epoch =  1120 ; loss =  0.10776\n",
      "epoch =  1121 ; loss =  0.108371\n",
      "epoch =  1122 ; loss =  0.107167\n",
      "epoch =  1123 ; loss =  0.107093\n",
      "epoch =  1124 ; loss =  0.106464\n",
      "epoch =  1125 ; loss =  0.106689\n",
      "epoch =  1126 ; loss =  0.106298\n",
      "epoch =  1127 ; loss =  0.106921\n",
      "epoch =  1128 ; loss =  0.106351\n",
      "epoch =  1129 ; loss =  0.107207\n",
      "epoch =  1130 ; loss =  0.107412\n",
      "epoch =  1131 ; loss =  0.108997\n",
      "epoch =  1132 ; loss =  0.107532\n",
      "epoch =  1133 ; loss =  0.107216\n",
      "epoch =  1134 ; loss =  0.106755\n",
      "epoch =  1135 ; loss =  0.106728\n",
      "epoch =  1136 ; loss =  0.105994\n",
      "epoch =  1137 ; loss =  0.106196\n",
      "epoch =  1138 ; loss =  0.105729\n",
      "epoch =  1139 ; loss =  0.105944\n",
      "epoch =  1140 ; loss =  0.106372\n",
      "epoch =  1141 ; loss =  0.107147\n",
      "epoch =  1142 ; loss =  0.106916\n",
      "epoch =  1143 ; loss =  0.108318\n",
      "epoch =  1144 ; loss =  0.10664\n",
      "epoch =  1145 ; loss =  0.106774\n",
      "epoch =  1146 ; loss =  0.106093\n",
      "epoch =  1147 ; loss =  0.105974\n",
      "epoch =  1148 ; loss =  0.105612\n",
      "epoch =  1149 ; loss =  0.105659\n",
      "epoch =  1150 ; loss =  0.105787\n",
      "epoch =  1151 ; loss =  0.10582\n",
      "epoch =  1152 ; loss =  0.106614\n",
      "epoch =  1153 ; loss =  0.107848\n",
      "epoch =  1154 ; loss =  0.107218\n",
      "epoch =  1155 ; loss =  0.108675\n",
      "epoch =  1156 ; loss =  0.107062\n",
      "epoch =  1157 ; loss =  0.108976\n",
      "epoch =  1158 ; loss =  0.107993\n",
      "epoch =  1159 ; loss =  0.106451\n",
      "epoch =  1160 ; loss =  0.105559\n",
      "epoch =  1161 ; loss =  0.105806\n",
      "epoch =  1162 ; loss =  0.106015\n",
      "epoch =  1163 ; loss =  0.105519\n",
      "epoch =  1164 ; loss =  0.105545\n",
      "epoch =  1165 ; loss =  0.10544\n",
      "epoch =  1166 ; loss =  0.105327\n",
      "epoch =  1167 ; loss =  0.106479\n",
      "epoch =  1168 ; loss =  0.106842\n",
      "epoch =  1169 ; loss =  0.109885\n",
      "epoch =  1170 ; loss =  0.106333\n",
      "epoch =  1171 ; loss =  0.105013\n",
      "epoch =  1172 ; loss =  0.106053\n",
      "epoch =  1173 ; loss =  0.105577\n",
      "epoch =  1174 ; loss =  0.105042\n",
      "epoch =  1175 ; loss =  0.104959\n",
      "epoch =  1176 ; loss =  0.10522\n",
      "epoch =  1177 ; loss =  0.105864\n",
      "epoch =  1178 ; loss =  0.10521\n",
      "epoch =  1179 ; loss =  0.105547\n",
      "epoch =  1180 ; loss =  0.105999\n",
      "epoch =  1181 ; loss =  0.106103\n",
      "epoch =  1182 ; loss =  0.108822\n",
      "epoch =  1183 ; loss =  0.107976\n",
      "epoch =  1184 ; loss =  0.11063\n",
      "epoch =  1185 ; loss =  0.106623\n",
      "epoch =  1186 ; loss =  0.10628\n",
      "epoch =  1187 ; loss =  0.106818\n",
      "epoch =  1188 ; loss =  0.105662\n",
      "epoch =  1189 ; loss =  0.105309\n",
      "epoch =  1190 ; loss =  0.105287\n",
      "epoch =  1191 ; loss =  0.104787\n",
      "epoch =  1192 ; loss =  0.105769\n",
      "epoch =  1193 ; loss =  0.105263\n",
      "epoch =  1194 ; loss =  0.105163\n",
      "epoch =  1195 ; loss =  0.106012\n",
      "epoch =  1196 ; loss =  0.106212\n",
      "epoch =  1197 ; loss =  0.107889\n",
      "epoch =  1198 ; loss =  0.105935\n",
      "epoch =  1199 ; loss =  0.106017\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1200 ; loss =  0.094859\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1201 ; loss =  0.0944616\n",
      "epoch =  1202 ; loss =  0.0945958\n",
      "epoch =  1203 ; loss =  0.0928886\n",
      "epoch =  1204 ; loss =  0.0935493\n",
      "epoch =  1205 ; loss =  0.0933354\n",
      "epoch =  1206 ; loss =  0.0932088\n",
      "epoch =  1207 ; loss =  0.0945791\n",
      "epoch =  1208 ; loss =  0.0948289\n",
      "epoch =  1209 ; loss =  0.096119\n",
      "epoch =  1210 ; loss =  0.0947561\n",
      "epoch =  1211 ; loss =  0.0938504\n",
      "epoch =  1212 ; loss =  0.0932534\n",
      "epoch =  1213 ; loss =  0.0926142\n",
      "epoch =  1214 ; loss =  0.0925442\n",
      "epoch =  1215 ; loss =  0.0928266\n",
      "epoch =  1216 ; loss =  0.0929238\n",
      "epoch =  1217 ; loss =  0.0942131\n",
      "epoch =  1218 ; loss =  0.0951817\n",
      "epoch =  1219 ; loss =  0.0991464\n",
      "epoch =  1220 ; loss =  0.0934378\n",
      "epoch =  1221 ; loss =  0.0934363\n",
      "epoch =  1222 ; loss =  0.0929456\n",
      "epoch =  1223 ; loss =  0.092668\n",
      "epoch =  1224 ; loss =  0.0924431\n",
      "epoch =  1225 ; loss =  0.0926432\n",
      "epoch =  1226 ; loss =  0.0918852\n",
      "epoch =  1227 ; loss =  0.0931912\n",
      "epoch =  1228 ; loss =  0.0934881\n",
      "epoch =  1229 ; loss =  0.0960688\n",
      "epoch =  1230 ; loss =  0.0955246\n",
      "epoch =  1231 ; loss =  0.0961929\n",
      "epoch =  1232 ; loss =  0.0933375\n",
      "epoch =  1233 ; loss =  0.0930291\n",
      "epoch =  1234 ; loss =  0.092981\n",
      "epoch =  1235 ; loss =  0.0924554\n",
      "epoch =  1236 ; loss =  0.0918713\n",
      "epoch =  1237 ; loss =  0.0916938\n",
      "epoch =  1238 ; loss =  0.0924266\n",
      "epoch =  1239 ; loss =  0.0934377\n",
      "epoch =  1240 ; loss =  0.0939548\n",
      "epoch =  1241 ; loss =  0.0966433\n",
      "epoch =  1242 ; loss =  0.092689\n",
      "epoch =  1243 ; loss =  0.0919445\n",
      "epoch =  1244 ; loss =  0.0916116\n",
      "epoch =  1245 ; loss =  0.0919463\n",
      "epoch =  1246 ; loss =  0.0918748\n",
      "epoch =  1247 ; loss =  0.0924015\n",
      "epoch =  1248 ; loss =  0.0928431\n",
      "epoch =  1249 ; loss =  0.0948515\n",
      "epoch =  1250 ; loss =  0.0935758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1251 ; loss =  0.0946628\n",
      "epoch =  1252 ; loss =  0.0929604\n",
      "epoch =  1253 ; loss =  0.0931516\n",
      "epoch =  1254 ; loss =  0.0925209\n",
      "epoch =  1255 ; loss =  0.0918266\n",
      "epoch =  1256 ; loss =  0.0921045\n",
      "epoch =  1257 ; loss =  0.0930412\n",
      "epoch =  1258 ; loss =  0.0927824\n",
      "epoch =  1259 ; loss =  0.0944379\n",
      "epoch =  1260 ; loss =  0.0927942\n",
      "epoch =  1261 ; loss =  0.0920998\n",
      "epoch =  1262 ; loss =  0.0921018\n",
      "epoch =  1263 ; loss =  0.0921807\n",
      "epoch =  1264 ; loss =  0.0917037\n",
      "epoch =  1265 ; loss =  0.0928824\n",
      "epoch =  1266 ; loss =  0.0925581\n",
      "epoch =  1267 ; loss =  0.0958136\n",
      "epoch =  1268 ; loss =  0.0923682\n",
      "epoch =  1269 ; loss =  0.0922547\n",
      "epoch =  1270 ; loss =  0.0917229\n",
      "epoch =  1271 ; loss =  0.0914716\n",
      "epoch =  1272 ; loss =  0.0919464\n",
      "epoch =  1273 ; loss =  0.0924012\n",
      "epoch =  1274 ; loss =  0.0928189\n",
      "epoch =  1275 ; loss =  0.0942724\n",
      "epoch =  1276 ; loss =  0.0925463\n",
      "epoch =  1277 ; loss =  0.0923912\n",
      "epoch =  1278 ; loss =  0.0915485\n",
      "epoch =  1279 ; loss =  0.0918923\n",
      "epoch =  1280 ; loss =  0.0913497\n",
      "epoch =  1281 ; loss =  0.091332\n",
      "epoch =  1282 ; loss =  0.091778\n",
      "epoch =  1283 ; loss =  0.0925268\n",
      "epoch =  1284 ; loss =  0.0919644\n",
      "epoch =  1285 ; loss =  0.0928356\n",
      "epoch =  1286 ; loss =  0.0916807\n",
      "epoch =  1287 ; loss =  0.0920312\n",
      "epoch =  1288 ; loss =  0.0918659\n",
      "epoch =  1289 ; loss =  0.0916983\n",
      "epoch =  1290 ; loss =  0.0921175\n",
      "epoch =  1291 ; loss =  0.0921913\n",
      "epoch =  1292 ; loss =  0.092072\n",
      "epoch =  1293 ; loss =  0.0929339\n",
      "epoch =  1294 ; loss =  0.0918504\n",
      "epoch =  1295 ; loss =  0.0921353\n",
      "epoch =  1296 ; loss =  0.0909657\n",
      "epoch =  1297 ; loss =  0.0913634\n",
      "epoch =  1298 ; loss =  0.0917573\n",
      "epoch =  1299 ; loss =  0.0923611\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1300 ; loss =  0.111305\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1301 ; loss =  0.114343\n",
      "epoch =  1302 ; loss =  0.112392\n",
      "epoch =  1303 ; loss =  0.114778\n",
      "epoch =  1304 ; loss =  0.110028\n",
      "epoch =  1305 ; loss =  0.110699\n",
      "epoch =  1306 ; loss =  0.110006\n",
      "epoch =  1307 ; loss =  0.109797\n",
      "epoch =  1308 ; loss =  0.110343\n",
      "epoch =  1309 ; loss =  0.110071\n",
      "epoch =  1310 ; loss =  0.109048\n",
      "epoch =  1311 ; loss =  0.110091\n",
      "epoch =  1312 ; loss =  0.110021\n",
      "epoch =  1313 ; loss =  0.109461\n",
      "epoch =  1314 ; loss =  0.11037\n",
      "epoch =  1315 ; loss =  0.111281\n",
      "epoch =  1316 ; loss =  0.110355\n",
      "epoch =  1317 ; loss =  0.10995\n",
      "epoch =  1318 ; loss =  0.109614\n",
      "epoch =  1319 ; loss =  0.110273\n",
      "epoch =  1320 ; loss =  0.109673\n",
      "epoch =  1321 ; loss =  0.109265\n",
      "epoch =  1322 ; loss =  0.109625\n",
      "epoch =  1323 ; loss =  0.109804\n",
      "epoch =  1324 ; loss =  0.10952\n",
      "epoch =  1325 ; loss =  0.109286\n",
      "epoch =  1326 ; loss =  0.109924\n",
      "epoch =  1327 ; loss =  0.109422\n",
      "epoch =  1328 ; loss =  0.108653\n",
      "epoch =  1329 ; loss =  0.109135\n",
      "epoch =  1330 ; loss =  0.10918\n",
      "epoch =  1331 ; loss =  0.109441\n",
      "epoch =  1332 ; loss =  0.109068\n",
      "epoch =  1333 ; loss =  0.109537\n",
      "epoch =  1334 ; loss =  0.109101\n",
      "epoch =  1335 ; loss =  0.108917\n",
      "epoch =  1336 ; loss =  0.109592\n",
      "epoch =  1337 ; loss =  0.111175\n",
      "epoch =  1338 ; loss =  0.110712\n",
      "epoch =  1339 ; loss =  0.111696\n",
      "epoch =  1340 ; loss =  0.109313\n",
      "epoch =  1341 ; loss =  0.109749\n",
      "epoch =  1342 ; loss =  0.108825\n",
      "epoch =  1343 ; loss =  0.108638\n",
      "epoch =  1344 ; loss =  0.108928\n",
      "epoch =  1345 ; loss =  0.109299\n",
      "epoch =  1346 ; loss =  0.109136\n",
      "epoch =  1347 ; loss =  0.109015\n",
      "epoch =  1348 ; loss =  0.108918\n",
      "epoch =  1349 ; loss =  0.108857\n",
      "epoch =  1350 ; loss =  0.108789\n",
      "epoch =  1351 ; loss =  0.108543\n",
      "epoch =  1352 ; loss =  0.108348\n",
      "epoch =  1353 ; loss =  0.10819\n",
      "epoch =  1354 ; loss =  0.10863\n",
      "epoch =  1355 ; loss =  0.108083\n",
      "epoch =  1356 ; loss =  0.108512\n",
      "epoch =  1357 ; loss =  0.108551\n",
      "epoch =  1358 ; loss =  0.108414\n",
      "epoch =  1359 ; loss =  0.108018\n",
      "epoch =  1360 ; loss =  0.109245\n",
      "epoch =  1361 ; loss =  0.109599\n",
      "epoch =  1362 ; loss =  0.111885\n",
      "epoch =  1363 ; loss =  0.109381\n",
      "epoch =  1364 ; loss =  0.10953\n",
      "epoch =  1365 ; loss =  0.108695\n",
      "epoch =  1366 ; loss =  0.109024\n",
      "epoch =  1367 ; loss =  0.109627\n",
      "epoch =  1368 ; loss =  0.10892\n",
      "epoch =  1369 ; loss =  0.109051\n",
      "epoch =  1370 ; loss =  0.108811\n",
      "epoch =  1371 ; loss =  0.109507\n",
      "epoch =  1372 ; loss =  0.109531\n",
      "epoch =  1373 ; loss =  0.109866\n",
      "epoch =  1374 ; loss =  0.109715\n",
      "epoch =  1375 ; loss =  0.109526\n",
      "epoch =  1376 ; loss =  0.108741\n",
      "epoch =  1377 ; loss =  0.108563\n",
      "epoch =  1378 ; loss =  0.108491\n",
      "epoch =  1379 ; loss =  0.10925\n",
      "epoch =  1380 ; loss =  0.108481\n",
      "epoch =  1381 ; loss =  0.108475\n",
      "epoch =  1382 ; loss =  0.108221\n",
      "epoch =  1383 ; loss =  0.108481\n",
      "epoch =  1384 ; loss =  0.108714\n",
      "epoch =  1385 ; loss =  0.109436\n",
      "epoch =  1386 ; loss =  0.108881\n",
      "epoch =  1387 ; loss =  0.108749\n",
      "epoch =  1388 ; loss =  0.108789\n",
      "epoch =  1389 ; loss =  0.108852\n",
      "epoch =  1390 ; loss =  0.108301\n",
      "epoch =  1391 ; loss =  0.10835\n",
      "epoch =  1392 ; loss =  0.108653\n",
      "epoch =  1393 ; loss =  0.108335\n",
      "epoch =  1394 ; loss =  0.107858\n",
      "epoch =  1395 ; loss =  0.107304\n",
      "epoch =  1396 ; loss =  0.107408\n",
      "epoch =  1397 ; loss =  0.107627\n",
      "epoch =  1398 ; loss =  0.107796\n",
      "epoch =  1399 ; loss =  0.107948\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1400 ; loss =  0.108204\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1401 ; loss =  0.109238\n",
      "epoch =  1402 ; loss =  0.109483\n",
      "epoch =  1403 ; loss =  0.110294\n",
      "epoch =  1404 ; loss =  0.110393\n",
      "epoch =  1405 ; loss =  0.111093\n",
      "epoch =  1406 ; loss =  0.108451\n",
      "epoch =  1407 ; loss =  0.108381\n",
      "epoch =  1408 ; loss =  0.10913\n",
      "epoch =  1409 ; loss =  0.110787\n",
      "epoch =  1410 ; loss =  0.109348\n",
      "epoch =  1411 ; loss =  0.110616\n",
      "epoch =  1412 ; loss =  0.111521\n",
      "epoch =  1413 ; loss =  0.117063\n",
      "epoch =  1414 ; loss =  0.107918\n",
      "epoch =  1415 ; loss =  0.10757\n",
      "epoch =  1416 ; loss =  0.107697\n",
      "epoch =  1417 ; loss =  0.106932\n",
      "epoch =  1418 ; loss =  0.107349\n",
      "epoch =  1419 ; loss =  0.10694\n",
      "epoch =  1420 ; loss =  0.106433\n",
      "epoch =  1421 ; loss =  0.106934\n",
      "epoch =  1422 ; loss =  0.106822\n",
      "epoch =  1423 ; loss =  0.106731\n",
      "epoch =  1424 ; loss =  0.107066\n",
      "epoch =  1425 ; loss =  0.107116\n",
      "epoch =  1426 ; loss =  0.107437\n",
      "epoch =  1427 ; loss =  0.109517\n",
      "epoch =  1428 ; loss =  0.110109\n",
      "epoch =  1429 ; loss =  0.112125\n",
      "epoch =  1430 ; loss =  0.107615\n",
      "epoch =  1431 ; loss =  0.106889\n",
      "epoch =  1432 ; loss =  0.106457\n",
      "epoch =  1433 ; loss =  0.107101\n",
      "epoch =  1434 ; loss =  0.1066\n",
      "epoch =  1435 ; loss =  0.106779\n",
      "epoch =  1436 ; loss =  0.106706\n",
      "epoch =  1437 ; loss =  0.106728\n",
      "epoch =  1438 ; loss =  0.106503\n",
      "epoch =  1439 ; loss =  0.107235\n",
      "epoch =  1440 ; loss =  0.107986\n",
      "epoch =  1441 ; loss =  0.111354\n",
      "epoch =  1442 ; loss =  0.107875\n",
      "epoch =  1443 ; loss =  0.107806\n",
      "epoch =  1444 ; loss =  0.106395\n",
      "epoch =  1445 ; loss =  0.106318\n",
      "epoch =  1446 ; loss =  0.106124\n",
      "epoch =  1447 ; loss =  0.106394\n",
      "epoch =  1448 ; loss =  0.106519\n",
      "epoch =  1449 ; loss =  0.106268\n",
      "epoch =  1450 ; loss =  0.106408\n",
      "epoch =  1451 ; loss =  0.107568\n",
      "epoch =  1452 ; loss =  0.107859\n",
      "epoch =  1453 ; loss =  0.110094\n",
      "epoch =  1454 ; loss =  0.107912\n",
      "epoch =  1455 ; loss =  0.10735\n",
      "epoch =  1456 ; loss =  0.106696\n",
      "epoch =  1457 ; loss =  0.106381\n",
      "epoch =  1458 ; loss =  0.105605\n",
      "epoch =  1459 ; loss =  0.106612\n",
      "epoch =  1460 ; loss =  0.105826\n",
      "epoch =  1461 ; loss =  0.10613\n",
      "epoch =  1462 ; loss =  0.106553\n",
      "epoch =  1463 ; loss =  0.107238\n",
      "epoch =  1464 ; loss =  0.108267\n",
      "epoch =  1465 ; loss =  0.10931\n",
      "epoch =  1466 ; loss =  0.107718\n",
      "epoch =  1467 ; loss =  0.107098\n",
      "epoch =  1468 ; loss =  0.106188\n",
      "epoch =  1469 ; loss =  0.106406\n",
      "epoch =  1470 ; loss =  0.106762\n",
      "epoch =  1471 ; loss =  0.105553\n",
      "epoch =  1472 ; loss =  0.105348\n",
      "epoch =  1473 ; loss =  0.104933\n",
      "epoch =  1474 ; loss =  0.105347\n",
      "epoch =  1475 ; loss =  0.106226\n",
      "epoch =  1476 ; loss =  0.106572\n",
      "epoch =  1477 ; loss =  0.109586\n",
      "epoch =  1478 ; loss =  0.1093\n",
      "epoch =  1479 ; loss =  0.109894\n",
      "epoch =  1480 ; loss =  0.106043\n",
      "epoch =  1481 ; loss =  0.105982\n",
      "epoch =  1482 ; loss =  0.106313\n",
      "epoch =  1483 ; loss =  0.106054\n",
      "epoch =  1484 ; loss =  0.106329\n",
      "epoch =  1485 ; loss =  0.105784\n",
      "epoch =  1486 ; loss =  0.105111\n",
      "epoch =  1487 ; loss =  0.105553\n",
      "epoch =  1488 ; loss =  0.105072\n",
      "epoch =  1489 ; loss =  0.104991\n",
      "epoch =  1490 ; loss =  0.105259\n",
      "epoch =  1491 ; loss =  0.106679\n",
      "epoch =  1492 ; loss =  0.10904\n",
      "epoch =  1493 ; loss =  0.114207\n",
      "epoch =  1494 ; loss =  0.106238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1495 ; loss =  0.106135\n",
      "epoch =  1496 ; loss =  0.106087\n",
      "epoch =  1497 ; loss =  0.105979\n",
      "epoch =  1498 ; loss =  0.105427\n",
      "epoch =  1499 ; loss =  0.105116\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1500 ; loss =  0.102621\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1501 ; loss =  0.102157\n",
      "epoch =  1502 ; loss =  0.101906\n",
      "epoch =  1503 ; loss =  0.101201\n",
      "epoch =  1504 ; loss =  0.101288\n",
      "epoch =  1505 ; loss =  0.10198\n",
      "epoch =  1506 ; loss =  0.102041\n",
      "epoch =  1507 ; loss =  0.104915\n",
      "epoch =  1508 ; loss =  0.103117\n",
      "epoch =  1509 ; loss =  0.103135\n",
      "epoch =  1510 ; loss =  0.101043\n",
      "epoch =  1511 ; loss =  0.10066\n",
      "epoch =  1512 ; loss =  0.100971\n",
      "epoch =  1513 ; loss =  0.100275\n",
      "epoch =  1514 ; loss =  0.100169\n",
      "epoch =  1515 ; loss =  0.100388\n",
      "epoch =  1516 ; loss =  0.100092\n",
      "epoch =  1517 ; loss =  0.0995889\n",
      "epoch =  1518 ; loss =  0.100025\n",
      "epoch =  1519 ; loss =  0.100079\n",
      "epoch =  1520 ; loss =  0.100895\n",
      "epoch =  1521 ; loss =  0.102438\n",
      "epoch =  1522 ; loss =  0.103245\n",
      "epoch =  1523 ; loss =  0.10387\n",
      "epoch =  1524 ; loss =  0.101091\n",
      "epoch =  1525 ; loss =  0.100669\n",
      "epoch =  1526 ; loss =  0.0999842\n",
      "epoch =  1527 ; loss =  0.0992539\n",
      "epoch =  1528 ; loss =  0.0988972\n",
      "epoch =  1529 ; loss =  0.0995342\n",
      "epoch =  1530 ; loss =  0.0992364\n",
      "epoch =  1531 ; loss =  0.098877\n",
      "epoch =  1532 ; loss =  0.0991493\n",
      "epoch =  1533 ; loss =  0.0990111\n",
      "epoch =  1534 ; loss =  0.0991654\n",
      "epoch =  1535 ; loss =  0.0996604\n",
      "epoch =  1536 ; loss =  0.0999283\n",
      "epoch =  1537 ; loss =  0.101594\n",
      "epoch =  1538 ; loss =  0.101323\n",
      "epoch =  1539 ; loss =  0.10181\n",
      "epoch =  1540 ; loss =  0.0998106\n",
      "epoch =  1541 ; loss =  0.099718\n",
      "epoch =  1542 ; loss =  0.099133\n",
      "epoch =  1543 ; loss =  0.0997779\n",
      "epoch =  1544 ; loss =  0.0992182\n",
      "epoch =  1545 ; loss =  0.0991475\n",
      "epoch =  1546 ; loss =  0.0992807\n",
      "epoch =  1547 ; loss =  0.0985493\n",
      "epoch =  1548 ; loss =  0.0986285\n",
      "epoch =  1549 ; loss =  0.0995496\n",
      "epoch =  1550 ; loss =  0.100275\n",
      "epoch =  1551 ; loss =  0.102442\n",
      "epoch =  1552 ; loss =  0.101376\n",
      "epoch =  1553 ; loss =  0.100709\n",
      "epoch =  1554 ; loss =  0.0986986\n",
      "epoch =  1555 ; loss =  0.0980185\n",
      "epoch =  1556 ; loss =  0.098288\n",
      "epoch =  1557 ; loss =  0.0981195\n",
      "epoch =  1558 ; loss =  0.0983518\n",
      "epoch =  1559 ; loss =  0.0979881\n",
      "epoch =  1560 ; loss =  0.0979886\n",
      "epoch =  1561 ; loss =  0.0976412\n",
      "epoch =  1562 ; loss =  0.0979177\n",
      "epoch =  1563 ; loss =  0.0976794\n",
      "epoch =  1564 ; loss =  0.0980532\n",
      "epoch =  1565 ; loss =  0.0975293\n",
      "epoch =  1566 ; loss =  0.0979872\n",
      "epoch =  1567 ; loss =  0.0988924\n",
      "epoch =  1568 ; loss =  0.0999127\n",
      "epoch =  1569 ; loss =  0.101893\n",
      "epoch =  1570 ; loss =  0.104888\n",
      "epoch =  1571 ; loss =  0.0998499\n",
      "epoch =  1572 ; loss =  0.0977909\n",
      "epoch =  1573 ; loss =  0.0978201\n",
      "epoch =  1574 ; loss =  0.0975831\n",
      "epoch =  1575 ; loss =  0.0976567\n",
      "epoch =  1576 ; loss =  0.0977253\n",
      "epoch =  1577 ; loss =  0.0974803\n",
      "epoch =  1578 ; loss =  0.0974831\n",
      "epoch =  1579 ; loss =  0.0972744\n",
      "epoch =  1580 ; loss =  0.0976253\n",
      "epoch =  1581 ; loss =  0.0972824\n",
      "epoch =  1582 ; loss =  0.0974172\n",
      "epoch =  1583 ; loss =  0.0976418\n",
      "epoch =  1584 ; loss =  0.0974124\n",
      "epoch =  1585 ; loss =  0.0981964\n",
      "epoch =  1586 ; loss =  0.0984237\n",
      "epoch =  1587 ; loss =  0.0990405\n",
      "epoch =  1588 ; loss =  0.0987759\n",
      "epoch =  1589 ; loss =  0.0992725\n",
      "epoch =  1590 ; loss =  0.0978626\n",
      "epoch =  1591 ; loss =  0.0972703\n",
      "epoch =  1592 ; loss =  0.0977053\n",
      "epoch =  1593 ; loss =  0.0973306\n",
      "epoch =  1594 ; loss =  0.0974302\n",
      "epoch =  1595 ; loss =  0.0969883\n",
      "epoch =  1596 ; loss =  0.0973247\n",
      "epoch =  1597 ; loss =  0.0963971\n",
      "epoch =  1598 ; loss =  0.0968739\n",
      "epoch =  1599 ; loss =  0.0964723\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1600 ; loss =  0.109632\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1601 ; loss =  0.110108\n",
      "epoch =  1602 ; loss =  0.109798\n",
      "epoch =  1603 ; loss =  0.110501\n",
      "epoch =  1604 ; loss =  0.114275\n",
      "epoch =  1605 ; loss =  0.122154\n",
      "epoch =  1606 ; loss =  0.130997\n",
      "epoch =  1607 ; loss =  0.111704\n",
      "epoch =  1608 ; loss =  0.111537\n",
      "epoch =  1609 ; loss =  0.112285\n",
      "epoch =  1610 ; loss =  0.112597\n",
      "epoch =  1611 ; loss =  0.111924\n",
      "epoch =  1612 ; loss =  0.110585\n",
      "epoch =  1613 ; loss =  0.110601\n",
      "epoch =  1614 ; loss =  0.109875\n",
      "epoch =  1615 ; loss =  0.110047\n",
      "epoch =  1616 ; loss =  0.109643\n",
      "epoch =  1617 ; loss =  0.109373\n",
      "epoch =  1618 ; loss =  0.109413\n",
      "epoch =  1619 ; loss =  0.108661\n",
      "epoch =  1620 ; loss =  0.108728\n",
      "epoch =  1621 ; loss =  0.108282\n",
      "epoch =  1622 ; loss =  0.108536\n",
      "epoch =  1623 ; loss =  0.109261\n",
      "epoch =  1624 ; loss =  0.108369\n",
      "epoch =  1625 ; loss =  0.108562\n",
      "epoch =  1626 ; loss =  0.108467\n",
      "epoch =  1627 ; loss =  0.108384\n",
      "epoch =  1628 ; loss =  0.108288\n",
      "epoch =  1629 ; loss =  0.10763\n",
      "epoch =  1630 ; loss =  0.108081\n",
      "epoch =  1631 ; loss =  0.108287\n",
      "epoch =  1632 ; loss =  0.107972\n",
      "epoch =  1633 ; loss =  0.107308\n",
      "epoch =  1634 ; loss =  0.107661\n",
      "epoch =  1635 ; loss =  0.108098\n",
      "epoch =  1636 ; loss =  0.108262\n",
      "epoch =  1637 ; loss =  0.108416\n",
      "epoch =  1638 ; loss =  0.108777\n",
      "epoch =  1639 ; loss =  0.10928\n",
      "epoch =  1640 ; loss =  0.10969\n",
      "epoch =  1641 ; loss =  0.11071\n",
      "epoch =  1642 ; loss =  0.10908\n",
      "epoch =  1643 ; loss =  0.108944\n",
      "epoch =  1644 ; loss =  0.108195\n",
      "epoch =  1645 ; loss =  0.108023\n",
      "epoch =  1646 ; loss =  0.108214\n",
      "epoch =  1647 ; loss =  0.107872\n",
      "epoch =  1648 ; loss =  0.107615\n",
      "epoch =  1649 ; loss =  0.107886\n",
      "epoch =  1650 ; loss =  0.107769\n",
      "epoch =  1651 ; loss =  0.107913\n",
      "epoch =  1652 ; loss =  0.108952\n",
      "epoch =  1653 ; loss =  0.109575\n",
      "epoch =  1654 ; loss =  0.108175\n",
      "epoch =  1655 ; loss =  0.108577\n",
      "epoch =  1656 ; loss =  0.108069\n",
      "epoch =  1657 ; loss =  0.108596\n",
      "epoch =  1658 ; loss =  0.107019\n",
      "epoch =  1659 ; loss =  0.107585\n",
      "epoch =  1660 ; loss =  0.107285\n",
      "epoch =  1661 ; loss =  0.107498\n",
      "epoch =  1662 ; loss =  0.107661\n",
      "epoch =  1663 ; loss =  0.107826\n",
      "epoch =  1664 ; loss =  0.108079\n",
      "epoch =  1665 ; loss =  0.108543\n",
      "epoch =  1666 ; loss =  0.1077\n",
      "epoch =  1667 ; loss =  0.108291\n",
      "epoch =  1668 ; loss =  0.107922\n",
      "epoch =  1669 ; loss =  0.107945\n",
      "epoch =  1670 ; loss =  0.107641\n",
      "epoch =  1671 ; loss =  0.108021\n",
      "epoch =  1672 ; loss =  0.107178\n",
      "epoch =  1673 ; loss =  0.107439\n",
      "epoch =  1674 ; loss =  0.106624\n",
      "epoch =  1675 ; loss =  0.107021\n",
      "epoch =  1676 ; loss =  0.106856\n",
      "epoch =  1677 ; loss =  0.107503\n",
      "epoch =  1678 ; loss =  0.107539\n",
      "epoch =  1679 ; loss =  0.108286\n",
      "epoch =  1680 ; loss =  0.107758\n",
      "epoch =  1681 ; loss =  0.107722\n",
      "epoch =  1682 ; loss =  0.10763\n",
      "epoch =  1683 ; loss =  0.107834\n",
      "epoch =  1684 ; loss =  0.108003\n",
      "epoch =  1685 ; loss =  0.107699\n",
      "epoch =  1686 ; loss =  0.107429\n",
      "epoch =  1687 ; loss =  0.106974\n",
      "epoch =  1688 ; loss =  0.106798\n",
      "epoch =  1689 ; loss =  0.107728\n",
      "epoch =  1690 ; loss =  0.106713\n",
      "epoch =  1691 ; loss =  0.107925\n",
      "epoch =  1692 ; loss =  0.107085\n",
      "epoch =  1693 ; loss =  0.106835\n",
      "epoch =  1694 ; loss =  0.106683\n",
      "epoch =  1695 ; loss =  0.106626\n",
      "epoch =  1696 ; loss =  0.106457\n",
      "epoch =  1697 ; loss =  0.106505\n",
      "epoch =  1698 ; loss =  0.106308\n",
      "epoch =  1699 ; loss =  0.107086\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1700 ; loss =  0.0922145\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1701 ; loss =  0.0933705\n",
      "epoch =  1702 ; loss =  0.0937068\n",
      "epoch =  1703 ; loss =  0.093985\n",
      "epoch =  1704 ; loss =  0.0923083\n",
      "epoch =  1705 ; loss =  0.0923987\n",
      "epoch =  1706 ; loss =  0.0919413\n",
      "epoch =  1707 ; loss =  0.0928487\n",
      "epoch =  1708 ; loss =  0.0927632\n",
      "epoch =  1709 ; loss =  0.0919043\n",
      "epoch =  1710 ; loss =  0.0912398\n",
      "epoch =  1711 ; loss =  0.0910673\n",
      "epoch =  1712 ; loss =  0.0924875\n",
      "epoch =  1713 ; loss =  0.0944892\n",
      "epoch =  1714 ; loss =  0.0943799\n",
      "epoch =  1715 ; loss =  0.0923626\n",
      "epoch =  1716 ; loss =  0.0915048\n",
      "epoch =  1717 ; loss =  0.0912803\n",
      "epoch =  1718 ; loss =  0.0918955\n",
      "epoch =  1719 ; loss =  0.0908897\n",
      "epoch =  1720 ; loss =  0.0903405\n",
      "epoch =  1721 ; loss =  0.0904399\n",
      "epoch =  1722 ; loss =  0.0909519\n",
      "epoch =  1723 ; loss =  0.0914296\n",
      "epoch =  1724 ; loss =  0.0917922\n",
      "epoch =  1725 ; loss =  0.0910267\n",
      "epoch =  1726 ; loss =  0.0908718\n",
      "epoch =  1727 ; loss =  0.0912006\n",
      "epoch =  1728 ; loss =  0.0910366\n",
      "epoch =  1729 ; loss =  0.0912695\n",
      "epoch =  1730 ; loss =  0.0906625\n",
      "epoch =  1731 ; loss =  0.0921446\n",
      "epoch =  1732 ; loss =  0.0914587\n",
      "epoch =  1733 ; loss =  0.0916433\n",
      "epoch =  1734 ; loss =  0.0910821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1735 ; loss =  0.0906835\n",
      "epoch =  1736 ; loss =  0.0912007\n",
      "epoch =  1737 ; loss =  0.0906629\n",
      "epoch =  1738 ; loss =  0.0904972\n",
      "epoch =  1739 ; loss =  0.0901082\n",
      "epoch =  1740 ; loss =  0.0901357\n",
      "epoch =  1741 ; loss =  0.0901954\n",
      "epoch =  1742 ; loss =  0.0900609\n",
      "epoch =  1743 ; loss =  0.0902371\n",
      "epoch =  1744 ; loss =  0.0892445\n",
      "epoch =  1745 ; loss =  0.0898188\n",
      "epoch =  1746 ; loss =  0.0901075\n",
      "epoch =  1747 ; loss =  0.0891813\n",
      "epoch =  1748 ; loss =  0.089869\n",
      "epoch =  1749 ; loss =  0.089914\n",
      "epoch =  1750 ; loss =  0.0900459\n",
      "epoch =  1751 ; loss =  0.0903202\n",
      "epoch =  1752 ; loss =  0.0907781\n",
      "epoch =  1753 ; loss =  0.0892799\n",
      "epoch =  1754 ; loss =  0.0891661\n",
      "epoch =  1755 ; loss =  0.089364\n",
      "epoch =  1756 ; loss =  0.089087\n",
      "epoch =  1757 ; loss =  0.0895001\n",
      "epoch =  1758 ; loss =  0.089155\n",
      "epoch =  1759 ; loss =  0.0890684\n",
      "epoch =  1760 ; loss =  0.0896793\n",
      "epoch =  1761 ; loss =  0.0906311\n",
      "epoch =  1762 ; loss =  0.090726\n",
      "epoch =  1763 ; loss =  0.0923989\n",
      "epoch =  1764 ; loss =  0.0925476\n",
      "epoch =  1765 ; loss =  0.0920202\n",
      "epoch =  1766 ; loss =  0.091705\n",
      "epoch =  1767 ; loss =  0.0897948\n",
      "epoch =  1768 ; loss =  0.0911407\n",
      "epoch =  1769 ; loss =  0.0980514\n",
      "epoch =  1770 ; loss =  0.106357\n",
      "epoch =  1771 ; loss =  0.0935763\n",
      "epoch =  1772 ; loss =  0.0954147\n",
      "epoch =  1773 ; loss =  0.0919244\n",
      "epoch =  1774 ; loss =  0.0900727\n",
      "epoch =  1775 ; loss =  0.0893359\n",
      "epoch =  1776 ; loss =  0.0892489\n",
      "epoch =  1777 ; loss =  0.0892868\n",
      "epoch =  1778 ; loss =  0.0885773\n",
      "epoch =  1779 ; loss =  0.0893699\n",
      "epoch =  1780 ; loss =  0.0888362\n",
      "epoch =  1781 ; loss =  0.089124\n",
      "epoch =  1782 ; loss =  0.0882627\n",
      "epoch =  1783 ; loss =  0.0887343\n",
      "epoch =  1784 ; loss =  0.0886325\n",
      "epoch =  1785 ; loss =  0.0887845\n",
      "epoch =  1786 ; loss =  0.0889876\n",
      "epoch =  1787 ; loss =  0.0881597\n",
      "epoch =  1788 ; loss =  0.0885884\n",
      "epoch =  1789 ; loss =  0.0885604\n",
      "epoch =  1790 ; loss =  0.0885016\n",
      "epoch =  1791 ; loss =  0.0884216\n",
      "epoch =  1792 ; loss =  0.0883493\n",
      "epoch =  1793 ; loss =  0.0883816\n",
      "epoch =  1794 ; loss =  0.0881935\n",
      "epoch =  1795 ; loss =  0.0884183\n",
      "epoch =  1796 ; loss =  0.0881896\n",
      "epoch =  1797 ; loss =  0.0883954\n",
      "epoch =  1798 ; loss =  0.088284\n",
      "epoch =  1799 ; loss =  0.0881122\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1800 ; loss =  0.0929335\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1801 ; loss =  0.0932858\n",
      "epoch =  1802 ; loss =  0.0929565\n",
      "epoch =  1803 ; loss =  0.0929497\n",
      "epoch =  1804 ; loss =  0.0928763\n",
      "epoch =  1805 ; loss =  0.0927975\n",
      "epoch =  1806 ; loss =  0.0928091\n",
      "epoch =  1807 ; loss =  0.0923721\n",
      "epoch =  1808 ; loss =  0.0925194\n",
      "epoch =  1809 ; loss =  0.0922582\n",
      "epoch =  1810 ; loss =  0.092591\n",
      "epoch =  1811 ; loss =  0.0929001\n",
      "epoch =  1812 ; loss =  0.0926781\n",
      "epoch =  1813 ; loss =  0.0923219\n",
      "epoch =  1814 ; loss =  0.092372\n",
      "epoch =  1815 ; loss =  0.092582\n",
      "epoch =  1816 ; loss =  0.0921617\n",
      "epoch =  1817 ; loss =  0.0925564\n",
      "epoch =  1818 ; loss =  0.0922692\n",
      "epoch =  1819 ; loss =  0.0924233\n",
      "epoch =  1820 ; loss =  0.0926821\n",
      "epoch =  1821 ; loss =  0.0928147\n",
      "epoch =  1822 ; loss =  0.0925617\n",
      "epoch =  1823 ; loss =  0.0925551\n",
      "epoch =  1824 ; loss =  0.0925475\n",
      "epoch =  1825 ; loss =  0.0917878\n",
      "epoch =  1826 ; loss =  0.0920191\n",
      "epoch =  1827 ; loss =  0.0922064\n",
      "epoch =  1828 ; loss =  0.0926601\n",
      "epoch =  1829 ; loss =  0.0919589\n",
      "epoch =  1830 ; loss =  0.0923796\n",
      "epoch =  1831 ; loss =  0.092285\n",
      "epoch =  1832 ; loss =  0.0921278\n",
      "epoch =  1833 ; loss =  0.0921366\n",
      "epoch =  1834 ; loss =  0.0912057\n",
      "epoch =  1835 ; loss =  0.0914429\n",
      "epoch =  1836 ; loss =  0.091682\n",
      "epoch =  1837 ; loss =  0.0919623\n",
      "epoch =  1838 ; loss =  0.0922537\n",
      "epoch =  1839 ; loss =  0.0920536\n",
      "epoch =  1840 ; loss =  0.092165\n",
      "epoch =  1841 ; loss =  0.0918502\n",
      "epoch =  1842 ; loss =  0.0920759\n",
      "epoch =  1843 ; loss =  0.0919132\n",
      "epoch =  1844 ; loss =  0.0929725\n",
      "epoch =  1845 ; loss =  0.0938549\n",
      "epoch =  1846 ; loss =  0.0920682\n",
      "epoch =  1847 ; loss =  0.0920927\n",
      "epoch =  1848 ; loss =  0.0916837\n",
      "epoch =  1849 ; loss =  0.0911783\n",
      "epoch =  1850 ; loss =  0.0911922\n",
      "epoch =  1851 ; loss =  0.0921461\n",
      "epoch =  1852 ; loss =  0.0931297\n",
      "epoch =  1853 ; loss =  0.0939701\n",
      "epoch =  1854 ; loss =  0.0934528\n",
      "epoch =  1855 ; loss =  0.0928847\n",
      "epoch =  1856 ; loss =  0.0918175\n",
      "epoch =  1857 ; loss =  0.0914133\n",
      "epoch =  1858 ; loss =  0.0913072\n",
      "epoch =  1859 ; loss =  0.0912775\n",
      "epoch =  1860 ; loss =  0.0904588\n",
      "epoch =  1861 ; loss =  0.0907751\n",
      "epoch =  1862 ; loss =  0.0905497\n",
      "epoch =  1863 ; loss =  0.0904519\n",
      "epoch =  1864 ; loss =  0.0908124\n",
      "epoch =  1865 ; loss =  0.0904998\n",
      "epoch =  1866 ; loss =  0.0909692\n",
      "epoch =  1867 ; loss =  0.0905821\n",
      "epoch =  1868 ; loss =  0.0914601\n",
      "epoch =  1869 ; loss =  0.0910753\n",
      "epoch =  1870 ; loss =  0.0915463\n",
      "epoch =  1871 ; loss =  0.0912471\n",
      "epoch =  1872 ; loss =  0.0906814\n",
      "epoch =  1873 ; loss =  0.0907031\n",
      "epoch =  1874 ; loss =  0.0904582\n",
      "epoch =  1875 ; loss =  0.0907464\n",
      "epoch =  1876 ; loss =  0.0913679\n",
      "epoch =  1877 ; loss =  0.0902785\n",
      "epoch =  1878 ; loss =  0.0906059\n",
      "epoch =  1879 ; loss =  0.0906473\n",
      "epoch =  1880 ; loss =  0.0906068\n",
      "epoch =  1881 ; loss =  0.0906686\n",
      "epoch =  1882 ; loss =  0.0909014\n",
      "epoch =  1883 ; loss =  0.0912478\n",
      "epoch =  1884 ; loss =  0.0908199\n",
      "epoch =  1885 ; loss =  0.0913299\n",
      "epoch =  1886 ; loss =  0.0909028\n",
      "epoch =  1887 ; loss =  0.0909842\n",
      "epoch =  1888 ; loss =  0.0908836\n",
      "epoch =  1889 ; loss =  0.09086\n",
      "epoch =  1890 ; loss =  0.0906706\n",
      "epoch =  1891 ; loss =  0.0906191\n",
      "epoch =  1892 ; loss =  0.0905845\n",
      "epoch =  1893 ; loss =  0.0896903\n",
      "epoch =  1894 ; loss =  0.0908711\n",
      "epoch =  1895 ; loss =  0.0902453\n",
      "epoch =  1896 ; loss =  0.0912929\n",
      "epoch =  1897 ; loss =  0.0901546\n",
      "epoch =  1898 ; loss =  0.0904047\n",
      "epoch =  1899 ; loss =  0.0908349\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1900 ; loss =  0.0847718\n",
      "Model saved in file: model/NoteGen_Fix\n",
      "epoch =  1901 ; loss =  0.0837571\n",
      "epoch =  1902 ; loss =  0.0828545\n",
      "epoch =  1903 ; loss =  0.0833596\n",
      "epoch =  1904 ; loss =  0.0830347\n",
      "epoch =  1905 ; loss =  0.0832954\n",
      "epoch =  1906 ; loss =  0.0835398\n",
      "epoch =  1907 ; loss =  0.0838502\n",
      "epoch =  1908 ; loss =  0.0834266\n",
      "epoch =  1909 ; loss =  0.0832936\n",
      "epoch =  1910 ; loss =  0.083171\n",
      "epoch =  1911 ; loss =  0.0832876\n",
      "epoch =  1912 ; loss =  0.0832285\n",
      "epoch =  1913 ; loss =  0.0830245\n",
      "epoch =  1914 ; loss =  0.0829093\n",
      "epoch =  1915 ; loss =  0.0824578\n",
      "epoch =  1916 ; loss =  0.0826481\n",
      "epoch =  1917 ; loss =  0.0827403\n",
      "epoch =  1918 ; loss =  0.082195\n",
      "epoch =  1919 ; loss =  0.0832424\n",
      "epoch =  1920 ; loss =  0.0822817\n",
      "epoch =  1921 ; loss =  0.0826823\n",
      "epoch =  1922 ; loss =  0.0828839\n",
      "epoch =  1923 ; loss =  0.082626\n",
      "epoch =  1924 ; loss =  0.0827197\n",
      "epoch =  1925 ; loss =  0.0830308\n",
      "epoch =  1926 ; loss =  0.0833804\n",
      "epoch =  1927 ; loss =  0.0825021\n",
      "epoch =  1928 ; loss =  0.0824536\n",
      "epoch =  1929 ; loss =  0.0821292\n",
      "epoch =  1930 ; loss =  0.0829051\n",
      "epoch =  1931 ; loss =  0.0828528\n",
      "epoch =  1932 ; loss =  0.0827521\n",
      "epoch =  1933 ; loss =  0.0824015\n",
      "epoch =  1934 ; loss =  0.0828202\n",
      "epoch =  1935 ; loss =  0.0831613\n",
      "epoch =  1936 ; loss =  0.0826647\n",
      "epoch =  1937 ; loss =  0.0823407\n",
      "epoch =  1938 ; loss =  0.0827874\n",
      "epoch =  1939 ; loss =  0.0827024\n",
      "epoch =  1940 ; loss =  0.08205\n",
      "epoch =  1941 ; loss =  0.0825168\n",
      "epoch =  1942 ; loss =  0.0819515\n",
      "epoch =  1943 ; loss =  0.0816854\n",
      "epoch =  1944 ; loss =  0.0822343\n",
      "epoch =  1945 ; loss =  0.0824815\n",
      "epoch =  1946 ; loss =  0.0828591\n",
      "epoch =  1947 ; loss =  0.0828556\n",
      "epoch =  1948 ; loss =  0.0820176\n",
      "epoch =  1949 ; loss =  0.0824363\n",
      "epoch =  1950 ; loss =  0.0824222\n",
      "epoch =  1951 ; loss =  0.0827473\n",
      "epoch =  1952 ; loss =  0.0821315\n",
      "epoch =  1953 ; loss =  0.081707\n",
      "epoch =  1954 ; loss =  0.082449\n",
      "epoch =  1955 ; loss =  0.082435\n",
      "epoch =  1956 ; loss =  0.0819766\n",
      "epoch =  1957 ; loss =  0.0821089\n",
      "epoch =  1958 ; loss =  0.0816158\n",
      "epoch =  1959 ; loss =  0.0808953\n",
      "epoch =  1960 ; loss =  0.0813877\n",
      "epoch =  1961 ; loss =  0.0816403\n",
      "epoch =  1962 ; loss =  0.0815347\n",
      "epoch =  1963 ; loss =  0.081039\n",
      "epoch =  1964 ; loss =  0.0812446\n",
      "epoch =  1965 ; loss =  0.0812986\n",
      "epoch =  1966 ; loss =  0.0811683\n",
      "epoch =  1967 ; loss =  0.0817301\n",
      "epoch =  1968 ; loss =  0.0824287\n",
      "epoch =  1969 ; loss =  0.0851961\n",
      "epoch =  1970 ; loss =  0.0862712\n",
      "epoch =  1971 ; loss =  0.0848027\n",
      "epoch =  1972 ; loss =  0.085963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1973 ; loss =  0.0844798\n",
      "epoch =  1974 ; loss =  0.0865332\n",
      "epoch =  1975 ; loss =  0.0838152\n",
      "epoch =  1976 ; loss =  0.0816943\n",
      "epoch =  1977 ; loss =  0.0813113\n",
      "epoch =  1978 ; loss =  0.0812632\n",
      "epoch =  1979 ; loss =  0.0813927\n",
      "epoch =  1980 ; loss =  0.0812939\n",
      "epoch =  1981 ; loss =  0.0814408\n",
      "epoch =  1982 ; loss =  0.0814993\n",
      "epoch =  1983 ; loss =  0.0812556\n",
      "epoch =  1984 ; loss =  0.0811521\n",
      "epoch =  1985 ; loss =  0.0809545\n",
      "epoch =  1986 ; loss =  0.0815066\n",
      "epoch =  1987 ; loss =  0.0810815\n",
      "epoch =  1988 ; loss =  0.0811816\n",
      "epoch =  1989 ; loss =  0.0812858\n",
      "epoch =  1990 ; loss =  0.0809084\n",
      "epoch =  1991 ; loss =  0.0809649\n",
      "epoch =  1992 ; loss =  0.0804516\n",
      "epoch =  1993 ; loss =  0.0807242\n",
      "epoch =  1994 ; loss =  0.0804377\n",
      "epoch =  1995 ; loss =  0.0813578\n",
      "epoch =  1996 ; loss =  0.0808195\n",
      "epoch =  1997 ; loss =  0.0807548\n",
      "epoch =  1998 ; loss =  0.0811541\n",
      "epoch =  1999 ; loss =  0.0815424\n",
      " Final Model saved in file: model/NoteGen_Fix\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "N_epochs = 2000\n",
    "loss_hist=[]\n",
    "restore_model_name = 'NoteGen_Fix'\n",
    "save_model_name = 'NoteGen_Fix'\n",
    "batch_size = 10\n",
    "num_timesteps = 128\n",
    "keep_prob=.5\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # try to restore the pre_trained\n",
    "    if restore_model_name is not None:\n",
    "        print(\"Load the model from: {}\".format(restore_model_name))\n",
    "        saver.restore(sess, 'model/{}'.format(restore_model_name))\n",
    "    \n",
    "    # Initial States\n",
    "    timewise_state_val=[]\n",
    "    for i in range(len(num_t_units)):\n",
    "        c_t = np.zeros((batch_size*num_notes, num_t_units[i])) #start every batch with zero state in LSTM time cells\n",
    "        h_t = np.zeros((batch_size*num_notes, num_t_units[i]))\n",
    "        timewise_state_val.append(LSTMStateTuple(h_t, c_t))\n",
    "        \n",
    "    notewise_state_val=[]\n",
    "    for i in range(len(num_n_units)):\n",
    "        c_n = np.zeros((batch_size*num_timesteps, num_n_units[i])) #start every batch with zero state in LSTM time cells\n",
    "        h_n = np.zeros((batch_size*num_timesteps, num_n_units[i]))\n",
    "        notewise_state_val.append(LSTMStateTuple(h_n, c_n))\n",
    "    \n",
    "  \n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(N_epochs):\n",
    "        \n",
    "        #Generate random batch of training data\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print('Obtaining new batch of pieces')\n",
    "            _, batch_input_state = multi_training.getPieceBatch(training_pieces, batch_size, num_timesteps) # not using their 'convolution' filter\n",
    "            batch_input_state = np.array(batch_input_state)\n",
    "            batch_input_state = np.swapaxes(batch_input_state, axis1=1, axis2=2)           \n",
    "     \n",
    "        \n",
    "        \"\"\"\n",
    "        print('Note_State_Batch shape = ', Note_State_Batch.get_shape())\n",
    "        print('batch_input_state shape = ', batch_input_state.shape)\n",
    "        print('')\n",
    "        print('timewise_state shape = ', Note_State_Batch.get_shape())\n",
    "        print('timewise_state_val shape = ', batch_input_state.shape)      \n",
    "        print('')\n",
    "        print('notewise_state shape = ', Note_State_Batch.get_shape())\n",
    "        print('notewise_state_val shape = ', batch_input_state.shape)\n",
    "        ('')\n",
    "        print('time_init shape = ', time_init.get_shape())\n",
    "        \"\"\"\n",
    "    \n",
    "        feed_dict = {Note_State_Batch: batch_input_state, timewise_state: timewise_state_val, notewise_state: notewise_state_val, time_init: 0, output_keep_prob: keep_prob}\n",
    "        #try:\n",
    "        loss_run, _ = sess.run([loss, optimizer], feed_dict=feed_dict)\n",
    "        #except:\n",
    "        #   save_path = saver.save(sess, 'model/{}'.format(save_model_name))\n",
    "        #    print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "        print('epoch = ', epoch, '; loss = ', loss_run)\n",
    "        loss_hist.append(loss_run)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            save_path = saver.save(sess, 'model/{}'.format(save_model_name))\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    save_path = saver.save(sess, 'model/{}'.format(save_model_name))\n",
    "    print(\" Final Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in range(len(tf.trainable_variables())):\n",
    "    print(tf.trainable_variables()[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJzOTTFYIJCwSICAgIgUUCm5AxVpxqdha\ne9WKS+u13l61/fnogl0stZteb3u91bYULdVaW5dbi1SwWLW1pW4EZZF9kSWsIQkkIXvy/f0xkzCE\nLJOQzGRm3s/Hg4czZ75zzicn4zvf+Z5zvsecc4iISHxJinYBIiLS/RTuIiJxSOEuIhKHFO4iInFI\n4S4iEocU7iIicUjhLiIShxTuIiJxSOEuIhKHvNHacE5OjsvPz4/W5kVEYtKqVasOO+dyO2oXtXDP\nz8+noKAgWpsXEYlJZrYrnHYalhERiUMKdxGROKRwFxGJQ1EbcxeR3q2uro7CwkKqq6ujXUpC8vv9\n5OXl4fP5uvR+hbuItKqwsJDMzEzy8/Mxs2iXk1CccxQXF1NYWMiIESO6tI6whmXMbLaZbTazbWY2\nr5XXv2Zmq4P/PjCzBjPr16WKRKRXqK6upn///gr2KDAz+vfvf0rfmjoMdzPzAD8HLgPGAdeb2bjQ\nNs65h5xzk5xzk4B7gTeccyVdrkpEegUFe/Sc6r4Pp+c+FdjmnNvhnKsFngHmtNP+euAPp1RVOzYf\nKOcnr2zmcEVNT21CRCTmhRPuQ4A9Ic8Lg8tOYmZpwGzgj228fruZFZhZQVFRUWdrBWB7UQWPvL6N\n4oraLr1fRGJHRkZGj28jPz+fw4cPNz//+9//zpVXXgnAkiVLeOCBB9p87+rVq1m2bFmP19gV3X0q\n5CeBf7U1JOOcW+icm+Kcm5Kb2+HVs63yJAW+qtQ1NHa5SBGRcFx11VXMm3fSYcZmXQn3+vr6Uy0r\nLOGE+15gaMjzvOCy1lxHDw7JAPg8gXBvaHQ9uRkR6aV27tzJrFmzmDBhAhdffDG7d+8G4Pnnn2f8\n+PFMnDiRGTNmALB+/XqmTp3KpEmTmDBhAlu3bu3Utp544gnuvPPOVtdfW1vLfffdx7PPPsukSZN4\n9tlnKSkp4eqrr2bChAmce+65rF27FoD58+czd+5cLrjgAubOncuMGTNYvXp183YuvPBC1qxZ0x27\np1k4p0KuBEab2QgCoX4dcEPLRmbWB5gJ3NitFbbgSQr8PapvVM9dJFK+9+f1bNhX1q3rHHdaFt/9\n5Fmdft9dd93FzTffzM0338yiRYu4++67Wbx4Mffffz/Lly9nyJAhHDlyBIAFCxbw5S9/mc997nPU\n1tbS0NDQ6jovuugiPB4PABUVFYwdO/akNi3Xn5yczP33309BQQGPPvpoc21nn302ixcv5vXXX+em\nm25qDvENGzawYsUKUlNTefLJJ3niiSd4+OGH2bJlC9XV1UycOLHT+6I9HfbcnXP1wJ3AcmAj8Jxz\nbr2Z3WFmd4Q0/RTwinPuWLdW2IIvOCxT36Ceu0gieuutt7jhhkD/cu7cuaxYsQKACy64gFtuuYXH\nHnusOcTPO+88fvSjH/Hggw+ya9cuUlNTW13n3/72N1avXs3q1at5/PHHW23T2vpbWrFiBXPnzgVg\n1qxZFBcXU1YW+KN41VVXNW//2muv5aWXXqKuro5FixZxyy23dG1ntCOsi5icc8uAZS2WLWjx/Ang\nie4qrC1NY+71GpYRiZiu9LAjbcGCBbzzzjssXbqUyZMns2rVKm644QamTZvG0qVLufzyy/nVr37F\nrFmzum39nZGent78OC0tjUsuuYQXX3yR5557rtPrCkfMzS3j9TQNyyjcRRLR+eefzzPPPAPA008/\nzfTp0wHYvn0706ZN4/777yc3N5c9e/awY8cORo4cyd13382cOXOax8C7orX1Z2ZmUl5e3txm+vTp\nPP3000DgrJucnByysrJaXd9tt93G3XffzUc/+lGys7O7XFdbYm76AW/zsIzG3EXiXWVlJXl5ec3P\n77nnHh555BFuvfVWHnroIXJzc/nNb34DwNe+9jW2bt2Kc46LL76YiRMn8uCDD/LUU0/h8/kYNGgQ\n3/zmN7tcS2vrHzZsGA888ACTJk3i3nvvZf78+Xz+859nwoQJpKWl8eSTT7a5vsmTJ5OVlcWtt97a\n5ZraY85Fpwc8ZcoU15Wbdazfd5QrfraCBTdOZvb4QT1QmYgAbNy4kTPPPDPaZcStffv28bGPfYxN\nmzaRlNT6IEprvwMzW+Wcm9LR+mNuWCbFGyi5Vj13EYlRv/3tb5k2bRo//OEP2wz2UxVzwzIp3sDp\nStW1rR+tFhHp7W666SZuuummHt1GzPXcU5OD4V6vcBfpadEatpVT3/cxF+5+XyDcq9RzF+lRfr+f\n4uJiBXwUNM3n7vf7u7yOmBuWSfN5MINjCneRHpWXl0dhYSFdneRPTk3TnZi6KubCPSnJyEjxUlZV\nF+1SROKaz+fr8l2AJPpiblgGIMvvo6xa4S4i0pbYDPdUH2VVkZk2U0QkFsVmuPu96rmLiLQjNsM9\n1acxdxGRdsRmuPt9lFdrWEZEpC2xGe6pOltGRKQ9sRnufh/lNfW61Z6ISBtiM9xTfQCU66CqiEir\nYjLc+wbD/Uilwl1EpDUxGe790pMBKK2sjXIlIiK9U0yGe7bCXUSkXbEZ7mmBYZmSYxqWERFpTWyG\ne7DnfkQ9dxGRVsVkuGemePEmGSXHFO4iIq0JK9zNbLaZbTazbWY2r402HzOz1Wa23sze6N4yT9oW\nfdOSKdXZMiIirepwPncz8wA/By4BCoGVZrbEObchpE1f4BfAbOfcbjMb0FMFN+mX7qNUPXcRkVaF\n03OfCmxzzu1wztUCzwBzWrS5AXjBObcbwDl3qHvLPFnftGRKNOYuItKqcMJ9CLAn5HlhcFmoMUC2\nmf3dzFaZWau39Taz282swMwKTvXWXf3SktVzFxFpQ3cdUPUCk4ErgEuB75jZmJaNnHMLnXNTnHNT\ncnNzT2mDfVJ9HNXkYSIirQrnHqp7gaEhz/OCy0IVAsXOuWPAMTP7BzAR2NItVbaiT5putSci0pZw\neu4rgdFmNsLMkoHrgCUt2rwIXGhmXjNLA6YBG7u31BP1SfVRXddITX1DT25GRCQmddhzd87Vm9md\nwHLAAyxyzq03szuCry9wzm00s78Aa4FG4HHn3Ac9WXiWP1D60ao6BmR6enJTIiIxJ5xhGZxzy4Bl\nLZYtaPH8IeCh7iutfU3T/pZV1TEg0x+pzYqIxISYvEIVAsMyAEerdLs9EZGWYj7cdbs9EZGTxWy4\nZzX33BXuIiItxWy4N/fcdTqkiMhJYjbcs/zBnrsmDxMROUnMhnuyN4lUn0c9dxGRVsRsuANkpXo1\n5i4i0oqYDvc+qT7KdCqkiMhJYj7cj1RpZkgRkZZiOtz7p6dwuELhLiLSUkyHe05mMsUVNdEuQ0Sk\n14npcM/y+yivrsc5F+1SRER6lZgO90y/j/pGR1Wdpv0VEQkV4+EemNSyvFpnzIiIhIrpcG+aX6Zc\nFzKJiJwgpsO9qedepp67iMgJYjrcszQsIyLSqpgO90y/5nQXEWlNTId708yQ6rmLiJwopsP9+Nky\n6rmLiISK6XBPS/bgSTL13EVEWojpcDczMlK8mtNdRKSFmA53CAzNqOcuInKisMLdzGab2WYz22Zm\n81p5/WNmdtTMVgf/3df9pbYuML+Meu4iIqG8HTUwMw/wc+ASoBBYaWZLnHMbWjT9p3Puyh6osV2Z\nfq8uYhIRaSGcnvtUYJtzbodzrhZ4BpjTs2WFLzM4M6SIiBwXTrgPAfaEPC8MLmvpfDNba2Yvm9lZ\nra3IzG43swIzKygqKupCuSfL8nt1EZOISAvddUD1PWCYc24C8AiwuLVGzrmFzrkpzrkpubm53bLh\nrFSNuYuItBROuO8FhoY8zwsua+acK3POVQQfLwN8ZpbTbVW2I9PvpaJGN+wQEQkVTrivBEab2Qgz\nSwauA5aENjCzQWZmwcdTg+st7u5iW5Pp99Lo4FitbtghItKkw7NlnHP1ZnYnsBzwAIucc+vN7I7g\n6wuAzwD/YWb1QBVwnYtQVzrTf3xO94yUDn8cEZGEEFYaBodalrVYtiDk8aPAo91bWniymmeGrGdw\nn2hUICLS+8TFFaqgycNERELFfLj3z0gG4EBZdZQrERHpPWI+3If3Twdgb2lVlCsREek9Yj7c05M9\nmMGxGl2lKiLSJObDvWna33KFu4hIs5gPd4DMFC8Vml9GRKRZXIR7RvAqVRERCYiPcE9RuIuIhIqP\ncNe0vyIiJ4iPcE/xqOcuIhIiTsLdqytURURCxEW4Z2lYRkTkBHER7n1SfVTWNlDX0BjtUkREeoW4\nCPes1KaZITU0IyICcRPugZkhyzQ0IyICxEm491HPXUTkBHER7k037DiqcBcRAeIl3Jt67jodUkQE\niJNwP343Jo25i4hA3IS7xtxFRELFRbinJ3tIMvXcRUSaxEW4mxmZfp+mIBARCQor3M1stpltNrNt\nZjavnXYfNbN6M/tM95UYnky/Vz13EZGgDsPdzDzAz4HLgHHA9WY2ro12DwKvdHeR4cj0+3QRk4hI\nUDg996nANufcDudcLfAMMKeVdncBfwQOdWN9Ycvye3UqpIhIUDjhPgTYE/K8MLismZkNAT4F/LL7\nSuucTM0MKSLSrLsOqD4MfMM51+60jGZ2u5kVmFlBUVFRN206IMuvOd1FRJp4w2izFxga8jwvuCzU\nFOAZMwPIAS43s3rn3OLQRs65hcBCgClTpriuFt0aHVAVETkunHBfCYw2sxEEQv064IbQBs65EU2P\nzewJ4KWWwd7TslIDp0I65wj+kRERSVgdDss45+qBO4HlwEbgOefcejO7w8zu6OkCw5Xp99Lo4Fht\nQ7RLERGJunB67jjnlgHLWixb0EbbW069rM5rmva39FgtGSlh/VgiInErLq5QBTitbyoA+45URbkS\nEZHoi5twz05LBjSnu4gIxFG4N92wQ1epiojEUbgfn9NdPXcRkbgJ9wzdsENEpFnchLvPk0Raskc3\n7BARIY7CHXSVqohIkzgLdx/lNeq5i4jEVbhnp/korqiNdhkiIlEXV+F+Wt9U9h+tjnYZIiJRF1fh\nnun3UlGjMXcRkbgK9/QULxU6oCoiEl/h3jc1mdqGRl3IJCIJL67CfUROOgA7D1dGuRIRkeiKq3Dv\nlx6YPKy0UmfMiEhii7NwD87prnAXkQQXV+HeNzjt75FKjbmLSGKLr3AP3o2p5Jh67iKS2OIq3L2e\nJPqk+jiiYRkRSXBxFe4QOKhaomEZEUlwcRfu2Wk+SjUsIyIJLu7CvV96isbcRSThxWG4+3QqpIgk\nvLDC3cxmm9lmM9tmZvNaeX2Oma01s9VmVmBmF3Z/qeHJTk+m5FgtzrlolSAiEnUdhruZeYCfA5cB\n44DrzWxci2avAROdc5OAzwOPd3eh4eqXlkxNfSOVtQ3RKkFEJOrC6blPBbY553Y452qBZ4A5oQ2c\ncxXueFc5HYhat7lpCgLdtENEElk44T4E2BPyvDC47ARm9ikz2wQsJdB7j4qczBQAiipqolWCiEjU\nddsBVefcn5xzY4Grge+31sbMbg+OyRcUFRV116ZPkJsRCPfDCncRSWDhhPteYGjI87zgslY55/4B\njDSznFZeW+icm+Kcm5Kbm9vpYsOR29RzL1e4i0jiCifcVwKjzWyEmSUD1wFLQhuY2Sgzs+Djc4AU\noLi7iw1H05i7eu4iksi8HTVwztWb2Z3AcsADLHLOrTezO4KvLwCuAW4yszqgCvg3F6VzEX2eJLLT\nfAp3EUloHYY7gHNuGbCsxbIFIY8fBB7s3tK6LicjRcMyIpLQ4u4KVQiMux/WqZAiksDiNtwPllVH\nuwwRkaiJy3DPy07lwNFq6hsao12KiEhUxGm4p1Hf6DiocXcRSVBxGu6pABSWVEa5EhGR6IjTcE8D\noLC0KsqViIhER1yG+2l9/YDCXUQSV1yGe4rXw8CsFApLNSwjIokpLsMdAkMz6rmLSKKK43BPZY96\n7iKSoOI63HWuu4gkqrgN9yF9da67iCSuuA33pnPd9+hcdxFJQHEb7sP6Bc51V7iLSCKK23Afkp2K\nJ8nYVaxwF5HEE7fh7vMkMaxfGtuLKqJdiohIxMVtuAOMGpDB1kMKdxFJPHEd7qMHZLDz8DHqdDqk\niCSYuA73MwdnUd/oeG9XabRLERGJqLgO91ljB2AGb+0ojnYpIiIRFdfhnp7iJS87lR1Fx6JdiohI\nRMV1uAOMzMlgy8HyaJchIhJRcR/uZw/ry6YD5Rwq1w2zRSRxhBXuZjbbzDab2TYzm9fK658zs7Vm\nts7M3jSzid1fatdcMCoHgLV7jka5EhGRyOkw3M3MA/wcuAwYB1xvZuNaNPsQmOmc+wjwfWBhdxfa\nVWedloUnyVhTeCTapYiIREw4PfepwDbn3A7nXC3wDDAntIFz7k3nXNP5hm8Ded1bZtelJXsZMzCT\n1XsU7iKSOMIJ9yHAnpDnhcFlbfkC8PKpFNXdJg3tw+rdR6ipb4h2KSIiEdGtB1TN7CIC4f6NNl6/\n3cwKzKygqKioOzfdrovOGEB5TT0rP9TFTCKSGMIJ973A0JDnecFlJzCzCcDjwBznXKtXDTnnFjrn\npjjnpuTm5nal3i459/T+JHuTeLZgT8eNRUTiQDjhvhIYbWYjzCwZuA5YEtrAzIYBLwBznXNbur/M\nU5Pl93HRGbn8ec0+aus1z4yIxL8Ow905Vw/cCSwHNgLPOefWm9kdZnZHsNl9QH/gF2a22swKeqzi\nLpo5ZgAAr208GOVKRER6njnnorLhKVOmuIKCyP0NqG9oZOqPXiMnI5lX/t/MiG1XRKQ7mdkq59yU\njtrF/RWqTbyeJC46YwBbDlawYV9ZtMsREelRCRPuANecEziDs6iiJsqViIj0rIQK9z5pPgCqanW+\nu4jEt4QK91SfB4DqOoW7iMS3hAr3DL8XgANlmiFSROJbQoX7gEw/HxnSh9++uVO9dxGJawkV7gBz\nzxvOvqPVLHhje7RLERHpMQkX7tdOzmPS0L4sfn8v0TrHX0SkpyVcuJsZ107JY2dxJX/doKtVRSQ+\nJVy4A1xzTh5D+6Vy1x/e11wzIhKXEjLc/T4Pt5w/gpr6RsZ8u1dNPS8i0i0SMtwBPjdtWPPj77+0\nIYqViIh0v4QNd7/Pw/rvXYrPY/x6xYe88F5htEsSEek2CRvuAOkpXtZ+91JyMpK557k1PPX2rnbb\nf+P/1pI/bymf+J83qKytj1CVIiKdl9DhDpCa7GHxf14AwHcWf8BVj66grqH1g6xNd3LacrCCcfct\n54IHXmfZuv0Rq1VEJFwJH+4AedlprJv/CQDWFh5l9Lde5pl3d5/Urk9qYOKxC0flALD3SBVfevo9\n8uctjciwzvu7S3VuvoiEReEelOn3sfWHl3HlhMEAzHthHfnzlvKvbYeb29Q1NHLbhSP43W3TeOFL\n53PWaVnNr93z3Bry5y0lf95S5jy6gsf/uYMXVx+/UOqpt3dx7YI3eXH13hPmky85VstXn1/D0cq6\nNmv78PAx8uct5VO/eJN/W/g2u4srqaipp7y6jk/8zxsn3F2qsdHx7ocl3bZfRHraLb95l28vXhft\nMuJOwtyJKVzOOW5a9C7/3Ho81KeO6Mc3Zo/ls796iy/OGMnXZ49tfq2x0bHsg/08/OpWth2qaHWd\n/dOTKT5We9LyS88ayPL1gWD+xLiBLLyp9ZurLPzHdn60bFO7dX9j9lgmDe3L9Y+9DcBzXzyPqSP6\ntf/DivQC+fOWArDzgSuiXElsCPdOTAr3dnxn8QcnHWS95fx85l91VqvtnXNs3F/O1b/4F5eeNYg/\nr9l3UpuRuensKDoWdg1f+fhoXl53gM0Hy1k3/xOc/8DrlFeHdzDX70uiuq6Ra87J48FrPoLXoy9q\n0vso3DtH4d5NnHPc8Ng7vLWjGIAfXD2eG88dHtZ7q2ob2FNayXdfXM9bO4p56gtTmT46t3m9x2ob\n2HKwnL9vOsTv3tlNybFaMlK8VNS0Ht4tP/x1DY2UHKvllfUHeOLNnTigb6qP93YfafX9yd4kausb\nyfR7uWPm6WT5vWSnJzNjTC4WfL28up6cjJQTfobUZE9YP69IVyjcO0fh3gOOVtWRmeIlKcl6dDuN\njY4Pi4/xx1WFVNU1UF3XyMwxucweP6jT63LO8f2XNrLoXx92uZ6/fGU6YwdlddxQYppzjudXFTJn\n0mmkeCPzB7025CpxhXt4wg13bySKiRdNZ8v0tKQk4/TcjBPG9rvKzLjvk+P46qVjOFRWQ2qyh9LK\nWjYfKKemrpHfvbOLs4f25cm3djF6QAaFpVVUtZjr/r1dR7oU7i+8V8g9z60B4Pf/Po3zT8/BOceR\nyjqy05PDXs8fVxUyakAGE4f2PWF5cUUNKT4PGSknfozLquswAgfJE0VlbT3j7lvOghvPYfb4wV1a\nx9J1+/n6/62lsLSKey4ZE9Z7fv/Obi4clcOw/mld2mZb31Ll1CncE0Raspf8nMCve2CWvzmsP/vR\noQB8b8745rbOOZyD0spaJv/g1ZPCPlz/vXxz8+MbHnuHMwZmsvlgefOyj+Zn0z89haq6Bj7Ye5S8\n7FTWFB5l7rnDqalv4I0tRRwsO34z85vPG8700bm8saWIRud4+p3d9EtPZtW3P051XSNej+HzJDFh\n/ivN77nojFwuPnMg73xYwpc+djojctKpbWjkxff3MiQ7lRmjc3l/zxHqGhqZmt+PkmO1DMjyd+nn\n7QznHAfKqhncJ5XXNx1k8rB+zff47YrnVgauwbjjd+9x4agcLh0/iOmjcvjDu7t5ae1+/vH1i0gy\n+NLT73HRGQOYdeYA+qcn0+jAk2RsO1TOnb9/H4CjlbXc+8Ja+qYl0z89mUlD+5KV6mNkTjpeTxLO\nOXYVV9I/I5lv/mkdw/un8cbXLupS3R8ePn786cfLNjLvsrGYGfuPVrFqVymzzxpEg3MR+yYRT8Ia\nljGz2cD/Ah7gcefcAy1eHwv8BjgH+JZz7r87WmcsDsskmrqGRkZ/62VO6+Pnvk+OY3j/dMYOysQs\nvGGpS376BlsPVbDgxsk8/c4uNuwro77RcbSq7dM+e4tvX3Emt00fCUBhaSX/++pWfvCp8R2GzE//\nuoWfvbaVMwdnsXF/GX5fEmnJXr5w4QiWrt3PaX1TeXXjQT4zOY//W1XIPZeM4ad/3cKZg7P49NlD\nuHLiYAb3Se10vU+9vYvvLP6gSz/r2cP68n4bx2lCnXVaFtsOVTBjTC5/3XCQu2aN4pHXt5GZ4mXd\n9y7t0ra/+FRB8xljTdKSPVS2cRP7FG8SV3xkMC+8v5dvX3Ems8YOYGRuRofbcc6F/bnt7bptzN3M\nPMAW4BKgEFgJXO+c2xDSZgAwHLgaKFW4x48rfvZP1oecl9+WjBQvF47K4S/rDzBtRD+G90/juYJC\nPjnxNB65/uwT2jY2OpKSjJr6BvaWVpHh91JUXsPmA+Wkp3hZv6+Msqo6vnDhCAb38bNhfxmjBmSw\n+UA56/Ye5cngweMZo3MpKq+hsLSSovIa9h0N3Bv32sl5DO7j52evbzuln337jy7n+YI9vLLhIK9v\nOsTMMbk8+fmp7b6n6eBgV43MSef1r36s0+9btOJD7n9pA5eNH0S/9GSKK2op2FXC6AGZHCyvZtzg\nLF5aG7iaOslg5phc/ra5iD6pPqpqG2hwjobGrh9/++KMkcw8I5fJw7PZUXSMMwefPIxXXl3Hwn/s\n4M5Zo5r/SH76F/9iZ3Elr90zk+8uWc+S4Blmg7L8lByrpbaNq8VDhTNWP+qby5iSn80zt5/XyZ+s\n9+nOcD8PmO+cuzT4/F4A59yPW2k7H6hQuMePxkbHqxsPsnTdfl5cffKpnR25bPwgfnnj5B6orHOa\nhpoKdpWSnebjaFUdqcke/usvm5k5JpfC0qqwDzoP6ZvK0ao6zju9P9ecE/hDkpXqIycjmQseeJ3L\nPzKY+Vedxb4jVby4eh85mSnsP1LFrpJKstN8+L0e3t9zhFW7SgE4//T+vLk9cDbW5OHZ/PE/zu/0\nz/erN7bz45c3sfH+2Z0+u6m6rgGfJwlP8A/uH1ftZUJeH97eUcyALD/VtQ34vMb3/ryBqycN4c9r\n9nF6bgabD5bjnKMs5NTc/P5p7Cyu5B9fu4iHX93CjecN57+Xb2b+VWfx7Mo9/HrFh/zk2olcMzkP\ngKseXUF2WnK7fzTLqutI9XmoqmvgzW3FXHzmAJau3c9Xn19DfaMLK9zj6Yyc7gz3zwCznXO3BZ/P\nBaY55+5spe18FO4JwwV7e1sOVvCXD/YzIjcd56C+ITD0Ut/omD1+ECNy0qNdatiq6xoY+52/nNI6\n7rxoFF+99IxOv+/zT6zkUHk1L901naff2UVtfSO3XjAirPc+8tpWfvLXLWz74WURv57hh0s38Ng/\nwz8ba8rwbMYP6cPv391NbX0js88axIK5ne8AfGfxB7y0dh/v3/eJDtsmYrhH9ICqmd0O3A4wbNiw\nDlpLb2dmeD3GuNOyGHdafJwq6fd52PnAFdTWN1LX0Mh7u0v55p/WMfusQZQcq6O6roH3d5c2DwG1\n5pMTT+vStlOTPXywt4wbH3+HFcFpL360bCMzx+Ty6sZD/Pv0EVw1cQjFx2p4fdMhbjk/nxE56ZgZ\ndQ2NJBlRuVDtW1eM497LzuT5VXvYd6QaM/j1ig8ZOyiTlTtLT2pfsKuUgl3Hl5dUnnz1djhSvEmU\nVtbR0OjYXlTBmIGZrbZrayLA9lTXNdDoHGnJsXvOSTiV7wWGhjzPCy7rNOfcQmAhBHruXVmHSCQk\ne5NI9iYxfXQu//z6rIhss2/wVNsVJ8xn5Hh14yEAHvvnhyf0kH/7VvtTVEdSUpLxbx893mH7yseP\nn0rpnKO6rpEP9h1lzIBM1u87Sk1DI1W1DTzw8ia+Mbvz33IAiioCZ1Kd/s1lAHz67CGUVdfRNy2Z\nH1w9nhRvEjX1jfzy79ub31NRU09GipeDZdXcvOhdUnweKqrr6J+ewrs7S5iY14fdJZWUBud6+uKM\nkQzq4ycGAwpZAAAHLklEQVQt2cPI3Az2lFQyZmAmfp+HUQM6PpAbTeEMy3gJHFC9mECorwRucM6t\nb6XtfDQsI9Ilx2rquf2pwP8TOw9XcrCsmvpOHOQcNSCDV++Z2VPl9TqvbjjIbb/tfIacf3p/Ts/N\n4Km3dzGkbypF5TVhHbht6U9fOp+zh2V3+n2nqluvUDWzy4GHCZwKucg590MzuwPAObfAzAYBBUAW\n0AhUAOOcc22eZqFwFwlPfUMjDc6RHBxycQ6q6xuar0UYmOVnd0klg/v4Y3oYoav2lFSyvaiCt3YU\n4zGjqLyGv20+xOGKwHDPgMwUxg/pw6b9ZScMp/k8RsG3L2m+ONE5x+6SSgZm+altaKSypoGX1u5j\n+foD5GamkJ7sxetJ4g/B6cD/6zMT+OyUoScXFGJX8TFmPvR37ph5OvMuO/WLEkHTD4iInORIZS1H\nqwLj9D5PEkP7df7K2qargQHOHdmPt3eU0D89mTEDA0NOw/unc7SqjmvOyWPJmr1sD04U+Oo9MxiY\n5WfzgXLGDs466crqcCncRUR6yPdf2sBTb+0iO913wlXU4brx3GH84OqPdGnbCncRkQiormvAk2TU\nNzjKa+o4VFaD12O8tvEQ3iTj8o8M5r3dpazaVcr+o9UcLKvmiVun0q8T8yuF6pWnQoqIxBu/L3DR\nmM8TOJ11QGZgbqLQyfaG9ktjzqQhEa1Ld28QEYlDCncRkTikcBcRiUMKdxGROKRwFxGJQwp3EZE4\npHAXEYlDCncRkTgUtStUzawI6OqcpTnA4Q5bRV5vrQt6b22qq3NUV+fEY13DnXO5HTWKWrifCjMr\nCOfy20jrrXVB761NdXWO6uqcRK5LwzIiInFI4S4iEodiNdwXRruANvTWuqD31qa6Okd1dU7C1hWT\nY+4iItK+WO25i4hIO2Iu3M1stpltNrNtZjYvwtseamZ/M7MNZrbezL4cXD7fzPaa2ergv8tD3nNv\nsNbNZnZpD9a208zWBbdfEFzWz8z+amZbg//NDmnf43WZ2Rkh+2S1mZWZ2Veisb/MbJGZHTKzD0KW\ndXr/mNnk4H7eZmY/MzPrgboeMrNNZrbWzP5kZn2Dy/PNrCpkvy2IcF2d/r1FqK5nQ2raaWarg8sj\nub/ayobofcacczHzj8ANurcDI4FkYA2BG3FHavuDgXOCjzOBLcA4YD7w1VbajwvWmAKMCNbu6aHa\ndgI5LZb9FzAv+Hge8GCk62rxuzsADI/G/gJmAOcAH5zK/gHeBc4FDHgZuKwH6voE4A0+fjCkrvzQ\ndi3WE4m6Ov17i0RdLV7/CXBfFPZXW9kQtc9YrPXcpwLbnHM7nHO1wDPAnEht3Dm33zn3XvBxObAR\naO/2KnOAZ5xzNc65D4FtBH6GSJkDPBl8/CRwdRTruhjY7pxr78K1HqvLOfcPoKSV7YW9f8xsMJDl\nnHvbBf4v/G3Ie7qtLufcK865+uDTt4G89tYRqbraEdX91STYw/0s8If21tFDdbWVDVH7jMVauA8B\n9oQ8L6T9cO0xZpYPnA28E1x0V/Br9KKQr16RrNcBr5rZKjO7PbhsoHNuf/DxAWBgFOpqch0n/k8X\n7f0Fnd8/Q4KPI1UfwOcJ9N6ajAgOMbxhZtODyyJZV2d+b5HeX9OBg865rSHLIr6/WmRD1D5jsRbu\nvYKZZQB/BL7inCsDfklgqGgSsJ/AV8NIu9A5Nwm4DPhPM5sR+mKwFxCVU6PMLBm4Cng+uKg37K8T\nRHP/tMXMvgXUA08HF+0HhgV/z/cAvzezrLbe3wN63e+thes5sQMR8f3VSjY0i/RnLNbCfS8wNOR5\nXnBZxJiZj8Av72nn3AsAzrmDzrkG51wj8BjHhxIiVq9zbm/wv4eAPwVrOBj8mtf0VfRQpOsKugx4\nzzl3MFhj1PdXUGf3z15OHCLpsfrM7BbgSuBzwVAg+BW+OPh4FYFx2jGRqqsLv7dI7i8v8Gng2ZB6\nI7q/WssGovgZi7VwXwmMNrMRwd7gdcCSSG08OKb3a2Cjc+6nIcsHhzT7FNB0JH8JcJ2ZpZjZCGA0\ngYMl3V1XupllNj0mcEDug+D2bw42uxl4MZJ1hTihRxXt/RWiU/sn+PW6zMzODX4Wbgp5T7cxs9nA\n14GrnHOVIctzzcwTfDwyWNeOCNbVqd9bpOoK+jiwyTnXPKQRyf3VVjYQzc/YqRwhjsY/4HICR6K3\nA9+K8LYvJPC1ai2wOvjvcuApYF1w+RJgcMh7vhWsdTOneES+nbpGEjjyvgZY37RfgP7Aa8BW4FWg\nXyTrCm4nHSgG+oQsi/j+IvDHZT9QR2Ac8wtd2T/AFAKhth14lOCFgN1c1zYC47FNn7EFwbbXBH+/\nq4H3gE9GuK5O/94iUVdw+RPAHS3aRnJ/tZUNUfuM6QpVEZE4FGvDMiIiEgaFu4hIHFK4i4jEIYW7\niEgcUriLiMQhhbuISBxSuIuIxCGFu4hIHPr/WxT7tEfwm84AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a4a263630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist, label=\"Loss History\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "len(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: NoteGen_Fix\n",
      "INFO:tensorflow:Restoring parameters from model/NoteGen_Fix\n",
      "Timestep =  0\n",
      "Timestep =  50\n",
      "Timestep =  100\n",
      "Timestep =  150\n",
      "Timestep =  200\n",
      "Timestep =  250\n",
      "(5, 78, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "# Music Generation\n",
    "# input = initial note vector\n",
    "# for t = 1:Tsong\n",
    "#    input --> input kernel\n",
    "#    run through 1 'call' of Model LSTM with present parameters / states\n",
    "#    run through note-wise LSTM block as normally done to produce vector of generated samples\n",
    "#    input = generated samples\n",
    "#    music_sequence.append(input)\n",
    "\n",
    "# store batch of music sequences in .MIDI files\n",
    "\n",
    "\n",
    "#Load Model\n",
    "restore_model_name = 'NoteGen_Fix'\n",
    "\n",
    "#Length of generated music\n",
    "T_gen = 16*16\n",
    "batch_gen_size = 5\n",
    "keep_prob = 1\n",
    "\n",
    "# start with initial Note_State_Batch with 't' dimension = 1 (can still a batch of samples run in parallel)\n",
    "notes_gen_initial = np.zeros((batch_gen_size, num_notes, 1,2))\n",
    "\n",
    "# Initial States\n",
    "notes_gen = notes_gen_initial\n",
    "    \n",
    "timewise_state_val=[]\n",
    "for i in range(len(num_t_units)):\n",
    "    c = np.zeros((batch_gen_size*num_notes, num_t_units[i])) #start every batch with zero state in LSTM time cells\n",
    "    h = np.zeros((batch_gen_size*num_notes, num_t_units[i]))\n",
    "    timewise_state_val.append(LSTMStateTuple(h, c))\n",
    "        \n",
    "notewise_state_val=[]\n",
    "for i in range(len(num_n_units)):\n",
    "    c = np.zeros((batch_gen_size*1, num_n_units[i])) #start every batch with zero state in LSTM time cells\n",
    "    h = np.zeros((batch_gen_size*1, num_n_units[i]))\n",
    "    notewise_state_val.append(LSTMStateTuple(h, c))\n",
    "        \n",
    "notes_gen_arr=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print(\"Load the model from: {}\".format(restore_model_name))\n",
    "    saver.restore(sess, 'model/{}'.format(restore_model_name))\n",
    "    \n",
    "\n",
    "    for t in range(T_gen):\n",
    "        feed_dict = {Note_State_Batch: notes_gen, timewise_state: timewise_state_val, notewise_state: notewise_state_val, time_init: t % 16, output_keep_prob: keep_prob}    \n",
    "        timewise_state_val, notes_gen = np.squeeze(sess.run([timewise_state_out, note_gen_out], feed_dict = feed_dict), axis=2)\n",
    "        #print('notes_gen shape = ', notes_gen.shape)\n",
    "        #notes_gen = np.squeeze(notes_gen, axis=2)\n",
    "        notes_gen_arr.append(np.squeeze(notes_gen))\n",
    "        \n",
    "        \n",
    "        \n",
    "        if t % 50 == 0:\n",
    "            print('Timestep = ', t)\n",
    "    \n",
    "notes_gen_out = np.stack(notes_gen_arr, axis=2)\n",
    "print(notes_gen_out.shape)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 256, 78, 2)\n"
     ]
    }
   ],
   "source": [
    "# Save Generate Notes to .MIDI file\n",
    "\n",
    "notes_gen_out = np.swapaxes(notes_gen_out, axis1=1, axis2=2)\n",
    "print(notes_gen_out.shape)\n",
    "#_, notes_gen_out = Utils.multi_training.getPieceBatch(training_pieces)\n",
    "\n",
    "\n",
    "#print(test_batch.shape)\n",
    "for iter in range(3):\n",
    "    file = 'Generated_Midi_Files/NoteGen_Fix' + str(iter)\n",
    "    midi_out = midi_musical_matrix.noteStateMatrixToMidi(notes_gen_out[iter,:,:,:], name=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items to Experiment with:\n",
    "- different T length or variable length T from batch-to-batch for training\n",
    "- categorize music, either through (unsupervised) clustering or (supervised) labeled music folders.  For clustering, the model would possibly find 'k' 'centroids' in an unsupervised manner each with its own music distribution, so during the music generation stage, 1 of these centroids would be selected for a piece of music.  \n",
    "- use encoder to reduce dimensionality of each note vector (vector of 79 notes in 1 time step), similiar to encoding the words from the tweets in homework 3 (i.e. there are restricted combinations of notes that can be played simultaneously)\n",
    "- more advanced sampling/exploring for training/music generation.  This may help prevent the algorithm from getting 'stuck' on a chord, or "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
