{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#% reset\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import math_ops\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import midi_musical_matrix\n",
    "import data\n",
    "import multi_training\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "from MyFunctions import Input_Kernel, LSTM_TimeWise_Training_Layer, LSTM_NoteWise_Layer, Loss_Function\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip bad file =  Fugue12\n",
      "Loaded Fugue8\n",
      "Loaded Fugue22\n",
      "Loaded Fugue18\n",
      "Loaded Fugue5\n",
      "Skip bad file =  Fugue19\n",
      "Loaded Fugue4\n",
      "Loaded Fugue17\n",
      "Loaded Fugue9 (1)\n",
      "Loaded Fugue7 (1)\n",
      "Loaded Fugue3\n",
      "Loaded Fugue7\n",
      "Loaded Fugue16\n",
      "Loaded Fugue3 (1)\n",
      "Loaded Fugue12 (1)\n",
      "Loaded Fugue5 (1)\n",
      "Skip bad file =  Fugue13\n",
      "Skip bad file =  Fugue15\n",
      "Loaded Fugue1\n",
      "Skip bad file =  Fugue6\n",
      "Loaded Fugue1 (1)\n",
      "Loaded Fugue20\n",
      "Skip bad file =  Fugue11\n",
      "Loaded Fugue8 (1)\n",
      "Loaded Fugue23\n",
      "Loaded Fugue2\n",
      "Loaded Fugue24\n",
      "Loaded Fugue9\n",
      "\n",
      "Number of training pieces =  22\n",
      "Sample of State Input Batch: shape =  (15, 78, 128, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import All Training Data\n",
    "# Convert Entire Music .MIDI set to list of musical 'pieces'\n",
    "# During training runs, getPieceBatch will return a tensor for Note_State_Batch, and corresponding Note_State_Expand\n",
    "# Note_State_Expand will be fed into the graph input, and Note_State_Batch will be used for the loss function.\n",
    "\n",
    "# Import Midi files to list\n",
    "Working_Directory = os.getcwd()\n",
    "Training_Midi_Folder = Working_Directory + \"/Midi_Files/Bach_Handpicked\"\n",
    "max_time_steps = 256 # only files atleast this many 16th note steps are saved\n",
    "\n",
    "practice_batch_size = 15\n",
    "practice_num_timesteps = 128\n",
    "\n",
    "\n",
    "training_pieces = multi_training.loadPieces(Training_Midi_Folder, max_time_steps)\n",
    "print('')\n",
    "print('Number of training pieces = ', len(training_pieces))\n",
    "\n",
    "# Generate sample Note State Matrix for dimension measurement and numerical checking purposes\n",
    "# (Using external code to generate the Note State Matrix but using our own NoteInputForm (as defined in author's code) function\n",
    "_, sample_state = multi_training.getPieceBatch(training_pieces, practice_batch_size, practice_num_timesteps)\n",
    "sample_state = np.array(sample_state)\n",
    "sample_state = np.swapaxes(sample_state, axis1=1, axis2=2)\n",
    "print('Sample of State Input Batch: shape = ', sample_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note_State_Batch Placeholder Shape =  (?, 78, ?, 2)\n",
      "Note_State_Expand output Shape =  (?, 78, ?, 80)\n"
     ]
    }
   ],
   "source": [
    "# Beginning of Model Graph:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#input_size = sample_state.shape[-1]\n",
    "num_notes = sample_state.shape[1]\n",
    "\n",
    "\n",
    "#place holder inputs\n",
    "# num_batches and num_time steps are variable lengths.  These values do not affect the model parameters\n",
    "# Dimension(0) =  num_batches. Dimension(2) = num_time_steps\n",
    "\n",
    "#final_t_sample_run = np.zeros((batch_size, num_notes, 1, 2)) #start every batch with zero previous input\n",
    "\n",
    "\n",
    "        \n",
    "Note_State_Batch = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, 2])\n",
    "#prev_t_sample = tf.placeholder(dtype=tf.float32, shape=[None, num_notes,1,2])\n",
    "time_init = tf.placeholder(dtype=tf.int32, shape=())\n",
    "#Generates expanded tensor input to LSTM-timewise layer\n",
    "Note_State_Expand = Input_Kernel(Note_State_Batch, Midi_low=24, Midi_high=101, time_init=time_init)\n",
    "\n",
    "print('Note_State_Batch Placeholder Shape = ', Note_State_Batch.get_shape())\n",
    "print('Note_State_Expand output Shape = ', Note_State_Expand.get_shape())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_expand shape =  (15, 78, 128, 80)\n",
      "MIDI note_0, t_0 =  [ 24.]\n",
      "MIDI note_1, t_0 =  [ 25.]\n",
      "MIDI note_2, t_0 =  [ 26.]\n",
      "MIDI note_0, t_0 =  [ 24.]\n",
      "MIDI note_0, t_1 =  [ 24.]\n",
      "MIDI note_0, t_29 =  [ 24.]\n",
      "\n",
      "pitchclass note_0, t_0 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "pitchclass note_1, t_0 =  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "pitchclass note_11, t_0 =  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "pitchclass note_0, t_0 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "pitchclass note_0, t_1 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "pitchclass note_0, t_29 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "sample state local vicinity =  [[1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 1]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]\n",
      " [0 0]]\n",
      "calculated vicinity note_45, t_29 =  [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "\n",
      "calculated context note_45, t_29 =  [  6.18725338e-08  -8.86581475e-08  -1.08455026e-07   9.99999583e-01\n",
      "   1.43293846e-07   9.99999702e-01  -1.82741658e-07   8.39835295e-08\n",
      "  -1.09147940e-07   2.00144328e-08   1.03597998e-07   9.99999523e-01]\n",
      "actual all note plays at, t_29 =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0]\n",
      "\n",
      "beat note_0, t_0 =  [ 1.  0.  0.  0.]\n",
      "beat note_1, t_0 =  [ 1.  0.  0.  0.]\n",
      "beat note_2, t_0 =  [ 1.  0.  0.  0.]\n",
      "beat note_0, t_0 =  [ 1.  0.  0.  0.]\n",
      "beat note_0, t_1 =  [ 0.  1.  0.  0.]\n",
      "beat note_0, t_29 =  [ 0.  1.  1.  1.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Input Kernel on sample data\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sample_expand = sess.run(Note_State_Expand, feed_dict={Note_State_Batch: sample_state, time_init: 1})\n",
    "\n",
    "\n",
    "\n",
    "#check MIDI note\n",
    "print('sample_expand shape = ', sample_expand.shape)\n",
    "print('MIDI note_0, t_0 = ', sample_expand[0,0,0,[0]]) \n",
    "print('MIDI note_1, t_0 = ', sample_expand[0,1,0,[0]])  \n",
    "print('MIDI note_2, t_0 = ', sample_expand[0,2,0,[0]]) \n",
    "\n",
    "print('MIDI note_0, t_0 = ', sample_expand[0,0,0,[0]]) \n",
    "print('MIDI note_0, t_1 = ', sample_expand[0,0,1,[0]])  \n",
    "print('MIDI note_0, t_29 = ', sample_expand[0,0,29,[0]]) \n",
    "print('') \n",
    "\n",
    "#check pitchclass\n",
    "print('pitchclass note_0, t_0 = ', sample_expand[0,0,0,1:13]) \n",
    "print('pitchclass note_1, t_0 = ', sample_expand[0,1,0,1:13])  \n",
    "print('pitchclass note_11, t_0 = ', sample_expand[0,11,0,1:13]) \n",
    "\n",
    "print('pitchclass note_0, t_0 = ', sample_expand[0,0,0,1:13]) \n",
    "print('pitchclass note_0, t_1 = ', sample_expand[0,0,1,1:13])  \n",
    "print('pitchclass note_0, t_29 = ', sample_expand[0,0,29,1:13]) \n",
    "print('') \n",
    "\n",
    "#check vicinity\n",
    "print('sample state local vicinity = ', sample_state[0,33:58,29,:])\n",
    "print('calculated vicinity note_45, t_29 = ', sample_expand[0,45,29,13:63])\n",
    "print('')\n",
    "\n",
    "#check  context\n",
    "print('calculated context note_45, t_29 = ', sample_expand[0,45,29,63:75])\n",
    "print('actual all note plays at, t_29 = ', sample_state[0,:,29,0])\n",
    "print('')\n",
    "\n",
    "#check beat\n",
    "print('beat note_0, t_0 = ', sample_expand[0,0,0,75:79]) \n",
    "print('beat note_1, t_0 = ', sample_expand[0,1,0,75:79])  \n",
    "print('beat note_2, t_0 = ', sample_expand[0,2,0,75:79]) \n",
    "\n",
    "print('beat note_0, t_0 = ', sample_expand[0,0,0,75:79]) \n",
    "print('beat note_0, t_1 = ', sample_expand[0,0,1,75:79])  \n",
    "print('beat note_0, t_29 = ', sample_expand[0,0,29,75:79]) \n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-wise output shape =  (?, 78, ?, 200)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#lSTM Time Wise Training Graph \n",
    "#tf.reset_default_graph()\n",
    "#Note_State_Expand = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, 80])\n",
    "#Note_State_Expand_val = np.ones((10,78,128,80))\n",
    "\n",
    "num_t_units=[200, 200]\n",
    "output_keep_prob = tf.placeholder(dtype=tf.float32, shape=())\n",
    "\n",
    "# Generate initial state (at t=0) placeholder\n",
    "timewise_state=[]\n",
    "for i in range(len(num_t_units)):\n",
    "    timewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_t_units[i]]) #None = batch_size * num_notes\n",
    "    timewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_t_units[i]])\n",
    "    timewise_state.append(LSTMStateTuple(timewise_h, timewise_c))\n",
    "\n",
    "timewise_state=tuple(timewise_state)\n",
    "\n",
    "\n",
    "timewise_out, timewise_state_out = LSTM_TimeWise_Training_Layer(input_data=Note_State_Expand, state_init=timewise_state, output_keep_prob=output_keep_prob)\n",
    "\n",
    "\n",
    "print('Time-wise output shape = ', timewise_out.get_shape())\n",
    "print(len(timewise_state_out))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_out shape =  (?, 78, ?, 2, 2)\n",
      "generated samples shape =  (?, 78, ?, 2)\n"
     ]
    }
   ],
   "source": [
    "#LSTM Note Wise Graph\n",
    "#tf.reset_default_graph()\n",
    "#num_notes=78\n",
    "#timewise_out = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, 50])\n",
    "#output_keep_prob=1\n",
    "num_n_units = [100, 100]\n",
    "\n",
    "# Generate initial state (at n=0) placeholder\n",
    "notewise_state=[]\n",
    "for i in range(len(num_n_units)):\n",
    "    notewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_n_units[i]]) #None = batch_size * num_timesteps\n",
    "    notewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_n_units[i]])\n",
    "    notewise_state.append(LSTMStateTuple(notewise_h, notewise_c))\n",
    "\n",
    "notewise_state=tuple(notewise_state)\n",
    "\n",
    "\n",
    "y_out, note_gen_out = LSTM_NoteWise_Layer(timewise_out, state_init=notewise_state, output_keep_prob=output_keep_prob)\n",
    "\n",
    "\n",
    "print('y_out shape = ', y_out.get_shape())\n",
    "print('generated samples shape = ', note_gen_out.get_shape())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test for Notewise-LSTM\n",
    "#num_timesteps=5\n",
    "#batch_size=7\n",
    "#timewise_out_val = np.random.randn(batch_size, 78, num_timesteps, 50)*10\n",
    "\n",
    "#notewise_state_val=[]\n",
    "#for i in range(len(num_n_units)):\n",
    "#    c_n = np.zeros((batch_size*num_timesteps, num_n_units[i])) #start every batch with zero state in LSTM time cells\n",
    "#    h_n = np.zeros((batch_size*num_timesteps, num_n_units[i]))\n",
    "#    notewise_state_val.append(LSTMStateTuple(h_n, c_n))\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    sess.run(tf.global_variables_initializer())\n",
    "#    feed_dict = {timewise_out: timewise_out_val, notewise_state: notewise_state_val}\n",
    "#    y_out_run, note_gen_out_run = sess.run([y_out, note_gen_out], feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_align shape = :  (?, ?, ?, ?, ?)\n",
      "Note_State_Batch_align shape = :  (?, ?, ?, ?)\n",
      "cross entropy shape =  (?, ?, ?, ?)\n"
     ]
    }
   ],
   "source": [
    "# Loss Function and Optimizer\n",
    "\n",
    "#y_out_val = np.random.randn(1, 78, 128, 2, 2)*5\n",
    "\n",
    "\n",
    "loss, cross_entropy = Loss_Function(Note_State_Batch, y_out)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate = 1).minimize(loss)\n",
    "\n",
    "\n",
    "\n",
    "#with tf.Session() as sess:\n",
    "#    sess.run(tf.global_variables_initializer())\n",
    "#    cross_entropy_out, loss_out = sess.run([cross_entropy, loss], feed_dict={y_out: y_out_val, Note_State_Batch: batch_input_state})\n",
    "print('cross entropy shape = ', cross_entropy.get_shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining new batch of pieces\n",
      "epoch =  0 ; loss =  0.691344\n",
      "error =  [48650 25362]\n",
      "num_1 =  [3321 2010]\n",
      "epoch =  1 ; loss =  0.666124\n",
      "epoch =  2 ; loss =  0.648802\n",
      "epoch =  3 ; loss =  0.633244\n",
      "epoch =  4 ; loss =  0.617533\n",
      "epoch =  5 ; loss =  0.601373\n",
      "epoch =  6 ; loss =  0.584985\n",
      "epoch =  7 ; loss =  0.567797\n",
      "epoch =  8 ; loss =  0.550634\n",
      "epoch =  9 ; loss =  0.533605\n",
      "epoch =  10 ; loss =  0.515145\n",
      "epoch =  11 ; loss =  0.497537\n",
      "epoch =  12 ; loss =  0.481356\n",
      "epoch =  13 ; loss =  0.463749\n",
      "epoch =  14 ; loss =  0.447935\n",
      "epoch =  15 ; loss =  0.427932\n",
      "epoch =  16 ; loss =  0.410303\n",
      "epoch =  17 ; loss =  0.392965\n",
      "epoch =  18 ; loss =  0.376925\n",
      "epoch =  19 ; loss =  0.361242\n",
      "epoch =  20 ; loss =  0.351831\n",
      "epoch =  21 ; loss =  0.337302\n",
      "epoch =  22 ; loss =  0.328569\n",
      "epoch =  23 ; loss =  0.308442\n",
      "epoch =  24 ; loss =  0.296727\n",
      "epoch =  25 ; loss =  0.287219\n",
      "epoch =  26 ; loss =  0.284803\n",
      "epoch =  27 ; loss =  0.267011\n",
      "epoch =  28 ; loss =  0.259147\n",
      "epoch =  29 ; loss =  0.247412\n",
      "epoch =  30 ; loss =  0.238883\n",
      "epoch =  31 ; loss =  0.232185\n",
      "epoch =  32 ; loss =  0.22686\n",
      "epoch =  33 ; loss =  0.221932\n",
      "epoch =  34 ; loss =  0.223106\n",
      "epoch =  35 ; loss =  0.208601\n",
      "epoch =  36 ; loss =  0.203535\n",
      "epoch =  37 ; loss =  0.198266\n",
      "epoch =  38 ; loss =  0.194786\n",
      "epoch =  39 ; loss =  0.19076\n",
      "epoch =  40 ; loss =  0.187354\n",
      "epoch =  41 ; loss =  0.183697\n",
      "epoch =  42 ; loss =  0.180659\n",
      "epoch =  43 ; loss =  0.177981\n",
      "epoch =  44 ; loss =  0.175427\n",
      "epoch =  45 ; loss =  0.172841\n",
      "epoch =  46 ; loss =  0.170695\n",
      "epoch =  47 ; loss =  0.168421\n",
      "epoch =  48 ; loss =  0.166124\n",
      "epoch =  49 ; loss =  0.164368\n",
      "epoch =  50 ; loss =  0.163181\n",
      "epoch =  51 ; loss =  0.161134\n",
      "epoch =  52 ; loss =  0.160441\n",
      "epoch =  53 ; loss =  0.15908\n",
      "epoch =  54 ; loss =  0.157897\n",
      "epoch =  55 ; loss =  0.156298\n",
      "epoch =  56 ; loss =  0.158751\n",
      "epoch =  57 ; loss =  0.15304\n",
      "epoch =  58 ; loss =  0.152095\n",
      "epoch =  59 ; loss =  0.151109\n",
      "epoch =  60 ; loss =  0.150291\n",
      "epoch =  61 ; loss =  0.150074\n",
      "epoch =  62 ; loss =  0.149946\n",
      "epoch =  63 ; loss =  0.147356\n",
      "epoch =  64 ; loss =  0.147007\n",
      "epoch =  65 ; loss =  0.145958\n",
      "epoch =  66 ; loss =  0.145197\n",
      "epoch =  67 ; loss =  0.144334\n",
      "epoch =  68 ; loss =  0.143716\n",
      "epoch =  69 ; loss =  0.142761\n",
      "epoch =  70 ; loss =  0.142832\n",
      "epoch =  71 ; loss =  0.141092\n",
      "epoch =  72 ; loss =  0.14107\n",
      "epoch =  73 ; loss =  0.140407\n",
      "epoch =  74 ; loss =  0.139648\n",
      "epoch =  75 ; loss =  0.139908\n",
      "epoch =  76 ; loss =  0.139488\n",
      "epoch =  77 ; loss =  0.138499\n",
      "epoch =  78 ; loss =  0.137883\n",
      "epoch =  79 ; loss =  0.136572\n",
      "epoch =  80 ; loss =  0.137116\n",
      "epoch =  81 ; loss =  0.136677\n",
      "epoch =  82 ; loss =  0.137178\n",
      "epoch =  83 ; loss =  0.135308\n",
      "epoch =  84 ; loss =  0.135605\n",
      "epoch =  85 ; loss =  0.13572\n",
      "epoch =  86 ; loss =  0.135298\n",
      "epoch =  87 ; loss =  0.134625\n",
      "epoch =  88 ; loss =  0.134014\n",
      "epoch =  89 ; loss =  0.13367\n",
      "epoch =  90 ; loss =  0.133456\n",
      "epoch =  91 ; loss =  0.133533\n",
      "epoch =  92 ; loss =  0.132463\n",
      "epoch =  93 ; loss =  0.132773\n",
      "epoch =  94 ; loss =  0.132803\n",
      "epoch =  95 ; loss =  0.131756\n",
      "epoch =  96 ; loss =  0.131328\n",
      "epoch =  97 ; loss =  0.132062\n",
      "epoch =  98 ; loss =  0.130468\n",
      "epoch =  99 ; loss =  0.130829\n",
      "Obtaining new batch of pieces\n",
      "epoch =  100 ; loss =  0.128055\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  101 ; loss =  0.127929\n",
      "epoch =  102 ; loss =  0.127513\n",
      "epoch =  103 ; loss =  0.127027\n",
      "epoch =  104 ; loss =  0.126989\n",
      "epoch =  105 ; loss =  0.125875\n",
      "epoch =  106 ; loss =  0.124921\n",
      "epoch =  107 ; loss =  0.125155\n",
      "epoch =  108 ; loss =  0.124906\n",
      "epoch =  109 ; loss =  0.123798\n",
      "epoch =  110 ; loss =  0.124034\n",
      "epoch =  111 ; loss =  0.122741\n",
      "epoch =  112 ; loss =  0.123414\n",
      "epoch =  113 ; loss =  0.122536\n",
      "epoch =  114 ; loss =  0.122985\n",
      "epoch =  115 ; loss =  0.122393\n",
      "epoch =  116 ; loss =  0.122844\n",
      "epoch =  117 ; loss =  0.122341\n",
      "epoch =  118 ; loss =  0.122003\n",
      "epoch =  119 ; loss =  0.121646\n",
      "epoch =  120 ; loss =  0.121739\n",
      "epoch =  121 ; loss =  0.121542\n",
      "epoch =  122 ; loss =  0.121187\n",
      "epoch =  123 ; loss =  0.120847\n",
      "epoch =  124 ; loss =  0.120687\n",
      "epoch =  125 ; loss =  0.120299\n",
      "epoch =  126 ; loss =  0.120202\n",
      "epoch =  127 ; loss =  0.120506\n",
      "epoch =  128 ; loss =  0.119949\n",
      "epoch =  129 ; loss =  0.119109\n",
      "epoch =  130 ; loss =  0.120179\n",
      "epoch =  131 ; loss =  0.119503\n",
      "epoch =  132 ; loss =  0.119428\n",
      "epoch =  133 ; loss =  0.118919\n",
      "epoch =  134 ; loss =  0.119484\n",
      "epoch =  135 ; loss =  0.119397\n",
      "epoch =  136 ; loss =  0.11913\n",
      "epoch =  137 ; loss =  0.118658\n",
      "epoch =  138 ; loss =  0.118426\n",
      "epoch =  139 ; loss =  0.118557\n",
      "epoch =  140 ; loss =  0.117773\n",
      "epoch =  141 ; loss =  0.118254\n",
      "epoch =  142 ; loss =  0.118441\n",
      "epoch =  143 ; loss =  0.117447\n",
      "epoch =  144 ; loss =  0.117294\n",
      "epoch =  145 ; loss =  0.117317\n",
      "epoch =  146 ; loss =  0.117954\n",
      "epoch =  147 ; loss =  0.117069\n",
      "epoch =  148 ; loss =  0.11677\n",
      "epoch =  149 ; loss =  0.116692\n",
      "epoch =  150 ; loss =  0.116863\n",
      "epoch =  151 ; loss =  0.116914\n",
      "epoch =  152 ; loss =  0.116615\n",
      "epoch =  153 ; loss =  0.11666\n",
      "epoch =  154 ; loss =  0.116455\n",
      "epoch =  155 ; loss =  0.115904\n",
      "epoch =  156 ; loss =  0.115712\n",
      "epoch =  157 ; loss =  0.116733\n",
      "epoch =  158 ; loss =  0.116213\n",
      "epoch =  159 ; loss =  0.115782\n",
      "epoch =  160 ; loss =  0.116124\n",
      "epoch =  161 ; loss =  0.116445\n",
      "epoch =  162 ; loss =  0.115447\n",
      "epoch =  163 ; loss =  0.115978\n",
      "epoch =  164 ; loss =  0.115285\n",
      "epoch =  165 ; loss =  0.115742\n",
      "epoch =  166 ; loss =  0.115346\n",
      "epoch =  167 ; loss =  0.11567\n",
      "epoch =  168 ; loss =  0.114905\n",
      "epoch =  169 ; loss =  0.115229\n",
      "epoch =  170 ; loss =  0.114853\n",
      "epoch =  171 ; loss =  0.114996\n",
      "epoch =  172 ; loss =  0.114846\n",
      "epoch =  173 ; loss =  0.115906\n",
      "epoch =  174 ; loss =  0.114631\n",
      "epoch =  175 ; loss =  0.11395\n",
      "epoch =  176 ; loss =  0.114619\n",
      "epoch =  177 ; loss =  0.114133\n",
      "epoch =  178 ; loss =  0.113902\n",
      "epoch =  179 ; loss =  0.114334\n",
      "epoch =  180 ; loss =  0.114352\n",
      "epoch =  181 ; loss =  0.114213\n",
      "epoch =  182 ; loss =  0.114455\n",
      "epoch =  183 ; loss =  0.114556\n",
      "epoch =  184 ; loss =  0.114775\n",
      "epoch =  185 ; loss =  0.11415\n",
      "epoch =  186 ; loss =  0.11384\n",
      "epoch =  187 ; loss =  0.113606\n",
      "epoch =  188 ; loss =  0.113817\n",
      "epoch =  189 ; loss =  0.113536\n",
      "epoch =  190 ; loss =  0.113453\n",
      "epoch =  191 ; loss =  0.113906\n",
      "epoch =  192 ; loss =  0.113333\n",
      "epoch =  193 ; loss =  0.11333\n",
      "epoch =  194 ; loss =  0.11362\n",
      "epoch =  195 ; loss =  0.112887\n",
      "epoch =  196 ; loss =  0.113201\n",
      "epoch =  197 ; loss =  0.112622\n",
      "epoch =  198 ; loss =  0.112629\n",
      "epoch =  199 ; loss =  0.113355\n",
      "Obtaining new batch of pieces\n",
      "epoch =  200 ; loss =  0.125481\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  201 ; loss =  0.124778\n",
      "epoch =  202 ; loss =  0.123541\n",
      "epoch =  203 ; loss =  0.124076\n",
      "epoch =  204 ; loss =  0.123492\n",
      "epoch =  205 ; loss =  0.123059\n",
      "epoch =  206 ; loss =  0.122269\n",
      "epoch =  207 ; loss =  0.122679\n",
      "epoch =  208 ; loss =  0.122849\n",
      "epoch =  209 ; loss =  0.121829\n",
      "epoch =  210 ; loss =  0.1214\n",
      "epoch =  211 ; loss =  0.121508\n",
      "epoch =  212 ; loss =  0.121152\n",
      "epoch =  213 ; loss =  0.120772\n",
      "epoch =  214 ; loss =  0.121213\n",
      "epoch =  215 ; loss =  0.120996\n",
      "epoch =  216 ; loss =  0.120774\n",
      "epoch =  217 ; loss =  0.120814\n",
      "epoch =  218 ; loss =  0.120068\n",
      "epoch =  219 ; loss =  0.119834\n",
      "epoch =  220 ; loss =  0.120399\n",
      "epoch =  221 ; loss =  0.119961\n",
      "epoch =  222 ; loss =  0.12012\n",
      "epoch =  223 ; loss =  0.119706\n",
      "epoch =  224 ; loss =  0.119481\n",
      "epoch =  225 ; loss =  0.120019\n",
      "epoch =  226 ; loss =  0.119944\n",
      "epoch =  227 ; loss =  0.119682\n",
      "epoch =  228 ; loss =  0.119735\n",
      "epoch =  229 ; loss =  0.119555\n",
      "epoch =  230 ; loss =  0.119774\n",
      "epoch =  231 ; loss =  0.119315\n",
      "epoch =  232 ; loss =  0.12001\n",
      "epoch =  233 ; loss =  0.119688\n",
      "epoch =  234 ; loss =  0.11986\n",
      "epoch =  235 ; loss =  0.11993\n",
      "epoch =  236 ; loss =  0.120605\n",
      "epoch =  237 ; loss =  0.120901\n",
      "epoch =  238 ; loss =  0.121093\n",
      "epoch =  239 ; loss =  0.119325\n",
      "epoch =  240 ; loss =  0.119385\n",
      "epoch =  241 ; loss =  0.118733\n",
      "epoch =  242 ; loss =  0.118765\n",
      "epoch =  243 ; loss =  0.11842\n",
      "epoch =  244 ; loss =  0.118925\n",
      "epoch =  245 ; loss =  0.118371\n",
      "epoch =  246 ; loss =  0.118409\n",
      "epoch =  247 ; loss =  0.118457\n",
      "epoch =  248 ; loss =  0.117944\n",
      "epoch =  249 ; loss =  0.118388\n",
      "epoch =  250 ; loss =  0.119096\n",
      "epoch =  251 ; loss =  0.118577\n",
      "epoch =  252 ; loss =  0.118101\n",
      "epoch =  253 ; loss =  0.118179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  254 ; loss =  0.117843\n",
      "epoch =  255 ; loss =  0.118175\n",
      "epoch =  256 ; loss =  0.118568\n",
      "epoch =  257 ; loss =  0.118104\n",
      "epoch =  258 ; loss =  0.118419\n",
      "epoch =  259 ; loss =  0.118002\n",
      "epoch =  260 ; loss =  0.117793\n",
      "epoch =  261 ; loss =  0.118079\n",
      "epoch =  262 ; loss =  0.1179\n",
      "epoch =  263 ; loss =  0.11785\n",
      "epoch =  264 ; loss =  0.117914\n",
      "epoch =  265 ; loss =  0.117667\n",
      "epoch =  266 ; loss =  0.117218\n",
      "epoch =  267 ; loss =  0.117135\n",
      "epoch =  268 ; loss =  0.117553\n",
      "epoch =  269 ; loss =  0.117744\n",
      "epoch =  270 ; loss =  0.117323\n",
      "epoch =  271 ; loss =  0.117235\n",
      "epoch =  272 ; loss =  0.116954\n",
      "epoch =  273 ; loss =  0.117908\n",
      "epoch =  274 ; loss =  0.117561\n",
      "epoch =  275 ; loss =  0.117498\n",
      "epoch =  276 ; loss =  0.11719\n",
      "epoch =  277 ; loss =  0.117567\n",
      "epoch =  278 ; loss =  0.116746\n",
      "epoch =  279 ; loss =  0.116881\n",
      "epoch =  280 ; loss =  0.117161\n",
      "epoch =  281 ; loss =  0.117077\n",
      "epoch =  282 ; loss =  0.11696\n",
      "epoch =  283 ; loss =  0.116771\n",
      "epoch =  284 ; loss =  0.117785\n",
      "epoch =  285 ; loss =  0.117166\n",
      "epoch =  286 ; loss =  0.117405\n",
      "epoch =  287 ; loss =  0.117111\n",
      "epoch =  288 ; loss =  0.116938\n",
      "epoch =  289 ; loss =  0.117163\n",
      "epoch =  290 ; loss =  0.117031\n",
      "epoch =  291 ; loss =  0.117156\n",
      "epoch =  292 ; loss =  0.116602\n",
      "epoch =  293 ; loss =  0.116481\n",
      "epoch =  294 ; loss =  0.116327\n",
      "epoch =  295 ; loss =  0.116678\n",
      "epoch =  296 ; loss =  0.116369\n",
      "epoch =  297 ; loss =  0.116052\n",
      "epoch =  298 ; loss =  0.115894\n",
      "epoch =  299 ; loss =  0.11687\n",
      "Obtaining new batch of pieces\n",
      "epoch =  300 ; loss =  0.115797\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  301 ; loss =  0.115226\n",
      "epoch =  302 ; loss =  0.115509\n",
      "epoch =  303 ; loss =  0.114785\n",
      "epoch =  304 ; loss =  0.114821\n",
      "epoch =  305 ; loss =  0.114753\n",
      "epoch =  306 ; loss =  0.115298\n",
      "epoch =  307 ; loss =  0.114525\n",
      "epoch =  308 ; loss =  0.114381\n",
      "epoch =  309 ; loss =  0.115059\n",
      "epoch =  310 ; loss =  0.114263\n",
      "epoch =  311 ; loss =  0.114434\n",
      "epoch =  312 ; loss =  0.114104\n",
      "epoch =  313 ; loss =  0.114408\n",
      "epoch =  314 ; loss =  0.114461\n",
      "epoch =  315 ; loss =  0.114392\n",
      "epoch =  316 ; loss =  0.114541\n",
      "epoch =  317 ; loss =  0.11411\n",
      "epoch =  318 ; loss =  0.113549\n",
      "epoch =  319 ; loss =  0.114332\n",
      "epoch =  320 ; loss =  0.113688\n",
      "epoch =  321 ; loss =  0.11436\n",
      "epoch =  322 ; loss =  0.113901\n",
      "epoch =  323 ; loss =  0.113621\n",
      "epoch =  324 ; loss =  0.113468\n",
      "epoch =  325 ; loss =  0.113493\n",
      "epoch =  326 ; loss =  0.113246\n",
      "epoch =  327 ; loss =  0.113879\n",
      "epoch =  328 ; loss =  0.113266\n",
      "epoch =  329 ; loss =  0.11328\n",
      "epoch =  330 ; loss =  0.113756\n",
      "epoch =  331 ; loss =  0.113711\n",
      "epoch =  332 ; loss =  0.11377\n",
      "epoch =  333 ; loss =  0.113579\n",
      "epoch =  334 ; loss =  0.113001\n",
      "epoch =  335 ; loss =  0.113907\n",
      "epoch =  336 ; loss =  0.114416\n",
      "epoch =  337 ; loss =  0.113627\n",
      "epoch =  338 ; loss =  0.113375\n",
      "epoch =  339 ; loss =  0.113786\n",
      "epoch =  340 ; loss =  0.113029\n",
      "epoch =  341 ; loss =  0.112949\n",
      "epoch =  342 ; loss =  0.11333\n",
      "epoch =  343 ; loss =  0.112843\n",
      "epoch =  344 ; loss =  0.113483\n",
      "epoch =  345 ; loss =  0.11333\n",
      "epoch =  346 ; loss =  0.112842\n",
      "epoch =  347 ; loss =  0.112794\n",
      "epoch =  348 ; loss =  0.112809\n",
      "epoch =  349 ; loss =  0.113079\n",
      "epoch =  350 ; loss =  0.11292\n",
      "epoch =  351 ; loss =  0.113045\n",
      "epoch =  352 ; loss =  0.11299\n",
      "epoch =  353 ; loss =  0.112939\n",
      "epoch =  354 ; loss =  0.112824\n",
      "epoch =  355 ; loss =  0.113024\n",
      "epoch =  356 ; loss =  0.112747\n",
      "epoch =  357 ; loss =  0.112396\n",
      "epoch =  358 ; loss =  0.112932\n",
      "epoch =  359 ; loss =  0.112968\n",
      "epoch =  360 ; loss =  0.112971\n",
      "epoch =  361 ; loss =  0.112312\n",
      "epoch =  362 ; loss =  0.112846\n",
      "epoch =  363 ; loss =  0.112619\n",
      "epoch =  364 ; loss =  0.112549\n",
      "epoch =  365 ; loss =  0.11227\n",
      "epoch =  366 ; loss =  0.112188\n",
      "epoch =  367 ; loss =  0.112825\n",
      "epoch =  368 ; loss =  0.11247\n",
      "epoch =  369 ; loss =  0.11214\n",
      "epoch =  370 ; loss =  0.111871\n",
      "epoch =  371 ; loss =  0.112219\n",
      "epoch =  372 ; loss =  0.111764\n",
      "epoch =  373 ; loss =  0.112356\n",
      "epoch =  374 ; loss =  0.112331\n",
      "epoch =  375 ; loss =  0.112266\n",
      "epoch =  376 ; loss =  0.112278\n",
      "epoch =  377 ; loss =  0.112078\n",
      "epoch =  378 ; loss =  0.112433\n",
      "epoch =  379 ; loss =  0.111509\n",
      "epoch =  380 ; loss =  0.112316\n",
      "epoch =  381 ; loss =  0.112177\n",
      "epoch =  382 ; loss =  0.112284\n",
      "epoch =  383 ; loss =  0.112022\n",
      "epoch =  384 ; loss =  0.111685\n",
      "epoch =  385 ; loss =  0.112057\n",
      "epoch =  386 ; loss =  0.11265\n",
      "epoch =  387 ; loss =  0.11237\n",
      "epoch =  388 ; loss =  0.112597\n",
      "epoch =  389 ; loss =  0.111983\n",
      "epoch =  390 ; loss =  0.112125\n",
      "epoch =  391 ; loss =  0.111951\n",
      "epoch =  392 ; loss =  0.112133\n",
      "epoch =  393 ; loss =  0.112106\n",
      "epoch =  394 ; loss =  0.111692\n",
      "epoch =  395 ; loss =  0.112128\n",
      "epoch =  396 ; loss =  0.111925\n",
      "epoch =  397 ; loss =  0.111526\n",
      "epoch =  398 ; loss =  0.112062\n",
      "epoch =  399 ; loss =  0.112386\n",
      "Obtaining new batch of pieces\n",
      "epoch =  400 ; loss =  0.120132\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  401 ; loss =  0.119946\n",
      "epoch =  402 ; loss =  0.119476\n",
      "epoch =  403 ; loss =  0.119465\n",
      "epoch =  404 ; loss =  0.118472\n",
      "epoch =  405 ; loss =  0.119283\n",
      "epoch =  406 ; loss =  0.11808\n",
      "epoch =  407 ; loss =  0.118395\n",
      "epoch =  408 ; loss =  0.117755\n",
      "epoch =  409 ; loss =  0.117842\n",
      "epoch =  410 ; loss =  0.11744\n",
      "epoch =  411 ; loss =  0.11788\n",
      "epoch =  412 ; loss =  0.117547\n",
      "epoch =  413 ; loss =  0.11722\n",
      "epoch =  414 ; loss =  0.117308\n",
      "epoch =  415 ; loss =  0.117502\n",
      "epoch =  416 ; loss =  0.117204\n",
      "epoch =  417 ; loss =  0.117036\n",
      "epoch =  418 ; loss =  0.11678\n",
      "epoch =  419 ; loss =  0.117167\n",
      "epoch =  420 ; loss =  0.117038\n",
      "epoch =  421 ; loss =  0.11692\n",
      "epoch =  422 ; loss =  0.116709\n",
      "epoch =  423 ; loss =  0.116339\n",
      "epoch =  424 ; loss =  0.116607\n",
      "epoch =  425 ; loss =  0.116206\n",
      "epoch =  426 ; loss =  0.116661\n",
      "epoch =  427 ; loss =  0.116192\n",
      "epoch =  428 ; loss =  0.116397\n",
      "epoch =  429 ; loss =  0.115937\n",
      "epoch =  430 ; loss =  0.116285\n",
      "epoch =  431 ; loss =  0.11665\n",
      "epoch =  432 ; loss =  0.116213\n",
      "epoch =  433 ; loss =  0.115609\n",
      "epoch =  434 ; loss =  0.115785\n",
      "epoch =  435 ; loss =  0.115766\n",
      "epoch =  436 ; loss =  0.116201\n",
      "epoch =  437 ; loss =  0.115574\n",
      "epoch =  438 ; loss =  0.115683\n",
      "epoch =  439 ; loss =  0.115457\n",
      "epoch =  440 ; loss =  0.116148\n",
      "epoch =  441 ; loss =  0.115982\n",
      "epoch =  442 ; loss =  0.116133\n",
      "epoch =  443 ; loss =  0.116393\n",
      "epoch =  444 ; loss =  0.11552\n",
      "epoch =  445 ; loss =  0.115774\n",
      "epoch =  446 ; loss =  0.11584\n",
      "epoch =  447 ; loss =  0.115811\n",
      "epoch =  448 ; loss =  0.115722\n",
      "epoch =  449 ; loss =  0.115754\n",
      "epoch =  450 ; loss =  0.115786\n",
      "epoch =  451 ; loss =  0.115603\n",
      "epoch =  452 ; loss =  0.115313\n",
      "epoch =  453 ; loss =  0.115651\n",
      "epoch =  454 ; loss =  0.115048\n",
      "epoch =  455 ; loss =  0.115145\n",
      "epoch =  456 ; loss =  0.115361\n",
      "epoch =  457 ; loss =  0.1153\n",
      "epoch =  458 ; loss =  0.115404\n",
      "epoch =  459 ; loss =  0.115032\n",
      "epoch =  460 ; loss =  0.114893\n",
      "epoch =  461 ; loss =  0.115336\n",
      "epoch =  462 ; loss =  0.115224\n",
      "epoch =  463 ; loss =  0.11545\n",
      "epoch =  464 ; loss =  0.115445\n",
      "epoch =  465 ; loss =  0.115751\n",
      "epoch =  466 ; loss =  0.115584\n",
      "epoch =  467 ; loss =  0.115347\n",
      "epoch =  468 ; loss =  0.114676\n",
      "epoch =  469 ; loss =  0.115322\n",
      "epoch =  470 ; loss =  0.115425\n",
      "epoch =  471 ; loss =  0.115328\n",
      "epoch =  472 ; loss =  0.114796\n",
      "epoch =  473 ; loss =  0.115156\n",
      "epoch =  474 ; loss =  0.115149\n",
      "epoch =  475 ; loss =  0.115651\n",
      "epoch =  476 ; loss =  0.11539\n",
      "epoch =  477 ; loss =  0.114971\n",
      "epoch =  478 ; loss =  0.115201\n",
      "epoch =  479 ; loss =  0.114891\n",
      "epoch =  480 ; loss =  0.114705\n",
      "epoch =  481 ; loss =  0.114964\n",
      "epoch =  482 ; loss =  0.115073\n",
      "epoch =  483 ; loss =  0.114958\n",
      "epoch =  484 ; loss =  0.115207\n",
      "epoch =  485 ; loss =  0.114862\n",
      "epoch =  486 ; loss =  0.115577\n",
      "epoch =  487 ; loss =  0.114774\n",
      "epoch =  488 ; loss =  0.115304\n",
      "epoch =  489 ; loss =  0.114863\n",
      "epoch =  490 ; loss =  0.114826\n",
      "epoch =  491 ; loss =  0.114816\n",
      "epoch =  492 ; loss =  0.114708\n",
      "epoch =  493 ; loss =  0.114785\n",
      "epoch =  494 ; loss =  0.114303\n",
      "epoch =  495 ; loss =  0.115054\n",
      "epoch =  496 ; loss =  0.114604\n",
      "epoch =  497 ; loss =  0.114502\n",
      "epoch =  498 ; loss =  0.114743\n",
      "epoch =  499 ; loss =  0.11496\n",
      "Obtaining new batch of pieces\n",
      "epoch =  500 ; loss =  0.113913\n",
      "error =  [7469 1873]\n",
      "num_1 =  [3757 1703]\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  501 ; loss =  0.114092\n",
      "epoch =  502 ; loss =  0.113854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  503 ; loss =  0.113771\n",
      "epoch =  504 ; loss =  0.113604\n",
      "epoch =  505 ; loss =  0.113153\n",
      "epoch =  506 ; loss =  0.113291\n",
      "epoch =  507 ; loss =  0.11255\n",
      "epoch =  508 ; loss =  0.112736\n",
      "epoch =  509 ; loss =  0.112811\n",
      "epoch =  510 ; loss =  0.112618\n",
      "epoch =  511 ; loss =  0.112766\n",
      "epoch =  512 ; loss =  0.112438\n",
      "epoch =  513 ; loss =  0.112638\n",
      "epoch =  514 ; loss =  0.112339\n",
      "epoch =  515 ; loss =  0.112535\n",
      "epoch =  516 ; loss =  0.112522\n",
      "epoch =  517 ; loss =  0.112419\n",
      "epoch =  518 ; loss =  0.11242\n",
      "epoch =  519 ; loss =  0.111976\n",
      "epoch =  520 ; loss =  0.111768\n",
      "epoch =  521 ; loss =  0.112211\n",
      "epoch =  522 ; loss =  0.111986\n",
      "epoch =  523 ; loss =  0.112348\n",
      "epoch =  524 ; loss =  0.112497\n",
      "epoch =  525 ; loss =  0.111819\n",
      "epoch =  526 ; loss =  0.111396\n",
      "epoch =  527 ; loss =  0.111241\n",
      "epoch =  528 ; loss =  0.111784\n",
      "epoch =  529 ; loss =  0.11165\n",
      "epoch =  530 ; loss =  0.11133\n",
      "epoch =  531 ; loss =  0.111938\n",
      "epoch =  532 ; loss =  0.1116\n",
      "epoch =  533 ; loss =  0.111483\n",
      "epoch =  534 ; loss =  0.111757\n",
      "epoch =  535 ; loss =  0.111657\n",
      "epoch =  536 ; loss =  0.1115\n",
      "epoch =  537 ; loss =  0.111837\n",
      "epoch =  538 ; loss =  0.111082\n",
      "epoch =  539 ; loss =  0.111595\n",
      "epoch =  540 ; loss =  0.111666\n",
      "epoch =  541 ; loss =  0.111788\n",
      "epoch =  542 ; loss =  0.111507\n",
      "epoch =  543 ; loss =  0.111857\n",
      "epoch =  544 ; loss =  0.112047\n",
      "epoch =  545 ; loss =  0.111722\n",
      "epoch =  546 ; loss =  0.111241\n",
      "epoch =  547 ; loss =  0.111798\n",
      "epoch =  548 ; loss =  0.111813\n",
      "epoch =  549 ; loss =  0.111442\n",
      "epoch =  550 ; loss =  0.111298\n",
      "epoch =  551 ; loss =  0.111178\n",
      "epoch =  552 ; loss =  0.111236\n",
      "epoch =  553 ; loss =  0.111633\n",
      "epoch =  554 ; loss =  0.111234\n",
      "epoch =  555 ; loss =  0.111099\n",
      "epoch =  556 ; loss =  0.111391\n",
      "epoch =  557 ; loss =  0.111378\n",
      "epoch =  558 ; loss =  0.111203\n",
      "epoch =  559 ; loss =  0.111378\n",
      "epoch =  560 ; loss =  0.110911\n",
      "epoch =  561 ; loss =  0.111301\n",
      "epoch =  562 ; loss =  0.11144\n",
      "epoch =  563 ; loss =  0.111046\n",
      "epoch =  564 ; loss =  0.11112\n",
      "epoch =  565 ; loss =  0.111506\n",
      "epoch =  566 ; loss =  0.111486\n",
      "epoch =  567 ; loss =  0.111472\n",
      "epoch =  568 ; loss =  0.111171\n",
      "epoch =  569 ; loss =  0.110687\n",
      "epoch =  570 ; loss =  0.110918\n",
      "epoch =  571 ; loss =  0.111711\n",
      "epoch =  572 ; loss =  0.111157\n",
      "epoch =  573 ; loss =  0.111206\n",
      "epoch =  574 ; loss =  0.110399\n",
      "epoch =  575 ; loss =  0.110611\n",
      "epoch =  576 ; loss =  0.111045\n",
      "epoch =  577 ; loss =  0.110682\n",
      "epoch =  578 ; loss =  0.110411\n",
      "epoch =  579 ; loss =  0.111275\n",
      "epoch =  580 ; loss =  0.110818\n",
      "epoch =  581 ; loss =  0.110781\n",
      "epoch =  582 ; loss =  0.110903\n",
      "epoch =  583 ; loss =  0.111293\n",
      "epoch =  584 ; loss =  0.111294\n",
      "epoch =  585 ; loss =  0.11068\n",
      "epoch =  586 ; loss =  0.110797\n",
      "epoch =  587 ; loss =  0.111032\n",
      "epoch =  588 ; loss =  0.110576\n",
      "epoch =  589 ; loss =  0.110924\n",
      "epoch =  590 ; loss =  0.1107\n",
      "epoch =  591 ; loss =  0.110871\n",
      "epoch =  592 ; loss =  0.111271\n",
      "epoch =  593 ; loss =  0.111066\n",
      "epoch =  594 ; loss =  0.110804\n",
      "epoch =  595 ; loss =  0.11098\n",
      "epoch =  596 ; loss =  0.111034\n",
      "epoch =  597 ; loss =  0.111473\n",
      "epoch =  598 ; loss =  0.110551\n",
      "epoch =  599 ; loss =  0.110566\n",
      "Obtaining new batch of pieces\n",
      "epoch =  600 ; loss =  0.123809\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  601 ; loss =  0.123344\n",
      "epoch =  602 ; loss =  0.123149\n",
      "epoch =  603 ; loss =  0.122823\n",
      "epoch =  604 ; loss =  0.122591\n",
      "epoch =  605 ; loss =  0.122092\n",
      "epoch =  606 ; loss =  0.121363\n",
      "epoch =  607 ; loss =  0.122293\n",
      "epoch =  608 ; loss =  0.121396\n",
      "epoch =  609 ; loss =  0.121502\n",
      "epoch =  610 ; loss =  0.120822\n",
      "epoch =  611 ; loss =  0.120592\n",
      "epoch =  612 ; loss =  0.120793\n",
      "epoch =  613 ; loss =  0.120137\n",
      "epoch =  614 ; loss =  0.12077\n",
      "epoch =  615 ; loss =  0.120233\n",
      "epoch =  616 ; loss =  0.120686\n",
      "epoch =  617 ; loss =  0.121045\n",
      "epoch =  618 ; loss =  0.120739\n",
      "epoch =  619 ; loss =  0.120488\n",
      "epoch =  620 ; loss =  0.120436\n",
      "epoch =  621 ; loss =  0.120165\n",
      "epoch =  622 ; loss =  0.120384\n",
      "epoch =  623 ; loss =  0.119958\n",
      "epoch =  624 ; loss =  0.119877\n",
      "epoch =  625 ; loss =  0.119291\n",
      "epoch =  626 ; loss =  0.120113\n",
      "epoch =  627 ; loss =  0.119785\n",
      "epoch =  628 ; loss =  0.119464\n",
      "epoch =  629 ; loss =  0.119305\n",
      "epoch =  630 ; loss =  0.119808\n",
      "epoch =  631 ; loss =  0.119324\n",
      "epoch =  632 ; loss =  0.119378\n",
      "epoch =  633 ; loss =  0.119154\n",
      "epoch =  634 ; loss =  0.119281\n",
      "epoch =  635 ; loss =  0.119779\n",
      "epoch =  636 ; loss =  0.119197\n",
      "epoch =  637 ; loss =  0.119557\n",
      "epoch =  638 ; loss =  0.119178\n",
      "epoch =  639 ; loss =  0.119594\n",
      "epoch =  640 ; loss =  0.119056\n",
      "epoch =  641 ; loss =  0.118626\n",
      "epoch =  642 ; loss =  0.119333\n",
      "epoch =  643 ; loss =  0.119056\n",
      "epoch =  644 ; loss =  0.119441\n",
      "epoch =  645 ; loss =  0.119051\n",
      "epoch =  646 ; loss =  0.118906\n",
      "epoch =  647 ; loss =  0.118938\n",
      "epoch =  648 ; loss =  0.119144\n",
      "epoch =  649 ; loss =  0.11871\n",
      "epoch =  650 ; loss =  0.119494\n",
      "epoch =  651 ; loss =  0.119149\n",
      "epoch =  652 ; loss =  0.118764\n",
      "epoch =  653 ; loss =  0.118961\n",
      "epoch =  654 ; loss =  0.11852\n",
      "epoch =  655 ; loss =  0.118633\n",
      "epoch =  656 ; loss =  0.118776\n",
      "epoch =  657 ; loss =  0.118414\n",
      "epoch =  658 ; loss =  0.118712\n",
      "epoch =  659 ; loss =  0.119419\n",
      "epoch =  660 ; loss =  0.119065\n",
      "epoch =  661 ; loss =  0.118192\n",
      "epoch =  662 ; loss =  0.118669\n",
      "epoch =  663 ; loss =  0.118582\n",
      "epoch =  664 ; loss =  0.118553\n",
      "epoch =  665 ; loss =  0.118353\n",
      "epoch =  666 ; loss =  0.119191\n",
      "epoch =  667 ; loss =  0.118902\n",
      "epoch =  668 ; loss =  0.11828\n",
      "epoch =  669 ; loss =  0.118877\n",
      "epoch =  670 ; loss =  0.118325\n",
      "epoch =  671 ; loss =  0.11863\n",
      "epoch =  672 ; loss =  0.11844\n",
      "epoch =  673 ; loss =  0.118395\n",
      "epoch =  674 ; loss =  0.118946\n",
      "epoch =  675 ; loss =  0.118089\n",
      "epoch =  676 ; loss =  0.118399\n",
      "epoch =  677 ; loss =  0.118755\n",
      "epoch =  678 ; loss =  0.118182\n",
      "epoch =  679 ; loss =  0.117465\n",
      "epoch =  680 ; loss =  0.118638\n",
      "epoch =  681 ; loss =  0.118381\n",
      "epoch =  682 ; loss =  0.118415\n",
      "epoch =  683 ; loss =  0.118587\n",
      "epoch =  684 ; loss =  0.118776\n",
      "epoch =  685 ; loss =  0.118323\n",
      "epoch =  686 ; loss =  0.118425\n",
      "epoch =  687 ; loss =  0.118711\n",
      "epoch =  688 ; loss =  0.1184\n",
      "epoch =  689 ; loss =  0.118425\n",
      "epoch =  690 ; loss =  0.118101\n",
      "epoch =  691 ; loss =  0.118549\n",
      "epoch =  692 ; loss =  0.118343\n",
      "epoch =  693 ; loss =  0.118295\n",
      "epoch =  694 ; loss =  0.118093\n",
      "epoch =  695 ; loss =  0.118701\n",
      "epoch =  696 ; loss =  0.117997\n",
      "epoch =  697 ; loss =  0.118348\n",
      "epoch =  698 ; loss =  0.117878\n",
      "epoch =  699 ; loss =  0.117967\n",
      "Obtaining new batch of pieces\n",
      "epoch =  700 ; loss =  0.120966\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  701 ; loss =  0.119521\n",
      "epoch =  702 ; loss =  0.118346\n",
      "epoch =  703 ; loss =  0.118025\n",
      "epoch =  704 ; loss =  0.118389\n",
      "epoch =  705 ; loss =  0.118284\n",
      "epoch =  706 ; loss =  0.118601\n",
      "epoch =  707 ; loss =  0.117793\n",
      "epoch =  708 ; loss =  0.117325\n",
      "epoch =  709 ; loss =  0.116865\n",
      "epoch =  710 ; loss =  0.116616\n",
      "epoch =  711 ; loss =  0.116307\n",
      "epoch =  712 ; loss =  0.116323\n",
      "epoch =  713 ; loss =  0.116617\n",
      "epoch =  714 ; loss =  0.115825\n",
      "epoch =  715 ; loss =  0.115935\n",
      "epoch =  716 ; loss =  0.116011\n",
      "epoch =  717 ; loss =  0.116184\n",
      "epoch =  718 ; loss =  0.116352\n",
      "epoch =  719 ; loss =  0.115928\n",
      "epoch =  720 ; loss =  0.115735\n",
      "epoch =  721 ; loss =  0.115476\n",
      "epoch =  722 ; loss =  0.116507\n",
      "epoch =  723 ; loss =  0.115741\n",
      "epoch =  724 ; loss =  0.115153\n",
      "epoch =  725 ; loss =  0.115122\n",
      "epoch =  726 ; loss =  0.115372\n",
      "epoch =  727 ; loss =  0.115168\n",
      "epoch =  728 ; loss =  0.115188\n",
      "epoch =  729 ; loss =  0.115214\n",
      "epoch =  730 ; loss =  0.115063\n",
      "epoch =  731 ; loss =  0.114504\n",
      "epoch =  732 ; loss =  0.11523\n",
      "epoch =  733 ; loss =  0.115471\n",
      "epoch =  734 ; loss =  0.115512\n",
      "epoch =  735 ; loss =  0.115111\n",
      "epoch =  736 ; loss =  0.114527\n",
      "epoch =  737 ; loss =  0.115076\n",
      "epoch =  738 ; loss =  0.115047\n",
      "epoch =  739 ; loss =  0.114973\n",
      "epoch =  740 ; loss =  0.115042\n",
      "epoch =  741 ; loss =  0.114931\n",
      "epoch =  742 ; loss =  0.114763\n",
      "epoch =  743 ; loss =  0.114679\n",
      "epoch =  744 ; loss =  0.113909\n",
      "epoch =  745 ; loss =  0.114712\n",
      "epoch =  746 ; loss =  0.114822\n",
      "epoch =  747 ; loss =  0.114567\n",
      "epoch =  748 ; loss =  0.115074\n",
      "epoch =  749 ; loss =  0.114519\n",
      "epoch =  750 ; loss =  0.114456\n",
      "epoch =  751 ; loss =  0.114666\n",
      "epoch =  752 ; loss =  0.114667\n",
      "epoch =  753 ; loss =  0.114535\n",
      "epoch =  754 ; loss =  0.114817\n",
      "epoch =  755 ; loss =  0.114395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  756 ; loss =  0.114378\n",
      "epoch =  757 ; loss =  0.114277\n",
      "epoch =  758 ; loss =  0.113927\n",
      "epoch =  759 ; loss =  0.114461\n",
      "epoch =  760 ; loss =  0.114152\n",
      "epoch =  761 ; loss =  0.114424\n",
      "epoch =  762 ; loss =  0.114193\n",
      "epoch =  763 ; loss =  0.113666\n",
      "epoch =  764 ; loss =  0.114063\n",
      "epoch =  765 ; loss =  0.114401\n",
      "epoch =  766 ; loss =  0.113634\n",
      "epoch =  767 ; loss =  0.114222\n",
      "epoch =  768 ; loss =  0.114138\n",
      "epoch =  769 ; loss =  0.114313\n",
      "epoch =  770 ; loss =  0.114219\n",
      "epoch =  771 ; loss =  0.114332\n",
      "epoch =  772 ; loss =  0.114262\n",
      "epoch =  773 ; loss =  0.114356\n",
      "epoch =  774 ; loss =  0.11434\n",
      "epoch =  775 ; loss =  0.114052\n",
      "epoch =  776 ; loss =  0.113682\n",
      "epoch =  777 ; loss =  0.113869\n",
      "epoch =  778 ; loss =  0.113773\n",
      "epoch =  779 ; loss =  0.113745\n",
      "epoch =  780 ; loss =  0.113903\n",
      "epoch =  781 ; loss =  0.113919\n",
      "epoch =  782 ; loss =  0.114242\n",
      "epoch =  783 ; loss =  0.113926\n",
      "epoch =  784 ; loss =  0.11387\n",
      "epoch =  785 ; loss =  0.113637\n",
      "epoch =  786 ; loss =  0.114367\n",
      "epoch =  787 ; loss =  0.113802\n",
      "epoch =  788 ; loss =  0.11388\n",
      "epoch =  789 ; loss =  0.114183\n",
      "epoch =  790 ; loss =  0.113341\n",
      "epoch =  791 ; loss =  0.113975\n",
      "epoch =  792 ; loss =  0.113687\n",
      "epoch =  793 ; loss =  0.11385\n",
      "epoch =  794 ; loss =  0.114057\n",
      "epoch =  795 ; loss =  0.114035\n",
      "epoch =  796 ; loss =  0.113572\n",
      "epoch =  797 ; loss =  0.113616\n",
      "epoch =  798 ; loss =  0.11361\n",
      "epoch =  799 ; loss =  0.113623\n",
      "Obtaining new batch of pieces\n",
      "epoch =  800 ; loss =  0.114624\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  801 ; loss =  0.113891\n",
      "epoch =  802 ; loss =  0.113376\n",
      "epoch =  803 ; loss =  0.112332\n",
      "epoch =  804 ; loss =  0.111688\n",
      "epoch =  805 ; loss =  0.111366\n",
      "epoch =  806 ; loss =  0.110895\n",
      "epoch =  807 ; loss =  0.111379\n",
      "epoch =  808 ; loss =  0.112095\n",
      "epoch =  809 ; loss =  0.111253\n",
      "epoch =  810 ; loss =  0.110456\n",
      "epoch =  811 ; loss =  0.110337\n",
      "epoch =  812 ; loss =  0.109978\n",
      "epoch =  813 ; loss =  0.109581\n",
      "epoch =  814 ; loss =  0.109706\n",
      "epoch =  815 ; loss =  0.109277\n",
      "epoch =  816 ; loss =  0.109307\n",
      "epoch =  817 ; loss =  0.1095\n",
      "epoch =  818 ; loss =  0.108914\n",
      "epoch =  819 ; loss =  0.109061\n",
      "epoch =  820 ; loss =  0.109367\n",
      "epoch =  821 ; loss =  0.109085\n",
      "epoch =  822 ; loss =  0.108647\n",
      "epoch =  823 ; loss =  0.109072\n",
      "epoch =  824 ; loss =  0.108732\n",
      "epoch =  825 ; loss =  0.108698\n",
      "epoch =  826 ; loss =  0.108331\n",
      "epoch =  827 ; loss =  0.108578\n",
      "epoch =  828 ; loss =  0.108175\n",
      "epoch =  829 ; loss =  0.108551\n",
      "epoch =  830 ; loss =  0.108097\n",
      "epoch =  831 ; loss =  0.108331\n",
      "epoch =  832 ; loss =  0.108275\n",
      "epoch =  833 ; loss =  0.108313\n",
      "epoch =  834 ; loss =  0.108342\n",
      "epoch =  835 ; loss =  0.10787\n",
      "epoch =  836 ; loss =  0.108268\n",
      "epoch =  837 ; loss =  0.107885\n",
      "epoch =  838 ; loss =  0.107972\n",
      "epoch =  839 ; loss =  0.107742\n",
      "epoch =  840 ; loss =  0.108216\n",
      "epoch =  841 ; loss =  0.107897\n",
      "epoch =  842 ; loss =  0.107378\n",
      "epoch =  843 ; loss =  0.108255\n",
      "epoch =  844 ; loss =  0.108074\n",
      "epoch =  845 ; loss =  0.108076\n",
      "epoch =  846 ; loss =  0.107764\n",
      "epoch =  847 ; loss =  0.107892\n",
      "epoch =  848 ; loss =  0.108198\n",
      "epoch =  849 ; loss =  0.108135\n",
      "epoch =  850 ; loss =  0.107746\n",
      "epoch =  851 ; loss =  0.10803\n",
      "epoch =  852 ; loss =  0.107878\n",
      "epoch =  853 ; loss =  0.107675\n",
      "epoch =  854 ; loss =  0.107585\n",
      "epoch =  855 ; loss =  0.106969\n",
      "epoch =  856 ; loss =  0.107611\n",
      "epoch =  857 ; loss =  0.107796\n",
      "epoch =  858 ; loss =  0.107567\n",
      "epoch =  859 ; loss =  0.107823\n",
      "epoch =  860 ; loss =  0.10748\n",
      "epoch =  861 ; loss =  0.107321\n",
      "epoch =  862 ; loss =  0.107651\n",
      "epoch =  863 ; loss =  0.107671\n",
      "epoch =  864 ; loss =  0.10738\n",
      "epoch =  865 ; loss =  0.106976\n",
      "epoch =  866 ; loss =  0.107574\n",
      "epoch =  867 ; loss =  0.107979\n",
      "epoch =  868 ; loss =  0.107718\n",
      "epoch =  869 ; loss =  0.107209\n",
      "epoch =  870 ; loss =  0.107793\n",
      "epoch =  871 ; loss =  0.10748\n",
      "epoch =  872 ; loss =  0.107439\n",
      "epoch =  873 ; loss =  0.107259\n",
      "epoch =  874 ; loss =  0.107582\n",
      "epoch =  875 ; loss =  0.107538\n",
      "epoch =  876 ; loss =  0.107633\n",
      "epoch =  877 ; loss =  0.107589\n",
      "epoch =  878 ; loss =  0.10766\n",
      "epoch =  879 ; loss =  0.107029\n",
      "epoch =  880 ; loss =  0.107262\n",
      "epoch =  881 ; loss =  0.107242\n",
      "epoch =  882 ; loss =  0.10756\n",
      "epoch =  883 ; loss =  0.106944\n",
      "epoch =  884 ; loss =  0.107333\n",
      "epoch =  885 ; loss =  0.107724\n",
      "epoch =  886 ; loss =  0.107443\n",
      "epoch =  887 ; loss =  0.107482\n",
      "epoch =  888 ; loss =  0.107577\n",
      "epoch =  889 ; loss =  0.107304\n",
      "epoch =  890 ; loss =  0.107618\n",
      "epoch =  891 ; loss =  0.108017\n",
      "epoch =  892 ; loss =  0.107594\n",
      "epoch =  893 ; loss =  0.106843\n",
      "epoch =  894 ; loss =  0.1069\n",
      "epoch =  895 ; loss =  0.107259\n",
      "epoch =  896 ; loss =  0.10699\n",
      "epoch =  897 ; loss =  0.107727\n",
      "epoch =  898 ; loss =  0.107133\n",
      "epoch =  899 ; loss =  0.107557\n",
      "Obtaining new batch of pieces\n",
      "epoch =  900 ; loss =  0.10677\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  901 ; loss =  0.106611\n",
      "epoch =  902 ; loss =  0.106388\n",
      "epoch =  903 ; loss =  0.106093\n",
      "epoch =  904 ; loss =  0.106412\n",
      "epoch =  905 ; loss =  0.106182\n",
      "epoch =  906 ; loss =  0.106022\n",
      "epoch =  907 ; loss =  0.106041\n",
      "epoch =  908 ; loss =  0.105724\n",
      "epoch =  909 ; loss =  0.105724\n",
      "epoch =  910 ; loss =  0.106332\n",
      "epoch =  911 ; loss =  0.1056\n",
      "epoch =  912 ; loss =  0.105901\n",
      "epoch =  913 ; loss =  0.105976\n",
      "epoch =  914 ; loss =  0.105944\n",
      "epoch =  915 ; loss =  0.105866\n",
      "epoch =  916 ; loss =  0.105922\n",
      "epoch =  917 ; loss =  0.105863\n",
      "epoch =  918 ; loss =  0.105574\n",
      "epoch =  919 ; loss =  0.106022\n",
      "epoch =  920 ; loss =  0.105446\n",
      "epoch =  921 ; loss =  0.105773\n",
      "epoch =  922 ; loss =  0.105344\n",
      "epoch =  923 ; loss =  0.105237\n",
      "epoch =  924 ; loss =  0.105161\n",
      "epoch =  925 ; loss =  0.105652\n",
      "epoch =  926 ; loss =  0.105756\n",
      "epoch =  927 ; loss =  0.105387\n",
      "epoch =  928 ; loss =  0.105323\n",
      "epoch =  929 ; loss =  0.105215\n",
      "epoch =  930 ; loss =  0.105293\n",
      "epoch =  931 ; loss =  0.105258\n",
      "epoch =  932 ; loss =  0.105475\n",
      "epoch =  933 ; loss =  0.105633\n",
      "epoch =  934 ; loss =  0.105325\n",
      "epoch =  935 ; loss =  0.105302\n",
      "epoch =  936 ; loss =  0.10521\n",
      "epoch =  937 ; loss =  0.105102\n",
      "epoch =  938 ; loss =  0.105888\n",
      "epoch =  939 ; loss =  0.105228\n",
      "epoch =  940 ; loss =  0.10519\n",
      "epoch =  941 ; loss =  0.105561\n",
      "epoch =  942 ; loss =  0.105475\n",
      "epoch =  943 ; loss =  0.105147\n",
      "epoch =  944 ; loss =  0.105402\n",
      "epoch =  945 ; loss =  0.105194\n",
      "epoch =  946 ; loss =  0.105208\n",
      "epoch =  947 ; loss =  0.105345\n",
      "epoch =  948 ; loss =  0.105245\n",
      "epoch =  949 ; loss =  0.105466\n",
      "epoch =  950 ; loss =  0.105145\n",
      "epoch =  951 ; loss =  0.105173\n",
      "epoch =  952 ; loss =  0.105223\n",
      "epoch =  953 ; loss =  0.105211\n",
      "epoch =  954 ; loss =  0.10512\n",
      "epoch =  955 ; loss =  0.104719\n",
      "epoch =  956 ; loss =  0.104882\n",
      "epoch =  957 ; loss =  0.105312\n",
      "epoch =  958 ; loss =  0.105704\n",
      "epoch =  959 ; loss =  0.105384\n",
      "epoch =  960 ; loss =  0.104973\n",
      "epoch =  961 ; loss =  0.105674\n",
      "epoch =  962 ; loss =  0.104966\n",
      "epoch =  963 ; loss =  0.104857\n",
      "epoch =  964 ; loss =  0.105433\n",
      "epoch =  965 ; loss =  0.1048\n",
      "epoch =  966 ; loss =  0.10513\n",
      "epoch =  967 ; loss =  0.104737\n",
      "epoch =  968 ; loss =  0.104896\n",
      "epoch =  969 ; loss =  0.105317\n",
      "epoch =  970 ; loss =  0.105113\n",
      "epoch =  971 ; loss =  0.105035\n",
      "epoch =  972 ; loss =  0.104991\n",
      "epoch =  973 ; loss =  0.105224\n",
      "epoch =  974 ; loss =  0.105027\n",
      "epoch =  975 ; loss =  0.104913\n",
      "epoch =  976 ; loss =  0.104718\n",
      "epoch =  977 ; loss =  0.104669\n",
      "epoch =  978 ; loss =  0.105145\n",
      "epoch =  979 ; loss =  0.105023\n",
      "epoch =  980 ; loss =  0.105144\n",
      "epoch =  981 ; loss =  0.105068\n",
      "epoch =  982 ; loss =  0.105049\n",
      "epoch =  983 ; loss =  0.10545\n",
      "epoch =  984 ; loss =  0.104899\n",
      "epoch =  985 ; loss =  0.105008\n",
      "epoch =  986 ; loss =  0.105352\n",
      "epoch =  987 ; loss =  0.10525\n",
      "epoch =  988 ; loss =  0.105201\n",
      "epoch =  989 ; loss =  0.10499\n",
      "epoch =  990 ; loss =  0.105324\n",
      "epoch =  991 ; loss =  0.10523\n",
      "epoch =  992 ; loss =  0.104933\n",
      "epoch =  993 ; loss =  0.104332\n",
      "epoch =  994 ; loss =  0.104858\n",
      "epoch =  995 ; loss =  0.10489\n",
      "epoch =  996 ; loss =  0.104538\n",
      "epoch =  997 ; loss =  0.104532\n",
      "epoch =  998 ; loss =  0.104767\n",
      "epoch =  999 ; loss =  0.104928\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1000 ; loss =  0.112498\n",
      "error =  [6924 1969]\n",
      "num_1 =  [3724 1826]\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1001 ; loss =  0.111732\n",
      "epoch =  1002 ; loss =  0.112533\n",
      "epoch =  1003 ; loss =  0.112162\n",
      "epoch =  1004 ; loss =  0.111594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1005 ; loss =  0.111417\n",
      "epoch =  1006 ; loss =  0.111282\n",
      "epoch =  1007 ; loss =  0.110964\n",
      "epoch =  1008 ; loss =  0.111164\n",
      "epoch =  1009 ; loss =  0.110855\n",
      "epoch =  1010 ; loss =  0.111221\n",
      "epoch =  1011 ; loss =  0.111915\n",
      "epoch =  1012 ; loss =  0.111253\n",
      "epoch =  1013 ; loss =  0.111409\n",
      "epoch =  1014 ; loss =  0.110626\n",
      "epoch =  1015 ; loss =  0.110724\n",
      "epoch =  1016 ; loss =  0.110599\n",
      "epoch =  1017 ; loss =  0.110783\n",
      "epoch =  1018 ; loss =  0.110701\n",
      "epoch =  1019 ; loss =  0.110422\n",
      "epoch =  1020 ; loss =  0.110721\n",
      "epoch =  1021 ; loss =  0.11053\n",
      "epoch =  1022 ; loss =  0.110357\n",
      "epoch =  1023 ; loss =  0.110143\n",
      "epoch =  1024 ; loss =  0.110312\n",
      "epoch =  1025 ; loss =  0.110411\n",
      "epoch =  1026 ; loss =  0.110903\n",
      "epoch =  1027 ; loss =  0.110796\n",
      "epoch =  1028 ; loss =  0.110119\n",
      "epoch =  1029 ; loss =  0.110634\n",
      "epoch =  1030 ; loss =  0.110521\n",
      "epoch =  1031 ; loss =  0.110393\n",
      "epoch =  1032 ; loss =  0.110306\n",
      "epoch =  1033 ; loss =  0.110105\n",
      "epoch =  1034 ; loss =  0.110084\n",
      "epoch =  1035 ; loss =  0.110439\n",
      "epoch =  1036 ; loss =  0.11047\n",
      "epoch =  1037 ; loss =  0.110084\n",
      "epoch =  1038 ; loss =  0.1104\n",
      "epoch =  1039 ; loss =  0.110345\n",
      "epoch =  1040 ; loss =  0.110195\n",
      "epoch =  1041 ; loss =  0.110478\n",
      "epoch =  1042 ; loss =  0.11085\n",
      "epoch =  1043 ; loss =  0.110718\n",
      "epoch =  1044 ; loss =  0.110015\n",
      "epoch =  1045 ; loss =  0.110302\n",
      "epoch =  1046 ; loss =  0.11037\n",
      "epoch =  1047 ; loss =  0.110198\n",
      "epoch =  1048 ; loss =  0.110639\n",
      "epoch =  1049 ; loss =  0.109965\n",
      "epoch =  1050 ; loss =  0.110045\n",
      "epoch =  1051 ; loss =  0.110009\n",
      "epoch =  1052 ; loss =  0.110193\n",
      "epoch =  1053 ; loss =  0.110255\n",
      "epoch =  1054 ; loss =  0.11006\n",
      "epoch =  1055 ; loss =  0.110039\n",
      "epoch =  1056 ; loss =  0.110254\n",
      "epoch =  1057 ; loss =  0.109913\n",
      "epoch =  1058 ; loss =  0.110244\n",
      "epoch =  1059 ; loss =  0.110027\n",
      "epoch =  1060 ; loss =  0.109807\n",
      "epoch =  1061 ; loss =  0.109751\n",
      "epoch =  1062 ; loss =  0.110227\n",
      "epoch =  1063 ; loss =  0.110273\n",
      "epoch =  1064 ; loss =  0.110398\n",
      "epoch =  1065 ; loss =  0.109823\n",
      "epoch =  1066 ; loss =  0.109608\n",
      "epoch =  1067 ; loss =  0.110101\n",
      "epoch =  1068 ; loss =  0.109779\n",
      "epoch =  1069 ; loss =  0.110196\n",
      "epoch =  1070 ; loss =  0.109688\n",
      "epoch =  1071 ; loss =  0.109361\n",
      "epoch =  1072 ; loss =  0.109992\n",
      "epoch =  1073 ; loss =  0.109985\n",
      "epoch =  1074 ; loss =  0.110484\n",
      "epoch =  1075 ; loss =  0.109911\n",
      "epoch =  1076 ; loss =  0.110095\n",
      "epoch =  1077 ; loss =  0.110123\n",
      "epoch =  1078 ; loss =  0.109963\n",
      "epoch =  1079 ; loss =  0.110138\n",
      "epoch =  1080 ; loss =  0.109836\n",
      "epoch =  1081 ; loss =  0.110255\n",
      "epoch =  1082 ; loss =  0.110133\n",
      "epoch =  1083 ; loss =  0.109692\n",
      "epoch =  1084 ; loss =  0.109637\n",
      "epoch =  1085 ; loss =  0.109739\n",
      "epoch =  1086 ; loss =  0.110304\n",
      "epoch =  1087 ; loss =  0.109443\n",
      "epoch =  1088 ; loss =  0.109828\n",
      "epoch =  1089 ; loss =  0.109612\n",
      "epoch =  1090 ; loss =  0.109765\n",
      "epoch =  1091 ; loss =  0.110324\n",
      "epoch =  1092 ; loss =  0.109653\n",
      "epoch =  1093 ; loss =  0.110245\n",
      "epoch =  1094 ; loss =  0.109916\n",
      "epoch =  1095 ; loss =  0.109876\n",
      "epoch =  1096 ; loss =  0.109759\n",
      "epoch =  1097 ; loss =  0.109918\n",
      "epoch =  1098 ; loss =  0.109897\n",
      "epoch =  1099 ; loss =  0.109674\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1100 ; loss =  0.110867\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1101 ; loss =  0.110719\n",
      "epoch =  1102 ; loss =  0.110052\n",
      "epoch =  1103 ; loss =  0.110285\n",
      "epoch =  1104 ; loss =  0.109924\n",
      "epoch =  1105 ; loss =  0.110092\n",
      "epoch =  1106 ; loss =  0.110509\n",
      "epoch =  1107 ; loss =  0.109796\n",
      "epoch =  1108 ; loss =  0.109729\n",
      "epoch =  1109 ; loss =  0.109654\n",
      "epoch =  1110 ; loss =  0.109254\n",
      "epoch =  1111 ; loss =  0.109684\n",
      "epoch =  1112 ; loss =  0.109665\n",
      "epoch =  1113 ; loss =  0.109254\n",
      "epoch =  1114 ; loss =  0.109625\n",
      "epoch =  1115 ; loss =  0.110082\n",
      "epoch =  1116 ; loss =  0.109615\n",
      "epoch =  1117 ; loss =  0.109405\n",
      "epoch =  1118 ; loss =  0.109719\n",
      "epoch =  1119 ; loss =  0.109414\n",
      "epoch =  1120 ; loss =  0.109018\n",
      "epoch =  1121 ; loss =  0.109218\n",
      "epoch =  1122 ; loss =  0.109187\n",
      "epoch =  1123 ; loss =  0.108884\n",
      "epoch =  1124 ; loss =  0.108752\n",
      "epoch =  1125 ; loss =  0.108873\n",
      "epoch =  1126 ; loss =  0.109337\n",
      "epoch =  1127 ; loss =  0.109225\n",
      "epoch =  1128 ; loss =  0.108879\n",
      "epoch =  1129 ; loss =  0.109242\n",
      "epoch =  1130 ; loss =  0.108281\n",
      "epoch =  1131 ; loss =  0.108995\n",
      "epoch =  1132 ; loss =  0.108926\n",
      "epoch =  1133 ; loss =  0.109165\n",
      "epoch =  1134 ; loss =  0.108806\n",
      "epoch =  1135 ; loss =  0.109225\n",
      "epoch =  1136 ; loss =  0.109026\n",
      "epoch =  1137 ; loss =  0.108697\n",
      "epoch =  1138 ; loss =  0.108933\n",
      "epoch =  1139 ; loss =  0.108896\n",
      "epoch =  1140 ; loss =  0.108841\n",
      "epoch =  1141 ; loss =  0.108344\n",
      "epoch =  1142 ; loss =  0.109061\n",
      "epoch =  1143 ; loss =  0.108657\n",
      "epoch =  1144 ; loss =  0.108768\n",
      "epoch =  1145 ; loss =  0.108892\n",
      "epoch =  1146 ; loss =  0.109021\n",
      "epoch =  1147 ; loss =  0.108705\n",
      "epoch =  1148 ; loss =  0.108621\n",
      "epoch =  1149 ; loss =  0.108529\n",
      "epoch =  1150 ; loss =  0.108615\n",
      "epoch =  1151 ; loss =  0.108334\n",
      "epoch =  1152 ; loss =  0.108559\n",
      "epoch =  1153 ; loss =  0.108747\n",
      "epoch =  1154 ; loss =  0.108575\n",
      "epoch =  1155 ; loss =  0.108793\n",
      "epoch =  1156 ; loss =  0.10879\n",
      "epoch =  1157 ; loss =  0.108627\n",
      "epoch =  1158 ; loss =  0.108514\n",
      "epoch =  1159 ; loss =  0.10889\n",
      "epoch =  1160 ; loss =  0.108402\n",
      "epoch =  1161 ; loss =  0.108698\n",
      "epoch =  1162 ; loss =  0.108828\n",
      "epoch =  1163 ; loss =  0.108772\n",
      "epoch =  1164 ; loss =  0.108925\n",
      "epoch =  1165 ; loss =  0.108698\n",
      "epoch =  1166 ; loss =  0.108602\n",
      "epoch =  1167 ; loss =  0.108422\n",
      "epoch =  1168 ; loss =  0.108071\n",
      "epoch =  1169 ; loss =  0.108429\n",
      "epoch =  1170 ; loss =  0.108917\n",
      "epoch =  1171 ; loss =  0.108902\n",
      "epoch =  1172 ; loss =  0.108357\n",
      "epoch =  1173 ; loss =  0.10807\n",
      "epoch =  1174 ; loss =  0.108757\n",
      "epoch =  1175 ; loss =  0.108479\n",
      "epoch =  1176 ; loss =  0.108484\n",
      "epoch =  1177 ; loss =  0.109576\n",
      "epoch =  1178 ; loss =  0.108917\n",
      "epoch =  1179 ; loss =  0.108868\n",
      "epoch =  1180 ; loss =  0.108426\n",
      "epoch =  1181 ; loss =  0.109271\n",
      "epoch =  1182 ; loss =  0.108581\n",
      "epoch =  1183 ; loss =  0.108748\n",
      "epoch =  1184 ; loss =  0.108393\n",
      "epoch =  1185 ; loss =  0.108119\n",
      "epoch =  1186 ; loss =  0.108262\n",
      "epoch =  1187 ; loss =  0.108069\n",
      "epoch =  1188 ; loss =  0.108483\n",
      "epoch =  1189 ; loss =  0.108051\n",
      "epoch =  1190 ; loss =  0.108287\n",
      "epoch =  1191 ; loss =  0.107904\n",
      "epoch =  1192 ; loss =  0.108447\n",
      "epoch =  1193 ; loss =  0.108256\n",
      "epoch =  1194 ; loss =  0.108077\n",
      "epoch =  1195 ; loss =  0.108774\n",
      "epoch =  1196 ; loss =  0.108291\n",
      "epoch =  1197 ; loss =  0.10836\n",
      "epoch =  1198 ; loss =  0.108165\n",
      "epoch =  1199 ; loss =  0.108444\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1200 ; loss =  0.118911\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1201 ; loss =  0.119018\n",
      "epoch =  1202 ; loss =  0.118727\n",
      "epoch =  1203 ; loss =  0.119102\n",
      "epoch =  1204 ; loss =  0.118645\n",
      "epoch =  1205 ; loss =  0.118419\n",
      "epoch =  1206 ; loss =  0.118257\n",
      "epoch =  1207 ; loss =  0.118316\n",
      "epoch =  1208 ; loss =  0.11819\n",
      "epoch =  1209 ; loss =  0.118514\n",
      "epoch =  1210 ; loss =  0.118059\n",
      "epoch =  1211 ; loss =  0.118491\n",
      "epoch =  1212 ; loss =  0.117926\n",
      "epoch =  1213 ; loss =  0.117977\n",
      "epoch =  1214 ; loss =  0.117935\n",
      "epoch =  1215 ; loss =  0.118052\n",
      "epoch =  1216 ; loss =  0.118296\n",
      "epoch =  1217 ; loss =  0.118234\n",
      "epoch =  1218 ; loss =  0.117972\n",
      "epoch =  1219 ; loss =  0.118149\n",
      "epoch =  1220 ; loss =  0.118065\n",
      "epoch =  1221 ; loss =  0.117339\n",
      "epoch =  1222 ; loss =  0.117825\n",
      "epoch =  1223 ; loss =  0.117645\n",
      "epoch =  1224 ; loss =  0.117776\n",
      "epoch =  1225 ; loss =  0.117559\n",
      "epoch =  1226 ; loss =  0.117856\n",
      "epoch =  1227 ; loss =  0.117858\n",
      "epoch =  1228 ; loss =  0.117549\n",
      "epoch =  1229 ; loss =  0.118214\n",
      "epoch =  1230 ; loss =  0.117755\n",
      "epoch =  1231 ; loss =  0.117878\n",
      "epoch =  1232 ; loss =  0.11805\n",
      "epoch =  1233 ; loss =  0.117911\n",
      "epoch =  1234 ; loss =  0.117967\n",
      "epoch =  1235 ; loss =  0.117936\n",
      "epoch =  1236 ; loss =  0.117529\n",
      "epoch =  1237 ; loss =  0.117341\n",
      "epoch =  1238 ; loss =  0.117374\n",
      "epoch =  1239 ; loss =  0.117718\n",
      "epoch =  1240 ; loss =  0.117564\n",
      "epoch =  1241 ; loss =  0.117728\n",
      "epoch =  1242 ; loss =  0.117994\n",
      "epoch =  1243 ; loss =  0.117622\n",
      "epoch =  1244 ; loss =  0.117734\n",
      "epoch =  1245 ; loss =  0.117841\n",
      "epoch =  1246 ; loss =  0.117554\n",
      "epoch =  1247 ; loss =  0.118068\n",
      "epoch =  1248 ; loss =  0.117126\n",
      "epoch =  1249 ; loss =  0.117392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1250 ; loss =  0.117151\n",
      "epoch =  1251 ; loss =  0.117369\n",
      "epoch =  1252 ; loss =  0.117326\n",
      "epoch =  1253 ; loss =  0.117186\n",
      "epoch =  1254 ; loss =  0.117619\n",
      "epoch =  1255 ; loss =  0.117685\n",
      "epoch =  1256 ; loss =  0.117321\n",
      "epoch =  1257 ; loss =  0.118132\n",
      "epoch =  1258 ; loss =  0.117201\n",
      "epoch =  1259 ; loss =  0.118003\n",
      "epoch =  1260 ; loss =  0.117932\n",
      "epoch =  1261 ; loss =  0.118023\n",
      "epoch =  1262 ; loss =  0.117475\n",
      "epoch =  1263 ; loss =  0.11752\n",
      "epoch =  1264 ; loss =  0.117097\n",
      "epoch =  1265 ; loss =  0.117336\n",
      "epoch =  1266 ; loss =  0.117341\n",
      "epoch =  1267 ; loss =  0.117298\n",
      "epoch =  1268 ; loss =  0.117249\n",
      "epoch =  1269 ; loss =  0.117533\n",
      "epoch =  1270 ; loss =  0.117385\n",
      "epoch =  1271 ; loss =  0.117286\n",
      "epoch =  1272 ; loss =  0.117462\n",
      "epoch =  1273 ; loss =  0.117498\n",
      "epoch =  1274 ; loss =  0.117212\n",
      "epoch =  1275 ; loss =  0.117346\n",
      "epoch =  1276 ; loss =  0.117078\n",
      "epoch =  1277 ; loss =  0.117735\n",
      "epoch =  1278 ; loss =  0.117494\n",
      "epoch =  1279 ; loss =  0.117637\n",
      "epoch =  1280 ; loss =  0.117359\n",
      "epoch =  1281 ; loss =  0.117166\n",
      "epoch =  1282 ; loss =  0.117745\n",
      "epoch =  1283 ; loss =  0.117194\n",
      "epoch =  1284 ; loss =  0.117533\n",
      "epoch =  1285 ; loss =  0.117181\n",
      "epoch =  1286 ; loss =  0.117679\n",
      "epoch =  1287 ; loss =  0.116962\n",
      "epoch =  1288 ; loss =  0.117321\n",
      "epoch =  1289 ; loss =  0.117209\n",
      "epoch =  1290 ; loss =  0.11755\n",
      "epoch =  1291 ; loss =  0.11753\n",
      "epoch =  1292 ; loss =  0.116679\n",
      "epoch =  1293 ; loss =  0.117483\n",
      "epoch =  1294 ; loss =  0.117005\n",
      "epoch =  1295 ; loss =  0.117359\n",
      "epoch =  1296 ; loss =  0.1173\n",
      "epoch =  1297 ; loss =  0.117204\n",
      "epoch =  1298 ; loss =  0.117468\n",
      "epoch =  1299 ; loss =  0.117083\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1300 ; loss =  0.116277\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1301 ; loss =  0.116021\n",
      "epoch =  1302 ; loss =  0.115686\n",
      "epoch =  1303 ; loss =  0.115441\n",
      "epoch =  1304 ; loss =  0.115096\n",
      "epoch =  1305 ; loss =  0.115668\n",
      "epoch =  1306 ; loss =  0.115483\n",
      "epoch =  1307 ; loss =  0.115103\n",
      "epoch =  1308 ; loss =  0.115608\n",
      "epoch =  1309 ; loss =  0.11535\n",
      "epoch =  1310 ; loss =  0.115783\n",
      "epoch =  1311 ; loss =  0.115098\n",
      "epoch =  1312 ; loss =  0.115157\n",
      "epoch =  1313 ; loss =  0.115019\n",
      "epoch =  1314 ; loss =  0.115301\n",
      "epoch =  1315 ; loss =  0.11544\n",
      "epoch =  1316 ; loss =  0.114704\n",
      "epoch =  1317 ; loss =  0.114228\n",
      "epoch =  1318 ; loss =  0.114529\n",
      "epoch =  1319 ; loss =  0.11476\n",
      "epoch =  1320 ; loss =  0.114802\n",
      "epoch =  1321 ; loss =  0.114598\n",
      "epoch =  1322 ; loss =  0.114666\n",
      "epoch =  1323 ; loss =  0.114665\n",
      "epoch =  1324 ; loss =  0.114248\n",
      "epoch =  1325 ; loss =  0.114645\n",
      "epoch =  1326 ; loss =  0.114285\n",
      "epoch =  1327 ; loss =  0.114501\n",
      "epoch =  1328 ; loss =  0.114326\n",
      "epoch =  1329 ; loss =  0.114275\n",
      "epoch =  1330 ; loss =  0.114019\n",
      "epoch =  1331 ; loss =  0.114512\n",
      "epoch =  1332 ; loss =  0.114615\n",
      "epoch =  1333 ; loss =  0.11407\n",
      "epoch =  1334 ; loss =  0.114828\n",
      "epoch =  1335 ; loss =  0.114333\n",
      "epoch =  1336 ; loss =  0.114301\n",
      "epoch =  1337 ; loss =  0.114279\n",
      "epoch =  1338 ; loss =  0.114244\n",
      "epoch =  1339 ; loss =  0.114431\n",
      "epoch =  1340 ; loss =  0.114148\n",
      "epoch =  1341 ; loss =  0.114509\n",
      "epoch =  1342 ; loss =  0.114164\n",
      "epoch =  1343 ; loss =  0.114146\n",
      "epoch =  1344 ; loss =  0.114548\n",
      "epoch =  1345 ; loss =  0.114348\n",
      "epoch =  1346 ; loss =  0.114319\n",
      "epoch =  1347 ; loss =  0.114223\n",
      "epoch =  1348 ; loss =  0.114346\n",
      "epoch =  1349 ; loss =  0.114289\n",
      "epoch =  1350 ; loss =  0.113904\n",
      "epoch =  1351 ; loss =  0.114171\n",
      "epoch =  1352 ; loss =  0.114149\n",
      "epoch =  1353 ; loss =  0.11412\n",
      "epoch =  1354 ; loss =  0.114298\n",
      "epoch =  1355 ; loss =  0.114179\n",
      "epoch =  1356 ; loss =  0.114094\n",
      "epoch =  1357 ; loss =  0.113691\n",
      "epoch =  1358 ; loss =  0.114269\n",
      "epoch =  1359 ; loss =  0.114594\n",
      "epoch =  1360 ; loss =  0.114008\n",
      "epoch =  1361 ; loss =  0.114044\n",
      "epoch =  1362 ; loss =  0.114134\n",
      "epoch =  1363 ; loss =  0.11423\n",
      "epoch =  1364 ; loss =  0.11442\n",
      "epoch =  1365 ; loss =  0.114008\n",
      "epoch =  1366 ; loss =  0.114451\n",
      "epoch =  1367 ; loss =  0.114222\n",
      "epoch =  1368 ; loss =  0.113841\n",
      "epoch =  1369 ; loss =  0.114018\n",
      "epoch =  1370 ; loss =  0.11404\n",
      "epoch =  1371 ; loss =  0.114219\n",
      "epoch =  1372 ; loss =  0.114134\n",
      "epoch =  1373 ; loss =  0.113806\n",
      "epoch =  1374 ; loss =  0.114108\n",
      "epoch =  1375 ; loss =  0.114144\n",
      "epoch =  1376 ; loss =  0.113952\n",
      "epoch =  1377 ; loss =  0.114058\n",
      "epoch =  1378 ; loss =  0.114364\n",
      "epoch =  1379 ; loss =  0.114146\n",
      "epoch =  1380 ; loss =  0.113956\n",
      "epoch =  1381 ; loss =  0.113887\n",
      "epoch =  1382 ; loss =  0.114176\n",
      "epoch =  1383 ; loss =  0.114024\n",
      "epoch =  1384 ; loss =  0.114117\n",
      "epoch =  1385 ; loss =  0.114459\n",
      "epoch =  1386 ; loss =  0.114053\n",
      "epoch =  1387 ; loss =  0.114803\n",
      "epoch =  1388 ; loss =  0.114432\n",
      "epoch =  1389 ; loss =  0.114141\n",
      "epoch =  1390 ; loss =  0.114317\n",
      "epoch =  1391 ; loss =  0.114278\n",
      "epoch =  1392 ; loss =  0.113553\n",
      "epoch =  1393 ; loss =  0.113491\n",
      "epoch =  1394 ; loss =  0.113784\n",
      "epoch =  1395 ; loss =  0.115093\n",
      "epoch =  1396 ; loss =  0.114538\n",
      "epoch =  1397 ; loss =  0.113972\n",
      "epoch =  1398 ; loss =  0.113976\n",
      "epoch =  1399 ; loss =  0.114108\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1400 ; loss =  0.114562\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1401 ; loss =  0.114039\n",
      "epoch =  1402 ; loss =  0.113836\n",
      "epoch =  1403 ; loss =  0.11387\n",
      "epoch =  1404 ; loss =  0.113469\n",
      "epoch =  1405 ; loss =  0.113155\n",
      "epoch =  1406 ; loss =  0.113191\n",
      "epoch =  1407 ; loss =  0.113365\n",
      "epoch =  1408 ; loss =  0.112865\n",
      "epoch =  1409 ; loss =  0.112667\n",
      "epoch =  1410 ; loss =  0.112721\n",
      "epoch =  1411 ; loss =  0.11234\n",
      "epoch =  1412 ; loss =  0.11272\n",
      "epoch =  1413 ; loss =  0.112851\n",
      "epoch =  1414 ; loss =  0.112453\n",
      "epoch =  1415 ; loss =  0.11243\n",
      "epoch =  1416 ; loss =  0.112487\n",
      "epoch =  1417 ; loss =  0.112062\n",
      "epoch =  1418 ; loss =  0.112593\n",
      "epoch =  1419 ; loss =  0.112117\n",
      "epoch =  1420 ; loss =  0.111907\n",
      "epoch =  1421 ; loss =  0.11221\n",
      "epoch =  1422 ; loss =  0.112043\n",
      "epoch =  1423 ; loss =  0.111761\n",
      "epoch =  1424 ; loss =  0.112093\n",
      "epoch =  1425 ; loss =  0.112129\n",
      "epoch =  1426 ; loss =  0.111771\n",
      "epoch =  1427 ; loss =  0.112123\n",
      "epoch =  1428 ; loss =  0.112171\n",
      "epoch =  1429 ; loss =  0.111814\n",
      "epoch =  1430 ; loss =  0.111772\n",
      "epoch =  1431 ; loss =  0.112007\n",
      "epoch =  1432 ; loss =  0.111608\n",
      "epoch =  1433 ; loss =  0.111914\n",
      "epoch =  1434 ; loss =  0.112188\n",
      "epoch =  1435 ; loss =  0.111855\n",
      "epoch =  1436 ; loss =  0.111593\n",
      "epoch =  1437 ; loss =  0.112064\n",
      "epoch =  1438 ; loss =  0.111755\n",
      "epoch =  1439 ; loss =  0.111829\n",
      "epoch =  1440 ; loss =  0.111971\n",
      "epoch =  1441 ; loss =  0.111902\n",
      "epoch =  1442 ; loss =  0.111459\n",
      "epoch =  1443 ; loss =  0.111677\n",
      "epoch =  1444 ; loss =  0.111793\n",
      "epoch =  1445 ; loss =  0.111717\n",
      "epoch =  1446 ; loss =  0.111693\n",
      "epoch =  1447 ; loss =  0.111698\n",
      "epoch =  1448 ; loss =  0.1114\n",
      "epoch =  1449 ; loss =  0.111945\n",
      "epoch =  1450 ; loss =  0.111539\n",
      "epoch =  1451 ; loss =  0.111667\n",
      "epoch =  1452 ; loss =  0.111406\n",
      "epoch =  1453 ; loss =  0.111323\n",
      "epoch =  1454 ; loss =  0.11165\n",
      "epoch =  1455 ; loss =  0.111767\n",
      "epoch =  1456 ; loss =  0.111837\n",
      "epoch =  1457 ; loss =  0.111922\n",
      "epoch =  1458 ; loss =  0.111706\n",
      "epoch =  1459 ; loss =  0.11224\n",
      "epoch =  1460 ; loss =  0.111675\n",
      "epoch =  1461 ; loss =  0.111606\n",
      "epoch =  1462 ; loss =  0.111752\n",
      "epoch =  1463 ; loss =  0.111893\n",
      "epoch =  1464 ; loss =  0.111859\n",
      "epoch =  1465 ; loss =  0.111743\n",
      "epoch =  1466 ; loss =  0.111333\n",
      "epoch =  1467 ; loss =  0.111807\n",
      "epoch =  1468 ; loss =  0.111133\n",
      "epoch =  1469 ; loss =  0.111374\n",
      "epoch =  1470 ; loss =  0.111657\n",
      "epoch =  1471 ; loss =  0.111765\n",
      "epoch =  1472 ; loss =  0.111812\n",
      "epoch =  1473 ; loss =  0.111351\n",
      "epoch =  1474 ; loss =  0.111528\n",
      "epoch =  1475 ; loss =  0.111173\n",
      "epoch =  1476 ; loss =  0.111398\n",
      "epoch =  1477 ; loss =  0.111864\n",
      "epoch =  1478 ; loss =  0.111376\n",
      "epoch =  1479 ; loss =  0.111279\n",
      "epoch =  1480 ; loss =  0.111653\n",
      "epoch =  1481 ; loss =  0.111471\n",
      "epoch =  1482 ; loss =  0.111287\n",
      "epoch =  1483 ; loss =  0.111825\n",
      "epoch =  1484 ; loss =  0.111256\n",
      "epoch =  1485 ; loss =  0.111441\n",
      "epoch =  1486 ; loss =  0.111558\n",
      "epoch =  1487 ; loss =  0.111318\n",
      "epoch =  1488 ; loss =  0.111305\n",
      "epoch =  1489 ; loss =  0.111229\n",
      "epoch =  1490 ; loss =  0.111194\n",
      "epoch =  1491 ; loss =  0.111641\n",
      "epoch =  1492 ; loss =  0.111163\n",
      "epoch =  1493 ; loss =  0.11108\n",
      "epoch =  1494 ; loss =  0.11142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1495 ; loss =  0.11168\n",
      "epoch =  1496 ; loss =  0.110947\n",
      "epoch =  1497 ; loss =  0.111057\n",
      "epoch =  1498 ; loss =  0.111292\n",
      "epoch =  1499 ; loss =  0.111288\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1500 ; loss =  0.114013\n",
      "error =  [7059 2124]\n",
      "num_1 =  [3646 1930]\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1501 ; loss =  0.11331\n",
      "epoch =  1502 ; loss =  0.112421\n",
      "epoch =  1503 ; loss =  0.112639\n",
      "epoch =  1504 ; loss =  0.112531\n",
      "epoch =  1505 ; loss =  0.113091\n",
      "epoch =  1506 ; loss =  0.112666\n",
      "epoch =  1507 ; loss =  0.112019\n",
      "epoch =  1508 ; loss =  0.112231\n",
      "epoch =  1509 ; loss =  0.112675\n",
      "epoch =  1510 ; loss =  0.112241\n",
      "epoch =  1511 ; loss =  0.1122\n",
      "epoch =  1512 ; loss =  0.11169\n",
      "epoch =  1513 ; loss =  0.112221\n",
      "epoch =  1514 ; loss =  0.112605\n",
      "epoch =  1515 ; loss =  0.11226\n",
      "epoch =  1516 ; loss =  0.1117\n",
      "epoch =  1517 ; loss =  0.111993\n",
      "epoch =  1518 ; loss =  0.112231\n",
      "epoch =  1519 ; loss =  0.111656\n",
      "epoch =  1520 ; loss =  0.111505\n",
      "epoch =  1521 ; loss =  0.111402\n",
      "epoch =  1522 ; loss =  0.111378\n",
      "epoch =  1523 ; loss =  0.111107\n",
      "epoch =  1524 ; loss =  0.111397\n",
      "epoch =  1525 ; loss =  0.110935\n",
      "epoch =  1526 ; loss =  0.111438\n",
      "epoch =  1527 ; loss =  0.111556\n",
      "epoch =  1528 ; loss =  0.111221\n",
      "epoch =  1529 ; loss =  0.111204\n",
      "epoch =  1530 ; loss =  0.111599\n",
      "epoch =  1531 ; loss =  0.111407\n",
      "epoch =  1532 ; loss =  0.111246\n",
      "epoch =  1533 ; loss =  0.111242\n",
      "epoch =  1534 ; loss =  0.111442\n",
      "epoch =  1535 ; loss =  0.111045\n",
      "epoch =  1536 ; loss =  0.110631\n",
      "epoch =  1537 ; loss =  0.11118\n",
      "epoch =  1538 ; loss =  0.111063\n",
      "epoch =  1539 ; loss =  0.11139\n",
      "epoch =  1540 ; loss =  0.111074\n",
      "epoch =  1541 ; loss =  0.110828\n",
      "epoch =  1542 ; loss =  0.110824\n",
      "epoch =  1543 ; loss =  0.110646\n",
      "epoch =  1544 ; loss =  0.111443\n",
      "epoch =  1545 ; loss =  0.11088\n",
      "epoch =  1546 ; loss =  0.111113\n",
      "epoch =  1547 ; loss =  0.110787\n",
      "epoch =  1548 ; loss =  0.11082\n",
      "epoch =  1549 ; loss =  0.110959\n",
      "epoch =  1550 ; loss =  0.111319\n",
      "epoch =  1551 ; loss =  0.110896\n",
      "epoch =  1552 ; loss =  0.110687\n",
      "epoch =  1553 ; loss =  0.111019\n",
      "epoch =  1554 ; loss =  0.110943\n",
      "epoch =  1555 ; loss =  0.111202\n",
      "epoch =  1556 ; loss =  0.110854\n",
      "epoch =  1557 ; loss =  0.110494\n",
      "epoch =  1558 ; loss =  0.110882\n",
      "epoch =  1559 ; loss =  0.111114\n",
      "epoch =  1560 ; loss =  0.111092\n",
      "epoch =  1561 ; loss =  0.110594\n",
      "epoch =  1562 ; loss =  0.11066\n",
      "epoch =  1563 ; loss =  0.110368\n",
      "epoch =  1564 ; loss =  0.110254\n",
      "epoch =  1565 ; loss =  0.110879\n",
      "epoch =  1566 ; loss =  0.110576\n",
      "epoch =  1567 ; loss =  0.110292\n",
      "epoch =  1568 ; loss =  0.110515\n",
      "epoch =  1569 ; loss =  0.1107\n",
      "epoch =  1570 ; loss =  0.110496\n",
      "epoch =  1571 ; loss =  0.110737\n",
      "epoch =  1572 ; loss =  0.110893\n",
      "epoch =  1573 ; loss =  0.110679\n",
      "epoch =  1574 ; loss =  0.110361\n",
      "epoch =  1575 ; loss =  0.110696\n",
      "epoch =  1576 ; loss =  0.111013\n",
      "epoch =  1577 ; loss =  0.111079\n",
      "epoch =  1578 ; loss =  0.110346\n",
      "epoch =  1579 ; loss =  0.110692\n",
      "epoch =  1580 ; loss =  0.110702\n",
      "epoch =  1581 ; loss =  0.110698\n",
      "epoch =  1582 ; loss =  0.110816\n",
      "epoch =  1583 ; loss =  0.110876\n",
      "epoch =  1584 ; loss =  0.110391\n",
      "epoch =  1585 ; loss =  0.110605\n",
      "epoch =  1586 ; loss =  0.11101\n",
      "epoch =  1587 ; loss =  0.110606\n",
      "epoch =  1588 ; loss =  0.111333\n",
      "epoch =  1589 ; loss =  0.110968\n",
      "epoch =  1590 ; loss =  0.11075\n",
      "epoch =  1591 ; loss =  0.111051\n",
      "epoch =  1592 ; loss =  0.110653\n",
      "epoch =  1593 ; loss =  0.110558\n",
      "epoch =  1594 ; loss =  0.111749\n",
      "epoch =  1595 ; loss =  0.110558\n",
      "epoch =  1596 ; loss =  0.110671\n",
      "epoch =  1597 ; loss =  0.110921\n",
      "epoch =  1598 ; loss =  0.110507\n",
      "epoch =  1599 ; loss =  0.110567\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1600 ; loss =  0.112354\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1601 ; loss =  0.11236\n",
      "epoch =  1602 ; loss =  0.111662\n",
      "epoch =  1603 ; loss =  0.110429\n",
      "epoch =  1604 ; loss =  0.110733\n",
      "epoch =  1605 ; loss =  0.110021\n",
      "epoch =  1606 ; loss =  0.109582\n",
      "epoch =  1607 ; loss =  0.109759\n",
      "epoch =  1608 ; loss =  0.109159\n",
      "epoch =  1609 ; loss =  0.109084\n",
      "epoch =  1610 ; loss =  0.108675\n",
      "epoch =  1611 ; loss =  0.10944\n",
      "epoch =  1612 ; loss =  0.109219\n",
      "epoch =  1613 ; loss =  0.108262\n",
      "epoch =  1614 ; loss =  0.109078\n",
      "epoch =  1615 ; loss =  0.108498\n",
      "epoch =  1616 ; loss =  0.108449\n",
      "epoch =  1617 ; loss =  0.108144\n",
      "epoch =  1618 ; loss =  0.108098\n",
      "epoch =  1619 ; loss =  0.107912\n",
      "epoch =  1620 ; loss =  0.107563\n",
      "epoch =  1621 ; loss =  0.107592\n",
      "epoch =  1622 ; loss =  0.107837\n",
      "epoch =  1623 ; loss =  0.108583\n",
      "epoch =  1624 ; loss =  0.108259\n",
      "epoch =  1625 ; loss =  0.107582\n",
      "epoch =  1626 ; loss =  0.107657\n",
      "epoch =  1627 ; loss =  0.107821\n",
      "epoch =  1628 ; loss =  0.107261\n",
      "epoch =  1629 ; loss =  0.107453\n",
      "epoch =  1630 ; loss =  0.107516\n",
      "epoch =  1631 ; loss =  0.106902\n",
      "epoch =  1632 ; loss =  0.107029\n",
      "epoch =  1633 ; loss =  0.107586\n",
      "epoch =  1634 ; loss =  0.107233\n",
      "epoch =  1635 ; loss =  0.107528\n",
      "epoch =  1636 ; loss =  0.107038\n",
      "epoch =  1637 ; loss =  0.107101\n",
      "epoch =  1638 ; loss =  0.107057\n",
      "epoch =  1639 ; loss =  0.106916\n",
      "epoch =  1640 ; loss =  0.107254\n",
      "epoch =  1641 ; loss =  0.107029\n",
      "epoch =  1642 ; loss =  0.107432\n",
      "epoch =  1643 ; loss =  0.10686\n",
      "epoch =  1644 ; loss =  0.107333\n",
      "epoch =  1645 ; loss =  0.106733\n",
      "epoch =  1646 ; loss =  0.106826\n",
      "epoch =  1647 ; loss =  0.106836\n",
      "epoch =  1648 ; loss =  0.106887\n",
      "epoch =  1649 ; loss =  0.106457\n",
      "epoch =  1650 ; loss =  0.10695\n",
      "epoch =  1651 ; loss =  0.106898\n",
      "epoch =  1652 ; loss =  0.106806\n",
      "epoch =  1653 ; loss =  0.107025\n",
      "epoch =  1654 ; loss =  0.106418\n",
      "epoch =  1655 ; loss =  0.10652\n",
      "epoch =  1656 ; loss =  0.106408\n",
      "epoch =  1657 ; loss =  0.106311\n",
      "epoch =  1658 ; loss =  0.106708\n",
      "epoch =  1659 ; loss =  0.106364\n",
      "epoch =  1660 ; loss =  0.106252\n",
      "epoch =  1661 ; loss =  0.10666\n",
      "epoch =  1662 ; loss =  0.106675\n",
      "epoch =  1663 ; loss =  0.106778\n",
      "epoch =  1664 ; loss =  0.106387\n",
      "epoch =  1665 ; loss =  0.106321\n",
      "epoch =  1666 ; loss =  0.10656\n",
      "epoch =  1667 ; loss =  0.106335\n",
      "epoch =  1668 ; loss =  0.1067\n",
      "epoch =  1669 ; loss =  0.107019\n",
      "epoch =  1670 ; loss =  0.10682\n",
      "epoch =  1671 ; loss =  0.107036\n",
      "epoch =  1672 ; loss =  0.106317\n",
      "epoch =  1673 ; loss =  0.106655\n",
      "epoch =  1674 ; loss =  0.105935\n",
      "epoch =  1675 ; loss =  0.106345\n",
      "epoch =  1676 ; loss =  0.106436\n",
      "epoch =  1677 ; loss =  0.10658\n",
      "epoch =  1678 ; loss =  0.106741\n",
      "epoch =  1679 ; loss =  0.106176\n",
      "epoch =  1680 ; loss =  0.106221\n",
      "epoch =  1681 ; loss =  0.105815\n",
      "epoch =  1682 ; loss =  0.106404\n",
      "epoch =  1683 ; loss =  0.107285\n",
      "epoch =  1684 ; loss =  0.106493\n",
      "epoch =  1685 ; loss =  0.107418\n",
      "epoch =  1686 ; loss =  0.106685\n",
      "epoch =  1687 ; loss =  0.106115\n",
      "epoch =  1688 ; loss =  0.106434\n",
      "epoch =  1689 ; loss =  0.1063\n",
      "epoch =  1690 ; loss =  0.106502\n",
      "epoch =  1691 ; loss =  0.106328\n",
      "epoch =  1692 ; loss =  0.106468\n",
      "epoch =  1693 ; loss =  0.106351\n",
      "epoch =  1694 ; loss =  0.10634\n",
      "epoch =  1695 ; loss =  0.106038\n",
      "epoch =  1696 ; loss =  0.106127\n",
      "epoch =  1697 ; loss =  0.106232\n",
      "epoch =  1698 ; loss =  0.10637\n",
      "epoch =  1699 ; loss =  0.105805\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1700 ; loss =  0.111996\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1701 ; loss =  0.111204\n",
      "epoch =  1702 ; loss =  0.110333\n",
      "epoch =  1703 ; loss =  0.10995\n",
      "epoch =  1704 ; loss =  0.108832\n",
      "epoch =  1705 ; loss =  0.108372\n",
      "epoch =  1706 ; loss =  0.107748\n",
      "epoch =  1707 ; loss =  0.107335\n",
      "epoch =  1708 ; loss =  0.107604\n",
      "epoch =  1709 ; loss =  0.10766\n",
      "epoch =  1710 ; loss =  0.106577\n",
      "epoch =  1711 ; loss =  0.106444\n",
      "epoch =  1712 ; loss =  0.106348\n",
      "epoch =  1713 ; loss =  0.106404\n",
      "epoch =  1714 ; loss =  0.10583\n",
      "epoch =  1715 ; loss =  0.105666\n",
      "epoch =  1716 ; loss =  0.106069\n",
      "epoch =  1717 ; loss =  0.105769\n",
      "epoch =  1718 ; loss =  0.105289\n",
      "epoch =  1719 ; loss =  0.105584\n",
      "epoch =  1720 ; loss =  0.1056\n",
      "epoch =  1721 ; loss =  0.10509\n",
      "epoch =  1722 ; loss =  0.106334\n",
      "epoch =  1723 ; loss =  0.105439\n",
      "epoch =  1724 ; loss =  0.105069\n",
      "epoch =  1725 ; loss =  0.105118\n",
      "epoch =  1726 ; loss =  0.105141\n",
      "epoch =  1727 ; loss =  0.104865\n",
      "epoch =  1728 ; loss =  0.104326\n",
      "epoch =  1729 ; loss =  0.104652\n",
      "epoch =  1730 ; loss =  0.104431\n",
      "epoch =  1731 ; loss =  0.104218\n",
      "epoch =  1732 ; loss =  0.104502\n",
      "epoch =  1733 ; loss =  0.104652\n",
      "epoch =  1734 ; loss =  0.104056\n",
      "epoch =  1735 ; loss =  0.10396\n",
      "epoch =  1736 ; loss =  0.103882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1737 ; loss =  0.104238\n",
      "epoch =  1738 ; loss =  0.10406\n",
      "epoch =  1739 ; loss =  0.104116\n",
      "epoch =  1740 ; loss =  0.10398\n",
      "epoch =  1741 ; loss =  0.104004\n",
      "epoch =  1742 ; loss =  0.103906\n",
      "epoch =  1743 ; loss =  0.103956\n",
      "epoch =  1744 ; loss =  0.103868\n",
      "epoch =  1745 ; loss =  0.10411\n",
      "epoch =  1746 ; loss =  0.103954\n",
      "epoch =  1747 ; loss =  0.103957\n",
      "epoch =  1748 ; loss =  0.103829\n",
      "epoch =  1749 ; loss =  0.104418\n",
      "epoch =  1750 ; loss =  0.103588\n",
      "epoch =  1751 ; loss =  0.103512\n",
      "epoch =  1752 ; loss =  0.103615\n",
      "epoch =  1753 ; loss =  0.103391\n",
      "epoch =  1754 ; loss =  0.103975\n",
      "epoch =  1755 ; loss =  0.103921\n",
      "epoch =  1756 ; loss =  0.103644\n",
      "epoch =  1757 ; loss =  0.103433\n",
      "epoch =  1758 ; loss =  0.103284\n",
      "epoch =  1759 ; loss =  0.103259\n",
      "epoch =  1760 ; loss =  0.103569\n",
      "epoch =  1761 ; loss =  0.103274\n",
      "epoch =  1762 ; loss =  0.103768\n",
      "epoch =  1763 ; loss =  0.103722\n",
      "epoch =  1764 ; loss =  0.103381\n",
      "epoch =  1765 ; loss =  0.103284\n",
      "epoch =  1766 ; loss =  0.103001\n",
      "epoch =  1767 ; loss =  0.103193\n",
      "epoch =  1768 ; loss =  0.103293\n",
      "epoch =  1769 ; loss =  0.103501\n",
      "epoch =  1770 ; loss =  0.103206\n",
      "epoch =  1771 ; loss =  0.103583\n",
      "epoch =  1772 ; loss =  0.103117\n",
      "epoch =  1773 ; loss =  0.103701\n",
      "epoch =  1774 ; loss =  0.103682\n",
      "epoch =  1775 ; loss =  0.103417\n",
      "epoch =  1776 ; loss =  0.103437\n",
      "epoch =  1777 ; loss =  0.102982\n",
      "epoch =  1778 ; loss =  0.10424\n",
      "epoch =  1779 ; loss =  0.103554\n",
      "epoch =  1780 ; loss =  0.103357\n",
      "epoch =  1781 ; loss =  0.102967\n",
      "epoch =  1782 ; loss =  0.103645\n",
      "epoch =  1783 ; loss =  0.1035\n",
      "epoch =  1784 ; loss =  0.103238\n",
      "epoch =  1785 ; loss =  0.103286\n",
      "epoch =  1786 ; loss =  0.103356\n",
      "epoch =  1787 ; loss =  0.105185\n",
      "epoch =  1788 ; loss =  0.104617\n",
      "epoch =  1789 ; loss =  0.103258\n",
      "epoch =  1790 ; loss =  0.103258\n",
      "epoch =  1791 ; loss =  0.103069\n",
      "epoch =  1792 ; loss =  0.102861\n",
      "epoch =  1793 ; loss =  0.102923\n",
      "epoch =  1794 ; loss =  0.104724\n",
      "epoch =  1795 ; loss =  0.103207\n",
      "epoch =  1796 ; loss =  0.103175\n",
      "epoch =  1797 ; loss =  0.103699\n",
      "epoch =  1798 ; loss =  0.102809\n",
      "epoch =  1799 ; loss =  0.102893\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1800 ; loss =  0.107039\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1801 ; loss =  0.106814\n",
      "epoch =  1802 ; loss =  0.106751\n",
      "epoch =  1803 ; loss =  0.106507\n",
      "epoch =  1804 ; loss =  0.105979\n",
      "epoch =  1805 ; loss =  0.105751\n",
      "epoch =  1806 ; loss =  0.105833\n",
      "epoch =  1807 ; loss =  0.106115\n",
      "epoch =  1808 ; loss =  0.105914\n",
      "epoch =  1809 ; loss =  0.105517\n",
      "epoch =  1810 ; loss =  0.105585\n",
      "epoch =  1811 ; loss =  0.105331\n",
      "epoch =  1812 ; loss =  0.105155\n",
      "epoch =  1813 ; loss =  0.105319\n",
      "epoch =  1814 ; loss =  0.10551\n",
      "epoch =  1815 ; loss =  0.105168\n",
      "epoch =  1816 ; loss =  0.105179\n",
      "epoch =  1817 ; loss =  0.10504\n",
      "epoch =  1818 ; loss =  0.105025\n",
      "epoch =  1819 ; loss =  0.104749\n",
      "epoch =  1820 ; loss =  0.105108\n",
      "epoch =  1821 ; loss =  0.104594\n",
      "epoch =  1822 ; loss =  0.104941\n",
      "epoch =  1823 ; loss =  0.104805\n",
      "epoch =  1824 ; loss =  0.104839\n",
      "epoch =  1825 ; loss =  0.106953\n",
      "epoch =  1826 ; loss =  0.105119\n",
      "epoch =  1827 ; loss =  0.105141\n",
      "epoch =  1828 ; loss =  0.10475\n",
      "epoch =  1829 ; loss =  0.104911\n",
      "epoch =  1830 ; loss =  0.104684\n",
      "epoch =  1831 ; loss =  0.104646\n",
      "epoch =  1832 ; loss =  0.105147\n",
      "epoch =  1833 ; loss =  0.104449\n",
      "epoch =  1834 ; loss =  0.104294\n",
      "epoch =  1835 ; loss =  0.104656\n",
      "epoch =  1836 ; loss =  0.104698\n",
      "epoch =  1837 ; loss =  0.104577\n",
      "epoch =  1838 ; loss =  0.109026\n",
      "epoch =  1839 ; loss =  0.105543\n",
      "epoch =  1840 ; loss =  0.105938\n",
      "epoch =  1841 ; loss =  0.105226\n",
      "epoch =  1842 ; loss =  0.104846\n",
      "epoch =  1843 ; loss =  0.10542\n",
      "epoch =  1844 ; loss =  0.105053\n",
      "epoch =  1845 ; loss =  0.104949\n",
      "epoch =  1846 ; loss =  0.105099\n",
      "epoch =  1847 ; loss =  0.104508\n",
      "epoch =  1848 ; loss =  0.104844\n",
      "epoch =  1849 ; loss =  0.104612\n",
      "epoch =  1850 ; loss =  0.104599\n",
      "epoch =  1851 ; loss =  0.104432\n",
      "epoch =  1852 ; loss =  0.104504\n",
      "epoch =  1853 ; loss =  0.104683\n",
      "epoch =  1854 ; loss =  0.104485\n",
      "epoch =  1855 ; loss =  0.104641\n",
      "epoch =  1856 ; loss =  0.109099\n",
      "epoch =  1857 ; loss =  0.105871\n",
      "epoch =  1858 ; loss =  0.105528\n",
      "epoch =  1859 ; loss =  0.105392\n",
      "epoch =  1860 ; loss =  0.105277\n",
      "epoch =  1861 ; loss =  0.105174\n",
      "epoch =  1862 ; loss =  0.10489\n",
      "epoch =  1863 ; loss =  0.104979\n",
      "epoch =  1864 ; loss =  0.104687\n",
      "epoch =  1865 ; loss =  0.105086\n",
      "epoch =  1866 ; loss =  0.104849\n",
      "epoch =  1867 ; loss =  0.104959\n",
      "epoch =  1868 ; loss =  0.104794\n",
      "epoch =  1869 ; loss =  0.104777\n",
      "epoch =  1870 ; loss =  0.104909\n",
      "epoch =  1871 ; loss =  0.104844\n",
      "epoch =  1872 ; loss =  0.104682\n",
      "epoch =  1873 ; loss =  0.104793\n",
      "epoch =  1874 ; loss =  0.104731\n",
      "epoch =  1875 ; loss =  0.10491\n",
      "epoch =  1876 ; loss =  0.105137\n",
      "epoch =  1877 ; loss =  0.104617\n",
      "epoch =  1878 ; loss =  0.104697\n",
      "epoch =  1879 ; loss =  0.104947\n",
      "epoch =  1880 ; loss =  0.104671\n",
      "epoch =  1881 ; loss =  0.104506\n",
      "epoch =  1882 ; loss =  0.104572\n",
      "epoch =  1883 ; loss =  0.104792\n",
      "epoch =  1884 ; loss =  0.104703\n",
      "epoch =  1885 ; loss =  0.104405\n",
      "epoch =  1886 ; loss =  0.104325\n",
      "epoch =  1887 ; loss =  0.104712\n",
      "epoch =  1888 ; loss =  0.10478\n",
      "epoch =  1889 ; loss =  0.104389\n",
      "epoch =  1890 ; loss =  0.104627\n",
      "epoch =  1891 ; loss =  0.104474\n",
      "epoch =  1892 ; loss =  0.104731\n",
      "epoch =  1893 ; loss =  0.104724\n",
      "epoch =  1894 ; loss =  0.104702\n",
      "epoch =  1895 ; loss =  0.104525\n",
      "epoch =  1896 ; loss =  0.104296\n",
      "epoch =  1897 ; loss =  0.104246\n",
      "epoch =  1898 ; loss =  0.104459\n",
      "epoch =  1899 ; loss =  0.104461\n",
      "Obtaining new batch of pieces\n",
      "epoch =  1900 ; loss =  0.107429\n",
      "Model saved in file: model/Practice_fb_artic\n",
      "epoch =  1901 ; loss =  0.107364\n",
      "epoch =  1902 ; loss =  0.107429\n",
      "epoch =  1903 ; loss =  0.106895\n",
      "epoch =  1904 ; loss =  0.107352\n",
      "epoch =  1905 ; loss =  0.106776\n",
      "epoch =  1906 ; loss =  0.107208\n",
      "epoch =  1907 ; loss =  0.106471\n",
      "epoch =  1908 ; loss =  0.106685\n",
      "epoch =  1909 ; loss =  0.106683\n",
      "epoch =  1910 ; loss =  0.10651\n",
      "epoch =  1911 ; loss =  0.10653\n",
      "epoch =  1912 ; loss =  0.106372\n",
      "epoch =  1913 ; loss =  0.106478\n",
      "epoch =  1914 ; loss =  0.106314\n",
      "epoch =  1915 ; loss =  0.106378\n",
      "epoch =  1916 ; loss =  0.10581\n",
      "epoch =  1917 ; loss =  0.106132\n",
      "epoch =  1918 ; loss =  0.106388\n",
      "epoch =  1919 ; loss =  0.106317\n",
      "epoch =  1920 ; loss =  0.105988\n",
      "epoch =  1921 ; loss =  0.105615\n",
      "epoch =  1922 ; loss =  0.105733\n",
      "epoch =  1923 ; loss =  0.105636\n",
      "epoch =  1924 ; loss =  0.106353\n",
      "epoch =  1925 ; loss =  0.105994\n",
      "epoch =  1926 ; loss =  0.106265\n",
      "epoch =  1927 ; loss =  0.105873\n",
      "epoch =  1928 ; loss =  0.105816\n",
      "epoch =  1929 ; loss =  0.106103\n",
      "epoch =  1930 ; loss =  0.105693\n",
      "epoch =  1931 ; loss =  0.10611\n",
      "epoch =  1932 ; loss =  0.105705\n",
      "epoch =  1933 ; loss =  0.105932\n",
      "epoch =  1934 ; loss =  0.105636\n",
      "epoch =  1935 ; loss =  0.10584\n",
      "epoch =  1936 ; loss =  0.106131\n",
      "epoch =  1937 ; loss =  0.10581\n",
      "epoch =  1938 ; loss =  0.105705\n",
      "epoch =  1939 ; loss =  0.105976\n",
      "epoch =  1940 ; loss =  0.105879\n",
      "epoch =  1941 ; loss =  0.106062\n",
      "epoch =  1942 ; loss =  0.105959\n",
      "epoch =  1943 ; loss =  0.106069\n",
      "epoch =  1944 ; loss =  0.105524\n",
      "epoch =  1945 ; loss =  0.105794\n",
      "epoch =  1946 ; loss =  0.105844\n",
      "epoch =  1947 ; loss =  0.105852\n",
      "epoch =  1948 ; loss =  0.105791\n",
      "epoch =  1949 ; loss =  0.105701\n",
      "epoch =  1950 ; loss =  0.106128\n",
      "epoch =  1951 ; loss =  0.105895\n",
      "epoch =  1952 ; loss =  0.105541\n",
      "epoch =  1953 ; loss =  0.105932\n",
      "epoch =  1954 ; loss =  0.105742\n",
      "epoch =  1955 ; loss =  0.105905\n",
      "epoch =  1956 ; loss =  0.105993\n",
      "epoch =  1957 ; loss =  0.105701\n",
      "epoch =  1958 ; loss =  0.105559\n",
      "epoch =  1959 ; loss =  0.105647\n",
      "epoch =  1960 ; loss =  0.106015\n",
      "epoch =  1961 ; loss =  0.106055\n",
      "epoch =  1962 ; loss =  0.105553\n",
      "epoch =  1963 ; loss =  0.105963\n",
      "epoch =  1964 ; loss =  0.105911\n",
      "epoch =  1965 ; loss =  0.105426\n",
      "epoch =  1966 ; loss =  0.10553\n",
      "epoch =  1967 ; loss =  0.105744\n",
      "epoch =  1968 ; loss =  0.10551\n",
      "epoch =  1969 ; loss =  0.105969\n",
      "epoch =  1970 ; loss =  0.106138\n",
      "epoch =  1971 ; loss =  0.105482\n",
      "epoch =  1972 ; loss =  0.105591\n",
      "epoch =  1973 ; loss =  0.105659\n",
      "epoch =  1974 ; loss =  0.105745\n",
      "epoch =  1975 ; loss =  0.105722\n",
      "epoch =  1976 ; loss =  0.105875\n",
      "epoch =  1977 ; loss =  0.105705\n",
      "epoch =  1978 ; loss =  0.105814\n",
      "epoch =  1979 ; loss =  0.105401\n",
      "epoch =  1980 ; loss =  0.10541\n",
      "epoch =  1981 ; loss =  0.105482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  1982 ; loss =  0.105712\n",
      "epoch =  1983 ; loss =  0.105881\n",
      "epoch =  1984 ; loss =  0.105827\n",
      "epoch =  1985 ; loss =  0.105462\n",
      "epoch =  1986 ; loss =  0.105636\n",
      "epoch =  1987 ; loss =  0.106039\n",
      "epoch =  1988 ; loss =  0.105469\n",
      "epoch =  1989 ; loss =  0.105761\n",
      "epoch =  1990 ; loss =  0.10572\n",
      "epoch =  1991 ; loss =  0.105656\n",
      "epoch =  1992 ; loss =  0.105911\n",
      "epoch =  1993 ; loss =  0.105676\n",
      "epoch =  1994 ; loss =  0.105473\n",
      "epoch =  1995 ; loss =  0.105358\n",
      "epoch =  1996 ; loss =  0.105527\n",
      "epoch =  1997 ; loss =  0.105504\n",
      "epoch =  1998 ; loss =  0.106082\n",
      "epoch =  1999 ; loss =  0.105676\n",
      "Obtaining new batch of pieces\n",
      "epoch =  2000 ; loss =  0.104154\n",
      "error =  [6653 1868]\n",
      "num_1 =  [3382 1719]\n",
      "Model saved in file: model/Practice_fb_artic\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "N_epochs = 2000\n",
    "loss_hist=[]\n",
    "restore_model_name = None\n",
    "save_model_name = 'Practice_fb_artic'\n",
    "batch_size = 10\n",
    "num_timesteps = 128\n",
    "keep_prob=.75\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # try to restore the pre_trained\n",
    "    if restore_model_name is not None:\n",
    "        print(\"Load the model from: {}\".format(restore_model_name))\n",
    "        saver.restore(sess, 'model/{}'.format(restore_model_name))\n",
    "    \n",
    "    # Initial States\n",
    "    timewise_state_val=[]\n",
    "    for i in range(len(num_t_units)):\n",
    "        c_t = np.zeros((batch_size*num_notes, num_t_units[i])) #start every batch with zero state in LSTM time cells\n",
    "        h_t = np.zeros((batch_size*num_notes, num_t_units[i]))\n",
    "        timewise_state_val.append(LSTMStateTuple(h_t, c_t))\n",
    "        \n",
    "    notewise_state_val=[]\n",
    "    for i in range(len(num_n_units)):\n",
    "        c_n = np.zeros((batch_size*num_timesteps, num_n_units[i])) #start every batch with zero state in LSTM time cells\n",
    "        h_n = np.zeros((batch_size*num_timesteps, num_n_units[i]))\n",
    "        notewise_state_val.append(LSTMStateTuple(h_n, c_n))\n",
    "    \n",
    "  \n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(N_epochs+1):\n",
    "        \n",
    "        #Generate random batch of training data\n",
    "        \n",
    "        if (epoch % 100 == 0):         \n",
    "            print('Obtaining new batch of pieces')\n",
    "            _, batch_input_state = multi_training.getPieceBatch(training_pieces, batch_size, num_timesteps) # not using their 'convolution' filter\n",
    "            batch_input_state = np.array(batch_input_state)\n",
    "            batch_input_state = np.swapaxes(batch_input_state, axis1=1, axis2=2)           \n",
    "            #print('sum = ',sum(sum(sum(sum(batch_input_state)))) )\n",
    "        \n",
    "        \"\"\"\n",
    "        print('Note_State_Batch shape = ', Note_State_Batch.get_shape())\n",
    "        print('batch_input_state shape = ', batch_input_state.shape)\n",
    "        print('')\n",
    "        print('timewise_state shape = ', Note_State_Batch.get_shape())\n",
    "        print('timewise_state_val shape = ', batch_input_state.shape)      \n",
    "        print('')\n",
    "        print('notewise_state shape = ', Note_State_Batch.get_shape())\n",
    "        print('notewise_state_val shape = ', batch_input_state.shape)\n",
    "        ('')\n",
    "        print('time_init shape = ', time_init.get_shape())\n",
    "        \"\"\"\n",
    "    \n",
    "        feed_dict = {Note_State_Batch: batch_input_state, timewise_state: timewise_state_val, notewise_state: notewise_state_val, time_init: 0, output_keep_prob: keep_prob}\n",
    "        #try:\n",
    "        loss_run, _, note_gen_out_run = sess.run([loss, optimizer, note_gen_out], feed_dict=feed_dict)\n",
    "        #except:\n",
    "        #   save_path = saver.save(sess, 'model/{}'.format(save_model_name))\n",
    "        #    print(\"Model saved in file: %s\" % save_path)\n",
    "        \n",
    "        print('epoch = ', epoch, '; loss = ', loss_run)\n",
    "        loss_hist.append(loss_run)\n",
    "        \n",
    "        if (epoch % 500 ==0):\n",
    "            error = sum(sum(sum(abs(batch_input_state[:,:,1:,:] - note_gen_out_run[:,:,:-1,:]))))\n",
    "            num_1 = sum(sum(sum(abs(batch_input_state[:,:,1:,:]))))\n",
    "            print('error = ', error)\n",
    "            print('num_1 = ', num_1)\n",
    "        if (epoch % 100 == 0) & (epoch > 0):\n",
    "            save_path = saver.save(sess, 'model/{}'.format(save_model_name))\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "            \n",
    "    #save_path = saver.save(sess, 'model/{}'.format(save_model_name))\n",
    "    #print(\" Final Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error =  [3520 1723]\n",
      "num_1 =  [3824 1511]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#print(square_error/mag)\n",
    "#print('Actual input = ', batch_input_state[:,:,1,:])\n",
    "#print('Output predicting this input = ', note_gen_out_run[:,:,0,:])\n",
    "error = sum(sum(sum(abs(batch_input_state[:,:,1:,:] - note_gen_out_run[:,:,:-1,:]))))\n",
    "num_1 = sum(sum(sum(abs(batch_input_state[:,:,1:,:]))))\n",
    "print('error = ', error)\n",
    "print('num_1 = ', num_1)\n",
    "#for v in range(len(tf.trainable_variables())):\n",
    "#    print(tf.trainable_variables()[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1000 % 100 == 0) & (1000 > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2001"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUoQU7iIiRej/A01GyEVaTcavAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f230a261be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist, label=\"Loss History\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "len(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: Practice_fb_artic\n",
      "INFO:tensorflow:Restoring parameters from model/Practice_fb_artic\n",
      "Timestep =  0\n",
      "Timestep =  50\n",
      "Timestep =  100\n",
      "Timestep =  150\n",
      "Timestep =  200\n",
      "Timestep =  250\n",
      "(5, 78, 256, 2)\n"
     ]
    }
   ],
   "source": [
    "# Music Generation\n",
    "# input = initial note vector\n",
    "# for t = 1:Tsong\n",
    "#    input --> input kernel\n",
    "#    run through 1 'call' of Model LSTM with present parameters / states\n",
    "#    run through note-wise LSTM block as normally done to produce vector of generated samples\n",
    "#    input = generated samples\n",
    "#    music_sequence.append(input)\n",
    "\n",
    "# store batch of music sequences in .MIDI files\n",
    "\n",
    "\n",
    "#Load Model\n",
    "restore_model_name = 'Practice_fb_artic'\n",
    "\n",
    "#Length of generated music\n",
    "T_gen = 16*16\n",
    "batch_gen_size = 5\n",
    "keep_prob = 1\n",
    "\n",
    "# start with initial Note_State_Batch with 't' dimension = 1 (can still a batch of samples run in parallel)\n",
    "notes_gen_initial = np.zeros((batch_gen_size, num_notes, 1,2))\n",
    "\n",
    "# Initial States\n",
    "notes_gen = notes_gen_initial\n",
    "    \n",
    "timewise_state_val=[]\n",
    "for i in range(len(num_t_units)):\n",
    "    c = np.zeros((batch_gen_size*num_notes, num_t_units[i])) #start first time step with zero state in LSTM time cells\n",
    "    h = np.zeros((batch_gen_size*num_notes, num_t_units[i]))\n",
    "    timewise_state_val.append(LSTMStateTuple(h, c))\n",
    "        \n",
    "notewise_state_val=[]\n",
    "for i in range(len(num_n_units)):\n",
    "    c = np.zeros((batch_gen_size*1, num_n_units[i])) #start every batch with zero state in LSTM time cells\n",
    "    h = np.zeros((batch_gen_size*1, num_n_units[i]))\n",
    "    notewise_state_val.append(LSTMStateTuple(h, c))\n",
    "        \n",
    "notes_gen_arr=[]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print(\"Load the model from: {}\".format(restore_model_name))\n",
    "    saver.restore(sess, 'model/{}'.format(restore_model_name))\n",
    "    \n",
    "\n",
    "    for t in range(T_gen):\n",
    "        feed_dict = {Note_State_Batch: notes_gen, timewise_state: timewise_state_val, notewise_state: notewise_state_val, time_init: t % 16, output_keep_prob: keep_prob}    \n",
    "        timewise_state_val, notes_gen = np.squeeze(sess.run([timewise_state_out, note_gen_out], feed_dict = feed_dict), axis=2)\n",
    "        #print('notes_gen shape = ', notes_gen.shape)\n",
    "        #notes_gen = np.squeeze(notes_gen, axis=2)\n",
    "        notes_gen_arr.append(np.squeeze(notes_gen))\n",
    "        \n",
    "        \n",
    "        \n",
    "        if t % 50 == 0:\n",
    "            print('Timestep = ', t)\n",
    "    \n",
    "notes_gen_out = np.stack(notes_gen_arr, axis=2)\n",
    "print(notes_gen_out.shape)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 256, 78, 2)\n"
     ]
    }
   ],
   "source": [
    "# Save Generate Notes to .MIDI file\n",
    "\n",
    "notes_gen_out = np.swapaxes(notes_gen_out, axis1=1, axis2=2)\n",
    "print(notes_gen_out.shape)\n",
    "#_, notes_gen_out = Utils.multi_training.getPieceBatch(training_pieces)\n",
    "\n",
    "\n",
    "#print(test_batch.shape)\n",
    "for iter in range(4):\n",
    "    file = 'Generated_Midi_Files/Practice_fb_artic' + str(iter)\n",
    "    midi_out = midi_musical_matrix.noteStateMatrixToMidi(notes_gen_out[iter,:,:,:], name=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items to Experiment with:\n",
    "- different T length or variable length T from batch-to-batch for training\n",
    "- categorize music, either through (unsupervised) clustering or (supervised) labeled music folders.  For clustering, the model would possibly find 'k' 'centroids' in an unsupervised manner each with its own music distribution, so during the music generation stage, 1 of these centroids would be selected for a piece of music.  \n",
    "- use encoder to reduce dimensionality of each note vector (vector of 79 notes in 1 time step), similiar to encoding the words from the tweets in homework 3 (i.e. there are restricted combinations of notes that can be played simultaneously)\n",
    "- more advanced sampling/exploring for training/music generation.  This may help prevent the algorithm from getting 'stuck' on a chord, or "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
