{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import math_ops\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import Utils.midi_musical_matrix\n",
    "import Utils.data\n",
    "import Utils.multi_training\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "from tensorflow.contrib.rnn import LSTMStateTuple\n",
    "from MyFunctions import Input_Kernel, LSTM_TimeWise_Training_Layer, LSTM_NoteWise_Layer, Loss_Function\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 01Allemande\n",
      "Loaded 01Prelude\n",
      "Loaded 02Ichdankdir\n",
      "Loaded 03AchGott\n",
      "Loaded 04Allemande\n",
      "Loaded 04Bourree\n",
      "Loaded 04EsistdasHeiluns\n",
      "Loaded 04Prelude\n",
      "Loaded 05AnWasserflussen\n",
      "Loaded 06Christus\n",
      "Loaded 08Freueteuch\n",
      "Loaded 09Ermuntredich\n",
      "Loaded 10AustieferNot\n",
      "Loaded 11Jesu\n",
      "Loaded 13Alleinzudir\n",
      "Loaded 14OHerreGott\n",
      "Loaded 15ChristlaginTode\n",
      "Loaded BRAND1\n",
      "Loaded BRAND3\n",
      "Loaded BRAND43\n",
      "Loaded BRAND51\n",
      "Loaded BRAND52\n",
      "Loaded BRAND53\n",
      "Loaded BSGJG_A\n",
      "Loaded BSGJG_B\n",
      "Loaded BSGJG_C\n",
      "Loaded BSGJG_D\n",
      "Loaded BSGJG_E\n",
      "Loaded BSGJG_F\n",
      "Loaded BSGJG_G\n",
      "Loaded BSGJG_H\n",
      "Loaded BSGJG_I\n",
      "Loaded BSGJG_J\n",
      "Loaded BSGJG_K\n",
      "Loaded BSGJG_L\n",
      "Loaded can4\n",
      "Loaded cap2\n",
      "Loaded catech1\n",
      "Loaded catech10\n",
      "Loaded catech11\n",
      "Loaded catech2\n",
      "Loaded catech3\n",
      "Loaded catech4\n",
      "Loaded catech5\n",
      "Loaded catech6\n",
      "Loaded catech7\n",
      "Loaded catech8\n",
      "Loaded catech9\n",
      "Loaded catechor\n",
      "Loaded cnt1 (1)\n",
      "Loaded cnt1\n",
      "Loaded cnt2\n",
      "Loaded cnt3\n",
      "Loaded dou1\n",
      "Loaded dou2\n",
      "Loaded Fugue1 (1)\n",
      "Loaded Fugue1\n",
      "Skip bad file =  Fugue11\n",
      "Loaded Fugue12 (1)\n",
      "Skip bad file =  Fugue12\n",
      "Skip bad file =  Fugue13\n",
      "Skip bad file =  Fugue15\n",
      "Loaded Fugue16\n",
      "Loaded Fugue17\n",
      "Loaded Fugue18\n",
      "Skip bad file =  Fugue19\n",
      "Loaded Fugue2\n",
      "Loaded Fugue20\n",
      "Loaded Fugue22\n",
      "Loaded Fugue23\n",
      "Loaded Fugue24\n",
      "Loaded Fugue3 (1)\n",
      "Loaded Fugue3\n",
      "Loaded Fugue4\n",
      "Loaded Fugue5 (1)\n",
      "Loaded Fugue5\n",
      "Skip bad file =  Fugue6\n",
      "Loaded Fugue7 (1)\n",
      "Loaded Fugue7\n",
      "Loaded Fugue8 (1)\n",
      "Loaded Fugue8\n",
      "Loaded Fugue9 (1)\n",
      "Loaded Fugue9\n",
      "Loaded fuguecm\n",
      "Loaded fuguegm\n",
      "Loaded gig1\n",
      "Loaded invent1\n",
      "Loaded invent11\n",
      "Loaded invent13\n",
      "Loaded invent14\n",
      "Loaded invent15\n",
      "Loaded invent2\n",
      "Loaded invent5\n",
      "Loaded invent7\n",
      "Loaded inver1\n",
      "Loaded inver2\n",
      "Loaded mir2\n",
      "Loaded orgel19\n",
      "Loaded pre1\n",
      "Loaded prefug1\n",
      "Loaded prefug2\n",
      "Loaded prefug3\n",
      "Loaded prefug5\n",
      "Loaded prefug7\n",
      "Loaded prefug8\n",
      "Loaded Prelude1 (1)\n",
      "Loaded Prelude1\n",
      "Loaded Prelude10\n",
      "Loaded Prelude12 (1)\n",
      "Loaded Prelude12\n",
      "Skip bad file =  Prelude13\n",
      "Skip bad file =  Prelude14\n",
      "Skip bad file =  Prelude15\n",
      "Loaded Prelude16\n",
      "Skip bad file =  Prelude19\n",
      "Loaded Prelude2 (1)\n",
      "Loaded Prelude2\n",
      "Skip bad file =  Prelude20\n",
      "Loaded Prelude21\n",
      "Loaded Prelude22\n",
      "Loaded Prelude23\n",
      "Skip bad file =  Prelude24\n",
      "Loaded Prelude3 (1)\n",
      "Loaded Prelude5\n",
      "Loaded Prelude6\n",
      "Loaded Prelude7\n",
      "Loaded Prelude8 (1)\n",
      "Loaded reg1\n",
      "Loaded reg2\n",
      "Loaded schub5\n",
      "Loaded schub6\n",
      "Loaded sin2\n",
      "Loaded sinfon1 (1)\n",
      "Loaded sinfon1\n",
      "Loaded sinfon12\n",
      "Loaded sinfon14\n",
      "Loaded sinfon3\n",
      "Loaded sinfon4\n",
      "Loaded sinfon8\n",
      "Loaded sinfon9\n",
      "Loaded toccata1\n",
      "Loaded toccata2\n",
      "Loaded tri1\n",
      "Loaded tri2\n",
      "Loaded trio3a\n",
      "Loaded unfin\n",
      "\n",
      "Number of training pieces =  134\n",
      "Sample of Expanded Input Batch: shape =  (10, 78, 128, 80)\n",
      "Sample of State Input Batch: shape =  (10, 78, 128, 2)\n"
     ]
    }
   ],
   "source": [
    "# Import All Training Data\n",
    "# Convert Entire Music .MIDI set to list of 'pieces'\n",
    "# During training runs, getPieceBatch will return a tensor for Note_State_Batch, and corresponding Note_State_Expand\n",
    "# Note_State_Expand will be fed into the graph input, and Note_State_Batch will be used for the loss function.\n",
    "\n",
    "\n",
    "Training_Midi_Folder = \"C:/Users/Paul/Neural_Networks/Project/Generating_Music/Midi_Files/Bach\"\n",
    "\n",
    "training_pieces = Utils.multi_training.loadPieces(Training_Midi_Folder)\n",
    "\n",
    "print('')\n",
    "print('Number of training pieces = ', len(training_pieces))\n",
    "sample_expand, sample_state = Utils.multi_training.getPieceBatch(training_pieces)\n",
    "sample_expand, sample_state = np.array(x_expand), np.array(x_state)\n",
    "\n",
    "sample_expand = np.swapaxes(sample_expand, axis1=1, axis2=2)\n",
    "sample_state = np.swapaxes(sample_state, axis1=1, axis2=2)\n",
    "\n",
    "print('Sample of Expanded Input Batch: shape = ', sample_expand.shape)\n",
    "print('Sample of State Input Batch: shape = ', sample_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note_State_Expand Placeholder Shape =  (?, 78, ?, 2)\n",
      "Note_State_Expand Placeholder Shape =  (?, 78, ?, 80)\n"
     ]
    }
   ],
   "source": [
    "# Beginning of Model Graph:\n",
    "tf.reset_default_graph()\n",
    "\n",
    "input_size = sample_expand.shape[-1]\n",
    "num_notes = sample_expand.shape[1]\n",
    "\n",
    "\n",
    "#place holder inputs\n",
    "# num_batches and num_time steps are variable lengths.  These values do not affect the model parameters\n",
    "# Dimension(0) =  num_batches. Dimension(2) = num_time_steps\n",
    "Note_State_Batch = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, 2])\n",
    "Note_State_Expand = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, input_size])\n",
    "#prev_t_sample = tf.placeholder(dtype=tf.float32, shape=[None, num_notes,1])\n",
    "\n",
    "#Generates expanded tensor input to LSTM-timewise layer\n",
    "#Note_State_Expand, final_t_sample = Input_Kernel(Note_State_Batch, prev_t_sample)\n",
    "\n",
    "print('Note_State_Expand Placeholder Shape = ', Note_State_Batch.get_shape())\n",
    "print('Note_State_Expand Placeholder Shape = ', Note_State_Expand.get_shape())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-wise output shape =  (?, 78, ?, 50)\n",
      "Time-wise state  =  Tensor(\"Reshape_5:0\", shape=(?, 78, 50), dtype=float32)\n",
      "Time-wise state  =  Tensor(\"Reshape_4:0\", shape=(?, 78, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#lSTM Time Wise Training Graph \n",
    "num_units=50\n",
    "timewise_c=tf.placeholder(dtype=tf.float32, shape=[None, num_notes, num_units])\n",
    "timewise_h=tf.placeholder(dtype=tf.float32, shape=[None, num_notes, num_units])\n",
    "timewise_state_in = LSTMStateTuple(timewise_h, timewise_c)\n",
    "\n",
    "#Note_State_Expand = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, input_size])\n",
    "timewise_out, timewise_state = LSTM_TimeWise_Training_Layer(input_data=Note_State_Expand, state_in=timewise_state_in)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Time-wise output shape = ', timewise_out.get_shape())\n",
    "print('Time-wise state  = ', timewise_state[0])\n",
    "print('Time-wise state  = ', timewise_state[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notewise shape =  (?, 78, 50)\n",
      "pa_gen_n shape =  (?, 2, 1)\n",
      "logP out shape =  (?, 78, ?, 2, 2)\n",
      "generated samples shape =  (?, 78, ?, 2)\n"
     ]
    }
   ],
   "source": [
    "#LSTM Note Wise Graph\n",
    "#tf.reset_default_graph()\n",
    "#timewise_out = tf.placeholder(dtype=tf.float32, shape=[None, num_notes, None, 50])\n",
    "logP_out, pa_gen_out = LSTM_NoteWise_Layer(timewise_out)\n",
    "\n",
    "\n",
    "print('logP out shape = ', logP_out.get_shape())\n",
    "print('generated samples shape = ', pa_gen_out.get_shape())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function and Optimizer\n",
    "\n",
    "\n",
    "loss = Loss_Function(Note_State_Batch, logP_out)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = .1).minimize(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 78, 128, 2)\n",
      "epoch =  0 ; loss =  1.11176\n",
      "(10, 78, 128, 2)\n",
      "epoch =  1 ; loss =  0.885046\n",
      "(10, 78, 128, 2)\n",
      "epoch =  2 ; loss =  0.755614\n",
      "(10, 78, 128, 2)\n",
      "epoch =  3 ; loss =  0.648337\n",
      "(10, 78, 128, 2)\n",
      "epoch =  4 ; loss =  0.587148\n",
      "(10, 78, 128, 2)\n",
      "epoch =  5 ; loss =  0.556796\n",
      "(10, 78, 128, 2)\n",
      "epoch =  6 ; loss =  0.536144\n",
      "(10, 78, 128, 2)\n",
      "epoch =  7 ; loss =  0.517845\n",
      "(10, 78, 128, 2)\n",
      "epoch =  8 ; loss =  0.500042\n",
      "(10, 78, 128, 2)\n",
      "epoch =  9 ; loss =  0.481294\n",
      "(10, 78, 128, 2)\n",
      "epoch =  10 ; loss =  0.459205\n",
      "(10, 78, 128, 2)\n",
      "epoch =  11 ; loss =  0.436617\n",
      "(10, 78, 128, 2)\n",
      "epoch =  12 ; loss =  0.421303\n",
      "(10, 78, 128, 2)\n",
      "epoch =  13 ; loss =  0.408813\n",
      "(10, 78, 128, 2)\n",
      "epoch =  14 ; loss =  0.402079\n",
      "(10, 78, 128, 2)\n",
      "epoch =  15 ; loss =  0.394895\n",
      "(10, 78, 128, 2)\n",
      "epoch =  16 ; loss =  0.388836\n",
      "(10, 78, 128, 2)\n",
      "epoch =  17 ; loss =  0.383452\n",
      "(10, 78, 128, 2)\n",
      "epoch =  18 ; loss =  0.378383\n",
      "(10, 78, 128, 2)\n",
      "epoch =  19 ; loss =  0.373591\n",
      "(10, 78, 128, 2)\n",
      "epoch =  20 ; loss =  0.369186\n",
      "(10, 78, 128, 2)\n",
      "epoch =  21 ; loss =  0.365101\n",
      "(10, 78, 128, 2)\n",
      "epoch =  22 ; loss =  0.361251\n",
      "(10, 78, 128, 2)\n",
      "epoch =  23 ; loss =  0.35765\n",
      "(10, 78, 128, 2)\n",
      "epoch =  24 ; loss =  0.35425\n",
      "(10, 78, 128, 2)\n",
      "epoch =  25 ; loss =  0.350956\n",
      "(10, 78, 128, 2)\n",
      "epoch =  26 ; loss =  0.347721\n",
      "(10, 78, 128, 2)\n",
      "epoch =  27 ; loss =  0.344569\n",
      "(10, 78, 128, 2)\n",
      "epoch =  28 ; loss =  0.341465\n",
      "(10, 78, 128, 2)\n",
      "epoch =  29 ; loss =  0.33837\n",
      "(10, 78, 128, 2)\n",
      "epoch =  30 ; loss =  0.33542\n",
      "(10, 78, 128, 2)\n",
      "epoch =  31 ; loss =  0.332391\n",
      "(10, 78, 128, 2)\n",
      "epoch =  32 ; loss =  0.329284\n",
      "(10, 78, 128, 2)\n",
      "epoch =  33 ; loss =  0.326296\n",
      "(10, 78, 128, 2)\n",
      "epoch =  34 ; loss =  0.323254\n",
      "(10, 78, 128, 2)\n",
      "epoch =  35 ; loss =  0.320083\n",
      "(10, 78, 128, 2)\n",
      "epoch =  36 ; loss =  0.316964\n",
      "(10, 78, 128, 2)\n",
      "epoch =  37 ; loss =  0.313737\n",
      "(10, 78, 128, 2)\n",
      "epoch =  38 ; loss =  0.310457\n",
      "(10, 78, 128, 2)\n",
      "epoch =  39 ; loss =  0.30704\n",
      "(10, 78, 128, 2)\n",
      "epoch =  40 ; loss =  0.303575\n",
      "(10, 78, 128, 2)\n",
      "epoch =  41 ; loss =  0.299986\n",
      "(10, 78, 128, 2)\n",
      "epoch =  42 ; loss =  0.296319\n",
      "(10, 78, 128, 2)\n",
      "epoch =  43 ; loss =  0.292556\n",
      "(10, 78, 128, 2)\n",
      "epoch =  44 ; loss =  0.288723\n",
      "(10, 78, 128, 2)\n",
      "epoch =  45 ; loss =  0.28483\n",
      "(10, 78, 128, 2)\n",
      "epoch =  46 ; loss =  0.280687\n",
      "(10, 78, 128, 2)\n",
      "epoch =  47 ; loss =  0.276381\n",
      "(10, 78, 128, 2)\n",
      "epoch =  48 ; loss =  0.27187\n",
      "(10, 78, 128, 2)\n",
      "epoch =  49 ; loss =  0.267444\n",
      "(10, 78, 128, 2)\n",
      "epoch =  50 ; loss =  0.263224\n",
      "(10, 78, 128, 2)\n",
      "epoch =  51 ; loss =  0.259432\n",
      "(10, 78, 128, 2)\n",
      "epoch =  52 ; loss =  0.255949\n",
      "(10, 78, 128, 2)\n",
      "epoch =  53 ; loss =  0.252745\n",
      "(10, 78, 128, 2)\n",
      "epoch =  54 ; loss =  0.249711\n",
      "(10, 78, 128, 2)\n",
      "epoch =  55 ; loss =  0.246901\n",
      "(10, 78, 128, 2)\n",
      "epoch =  56 ; loss =  0.244422\n",
      "(10, 78, 128, 2)\n",
      "epoch =  57 ; loss =  0.242237\n",
      "(10, 78, 128, 2)\n",
      "epoch =  58 ; loss =  0.240348\n",
      "(10, 78, 128, 2)\n",
      "epoch =  59 ; loss =  0.23867\n",
      "(10, 78, 128, 2)\n",
      "epoch =  60 ; loss =  0.237221\n",
      "(10, 78, 128, 2)\n",
      "epoch =  61 ; loss =  0.235894\n",
      "(10, 78, 128, 2)\n",
      "epoch =  62 ; loss =  0.234646\n",
      "(10, 78, 128, 2)\n",
      "epoch =  63 ; loss =  0.233519\n",
      "(10, 78, 128, 2)\n",
      "epoch =  64 ; loss =  0.232465\n",
      "(10, 78, 128, 2)\n",
      "epoch =  65 ; loss =  0.231518\n",
      "(10, 78, 128, 2)\n",
      "epoch =  66 ; loss =  0.230558\n",
      "(10, 78, 128, 2)\n",
      "epoch =  67 ; loss =  0.229718\n",
      "(10, 78, 128, 2)\n",
      "epoch =  68 ; loss =  0.228881\n",
      "(10, 78, 128, 2)\n",
      "epoch =  69 ; loss =  0.22812\n",
      "(10, 78, 128, 2)\n",
      "epoch =  70 ; loss =  0.227379\n",
      "(10, 78, 128, 2)\n",
      "epoch =  71 ; loss =  0.226651\n",
      "(10, 78, 128, 2)\n",
      "epoch =  72 ; loss =  0.22601\n",
      "(10, 78, 128, 2)\n",
      "epoch =  73 ; loss =  0.225336\n",
      "(10, 78, 128, 2)\n",
      "epoch =  74 ; loss =  0.224735\n",
      "(10, 78, 128, 2)\n",
      "epoch =  75 ; loss =  0.224149\n",
      "(10, 78, 128, 2)\n",
      "epoch =  76 ; loss =  0.223562\n",
      "(10, 78, 128, 2)\n",
      "epoch =  77 ; loss =  0.22302\n",
      "(10, 78, 128, 2)\n",
      "epoch =  78 ; loss =  0.222467\n",
      "(10, 78, 128, 2)\n",
      "epoch =  79 ; loss =  0.221975\n",
      "(10, 78, 128, 2)\n",
      "epoch =  80 ; loss =  0.221457\n",
      "(10, 78, 128, 2)\n",
      "epoch =  81 ; loss =  0.220985\n",
      "(10, 78, 128, 2)\n",
      "epoch =  82 ; loss =  0.220505\n",
      "(10, 78, 128, 2)\n",
      "epoch =  83 ; loss =  0.220045\n",
      "(10, 78, 128, 2)\n",
      "epoch =  84 ; loss =  0.21962\n",
      "(10, 78, 128, 2)\n",
      "epoch =  85 ; loss =  0.219179\n",
      "(10, 78, 128, 2)\n",
      "epoch =  86 ; loss =  0.218791\n",
      "(10, 78, 128, 2)\n",
      "epoch =  87 ; loss =  0.218362\n",
      "(10, 78, 128, 2)\n",
      "epoch =  88 ; loss =  0.217954\n",
      "(10, 78, 128, 2)\n",
      "epoch =  89 ; loss =  0.217588\n",
      "(10, 78, 128, 2)\n",
      "epoch =  90 ; loss =  0.21722\n",
      "(10, 78, 128, 2)\n",
      "epoch =  91 ; loss =  0.216868\n",
      "(10, 78, 128, 2)\n",
      "epoch =  92 ; loss =  0.216511\n",
      "(10, 78, 128, 2)\n",
      "epoch =  93 ; loss =  0.216193\n",
      "(10, 78, 128, 2)\n",
      "epoch =  94 ; loss =  0.215834\n",
      "(10, 78, 128, 2)\n",
      "epoch =  95 ; loss =  0.215508\n",
      "(10, 78, 128, 2)\n",
      "epoch =  96 ; loss =  0.215207\n",
      "(10, 78, 128, 2)\n",
      "epoch =  97 ; loss =  0.214891\n",
      "(10, 78, 128, 2)\n",
      "epoch =  98 ; loss =  0.214609\n",
      "(10, 78, 128, 2)\n",
      "epoch =  99 ; loss =  0.214307\n",
      "Model saved in file: model/First_Trial\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "N_epochs = 100\n",
    "loss_hist=[]\n",
    "restore_model_name = 'First_Trial'\n",
    "save_model_name = 'First_Trial'\n",
    "batch_size = 10\n",
    "num_timesteps = 128\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    # try to restore the pre_trained\n",
    "    if restore_model_name is not None:\n",
    "        print(\"Load the model from: {}\".format(restore_model_name))\n",
    "        saver.restore(sess, 'model/{}'.format(restore_model_name))\n",
    "    \n",
    "    c_run = np.zeros((batch_size, num_notes, num_units))\n",
    "    h_run = np.zeros((batch_size, num_notes, num_units))\n",
    "    first_input_load = np.zeros((batch_size, num_notes, 1)) #start every sequence with zero previous input\n",
    "\n",
    "    for epoch in range(N_epochs):\n",
    "        \n",
    "        #Generate Note_State Batch numpy tensor (bogus data for now)\n",
    "        batch_input_expand, batch_input_state = Utils.multi_training.getPieceBatch(training_pieces)\n",
    "        batch_input_expand, batch_input_state = np.array(x_expand), np.array(x_state)\n",
    "\n",
    "        batch_input_expand = np.swapaxes(batch_input_expand, axis1=1, axis2=2)\n",
    "        batch_input_state = np.swapaxes(batch_input_state, axis1=1, axis2=2)       \n",
    "        \n",
    "        #print(batch_input_state.shape)\n",
    "        \n",
    "    \n",
    "        feed_dict = {Note_State_Batch: batch_input_state, Note_State_Expand: batch_input_expand, timewise_c: c_run, timewise_h: h_run}\n",
    "        state_run, loss_run, _ = sess.run([timewise_state, loss, optimizer], feed_dict=feed_dict)\n",
    "        c_run, h_run = state_run\n",
    "        \n",
    "        \n",
    "        print('epoch = ', epoch, '; loss = ', loss_run)\n",
    "        loss_hist.append(loss_run)\n",
    "        \n",
    "    save_path = saver.save(sess, 'model/{}'.format(save_model_name))\n",
    "    print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(130, 200) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(200,) dtype=float32_ref>\n",
      "<tf.Variable 'basic_lstm_cell/kernel:0' shape=(56, 16) dtype=float32_ref>\n",
      "<tf.Variable 'basic_lstm_cell/bias:0' shape=(16,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "for v in range(len(tf.trainable_variables())):\n",
    "    print(tf.trainable_variables()[v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRDzovwAkCGS/B2dy/AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2d68f1062e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_hist, label=\"Loss History\")\n",
    "plt.legend()\n",
    "plt.show\n",
    "len(loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the model from: Trained Model\n",
      "INFO:tensorflow:Restoring parameters from model/Trained Model\n",
      "Timestep =  0\n",
      "Timestep =  50\n",
      "Timestep =  100\n",
      "Timestep =  150\n",
      "Timestep =  200\n",
      "Timestep =  250\n",
      "Timestep =  300\n",
      "Timestep =  350\n",
      "Timestep =  400\n",
      "Timestep =  450\n",
      "Timestep =  500\n",
      "Timestep =  550\n",
      "Timestep =  600\n",
      "Timestep =  650\n",
      "Timestep =  700\n",
      "Timestep =  750\n",
      "(10, 79, 768)\n"
     ]
    }
   ],
   "source": [
    "# Music Generation\n",
    "# input = initial note vector\n",
    "# for t = 1:Tsong\n",
    "#    input --> input kernel\n",
    "#    run through 1 'call' of Model LSTM with present parameters / states\n",
    "#    run through note-wise LSTM block as normally done to produce vector of generated samples\n",
    "#    input = generated samples\n",
    "#    music_sequence.append(input)\n",
    "\n",
    "# store batch of music sequences in .MIDI files\n",
    "\n",
    "\n",
    "#Load Model\n",
    "restore_model_name = 'Trained Model'\n",
    "\n",
    "#Length of generated music\n",
    "T_gen = 48*16\n",
    "batch_gen_size = 10\n",
    "\n",
    "\n",
    "# start with initial Note_State_Batch with 't' dimension = 1 (can still a batch of samples run in parallel)\n",
    "notes_gen_initial = np.zeros((batch_gen_size, num_notes, 1))\n",
    "notes_gen = notes_gen_initial\n",
    "c_run = np.zeros((batch_gen_size, num_notes, num_units))\n",
    "h_run = np.zeros((batch_gen_size, num_notes, num_units))   \n",
    "notes_gen_arr=[]\n",
    "latest_input = np.zeros((batch_size, num_notes, 1))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    print(\"Load the model from: {}\".format(restore_model_name))\n",
    "    saver.restore(sess, 'model/{}'.format(restore_model_name))\n",
    "    \n",
    "\n",
    "    for t in range(T_gen):\n",
    "        feed_dict = {Note_State_Batch: notes_gen,timewise_c: c_run, timewise_h: h_run, prev_t_sample: latest_input}\n",
    "        final_t_sample_run, state_run, notes_gen = np.squeeze(sess.run([final_t_sample, timewise_state, pa_gen_out], feed_dict = feed_dict), axis=2)\n",
    "        c_run, h_run = state_run\n",
    "        notes_gen = np.squeeze(notes_gen, axis=2)\n",
    "        notes_gen_arr.append(np.squeeze(notes_gen))\n",
    "        latest_input = final_t_sample_run\n",
    "        \n",
    "        if t % 50 == 0:\n",
    "            print('Timestep = ', t)\n",
    "    \n",
    "notes_gen_out = np.stack(notes_gen_arr, axis=2)\n",
    "print(notes_gen_out.shape)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items to Experiment with:\n",
    "- different T length or variable length T from batch-to-batch for training\n",
    "- categorize music, either through (unsupervised) clustering or (supervised) labeled music folders.  For clustering, the model would possibly find 'k' 'centroids' in an unsupervised manner each with its own music distribution, so during the music generation stage, 1 of these centroids would be selected for a piece of music.  \n",
    "- use encoder to reduce dimensionality of each note vector (vector of 79 notes in 1 time step), similiar to encoding the words from the tweets in homework 3 (i.e. there are restricted combinations of notes that can be played simultaneously)\n",
    "- more advanced sampling/exploring for training/music generation.  This may help prevent the algorithm from getting 'stuck' on a chord, or "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
